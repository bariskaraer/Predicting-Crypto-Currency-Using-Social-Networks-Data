{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Currency</th>\n",
       "      <th>Date</th>\n",
       "      <th>Closing Price (USD)</th>\n",
       "      <th>24h Open (USD)</th>\n",
       "      <th>24h High (USD)</th>\n",
       "      <th>24h Low (USD)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>BTC</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>964.325000</td>\n",
       "      <td>952.455000</td>\n",
       "      <td>968.485000</td>\n",
       "      <td>949.086250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>BTC</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>1009.973750</td>\n",
       "      <td>964.325000</td>\n",
       "      <td>1011.525000</td>\n",
       "      <td>963.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>BTC</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>1028.333750</td>\n",
       "      <td>1009.973750</td>\n",
       "      <td>1034.105000</td>\n",
       "      <td>998.621250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>BTC</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1047.099990</td>\n",
       "      <td>1028.333750</td>\n",
       "      <td>1048.123750</td>\n",
       "      <td>1013.377500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>BTC</td>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>1140.385000</td>\n",
       "      <td>1047.099990</td>\n",
       "      <td>1141.997500</td>\n",
       "      <td>1047.063750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2629</th>\n",
       "      <td>BTC</td>\n",
       "      <td>2020-12-23</td>\n",
       "      <td>23433.980758</td>\n",
       "      <td>22729.659291</td>\n",
       "      <td>23629.404460</td>\n",
       "      <td>22384.127846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>BTC</td>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>23224.454138</td>\n",
       "      <td>23791.565294</td>\n",
       "      <td>24086.950661</td>\n",
       "      <td>22644.561058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>BTC</td>\n",
       "      <td>2020-12-25</td>\n",
       "      <td>23623.885533</td>\n",
       "      <td>23228.909087</td>\n",
       "      <td>23737.757149</td>\n",
       "      <td>22716.174167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>BTC</td>\n",
       "      <td>2020-12-26</td>\n",
       "      <td>24581.006171</td>\n",
       "      <td>23726.190999</td>\n",
       "      <td>24627.297914</td>\n",
       "      <td>23422.310325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>BTC</td>\n",
       "      <td>2020-12-27</td>\n",
       "      <td>26381.296233</td>\n",
       "      <td>24708.206784</td>\n",
       "      <td>26750.879954</td>\n",
       "      <td>24497.410869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1446 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Currency        Date  Closing Price (USD)  24h Open (USD)  \\\n",
       "1188      BTC  2017-01-01           964.325000      952.455000   \n",
       "1189      BTC  2017-01-02          1009.973750      964.325000   \n",
       "1190      BTC  2017-01-03          1028.333750     1009.973750   \n",
       "1191      BTC  2017-01-04          1047.099990     1028.333750   \n",
       "1192      BTC  2017-01-05          1140.385000     1047.099990   \n",
       "...       ...         ...                  ...             ...   \n",
       "2629      BTC  2020-12-23         23433.980758    22729.659291   \n",
       "2630      BTC  2020-12-24         23224.454138    23791.565294   \n",
       "2631      BTC  2020-12-25         23623.885533    23228.909087   \n",
       "2632      BTC  2020-12-26         24581.006171    23726.190999   \n",
       "2633      BTC  2020-12-27         26381.296233    24708.206784   \n",
       "\n",
       "      24h High (USD)  24h Low (USD)  \n",
       "1188      968.485000     949.086250  \n",
       "1189     1011.525000     963.530000  \n",
       "1190     1034.105000     998.621250  \n",
       "1191     1048.123750    1013.377500  \n",
       "1192     1141.997500    1047.063750  \n",
       "...              ...            ...  \n",
       "2629    23629.404460   22384.127846  \n",
       "2630    24086.950661   22644.561058  \n",
       "2631    23737.757149   22716.174167  \n",
       "2632    24627.297914   23422.310325  \n",
       "2633    26750.879954   24497.410869  \n",
       "\n",
       "[1446 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ARRANGE BTC PRICE DATAFRAME\n",
    "\n",
    "df = pd.read_csv(\"BTC_historical_data_last.csv\")\n",
    "#drop 2021 data\n",
    "n_tail = 32\n",
    "n_head = 1188\n",
    "df.drop(df.tail(n_tail).index,inplace=True)\n",
    "df.drop(df.head(n_head).index,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1001.9846414285714,\n",
       " 858.1341071428571,\n",
       " 870.8037499999999,\n",
       " 914.2276785714286,\n",
       " 963.7555357142858,\n",
       " 1027.1746414285715,\n",
       " 1017.6185714285714,\n",
       " 1112.9139271428571,\n",
       " 1212.71057,\n",
       " 1217.4027114285714,\n",
       " 1192.346425714286,\n",
       " 1023.576712857143,\n",
       " 1020.8408542857143,\n",
       " 1147.8891428571428,\n",
       " 1203.426282857143,\n",
       " 1221.29832,\n",
       " 1277.5333928571429,\n",
       " 1447.7299985714287,\n",
       " 1659.5298214285715,\n",
       " 1803.6194642857142,\n",
       " 2287.143032857143,\n",
       " 2324.5616042857146,\n",
       " 2756.7523214285716,\n",
       " 2696.0798199999995,\n",
       " 2710.0449985714286,\n",
       " 2559.4280314285716,\n",
       " 2559.18375,\n",
       " 2377.136248571428,\n",
       " 2343.620891428571,\n",
       " 2700.3805342857145,\n",
       " 2816.55375,\n",
       " 3420.284107142857,\n",
       " 4219.528035714286,\n",
       " 4179.868748571429,\n",
       " 4593.250714285715,\n",
       " 4485.219464285715,\n",
       " 3955.7485699999997,\n",
       " 3763.3749999999995,\n",
       " 3982.4425,\n",
       " 4332.706427142857,\n",
       " 4979.380354285714,\n",
       " 5705.200535714285,\n",
       " 5793.775000000001,\n",
       " 6533.885357142857,\n",
       " 7208.051607142858,\n",
       " 6908.555535714286,\n",
       " 8037.998035714286,\n",
       " 9943.708392857143,\n",
       " 13150.484285714285,\n",
       " 16201.342677142855,\n",
       " 17065.234997142856,\n",
       " 14408.954285714284,\n",
       " 14479.389642857144,\n",
       " 14686.020892857143,\n",
       " 12249.07625,\n",
       " 11408.264464285714,\n",
       " 10095.43606857143,\n",
       " 7990.786428571429,\n",
       " 9175.623927142859,\n",
       " 10669.577857142856,\n",
       " 10449.483392857142,\n",
       " 10356.738570000001,\n",
       " 8652.064105714288,\n",
       " 8542.924642857144,\n",
       " 7770.583570000001,\n",
       " 6968.959642857143,\n",
       " 7215.687321428571,\n",
       " 8228.766962857144,\n",
       " 9052.69749857143,\n",
       " 9330.19482,\n",
       " 9184.569997142857,\n",
       " 8329.448391428572,\n",
       " 7952.97375,\n",
       " 7380.325654133,\n",
       " 7157.664521822057,\n",
       " 6598.772225340928,\n",
       " 6243.34389535873,\n",
       " 6366.174124189,\n",
       " 6488.324847204657,\n",
       " 6972.861051169544,\n",
       " 7906.158062847043,\n",
       " 7830.3976603669,\n",
       " 6856.2848923232,\n",
       " 6326.7241519440295,\n",
       " 6459.831214875542,\n",
       " 6897.4815259026145,\n",
       " 6983.902572728929,\n",
       " 6261.870743429899,\n",
       " 6285.987944740671,\n",
       " 6450.388444538628,\n",
       " 6587.266812942785,\n",
       " 6532.484312012356,\n",
       " 6475.864491533686,\n",
       " 6402.641327482671,\n",
       " 6334.290003242271,\n",
       " 6409.101593872586,\n",
       " 5845.5771226544,\n",
       " 4506.072003570256,\n",
       " 3986.8478023837283,\n",
       " 3713.016205289,\n",
       " 3339.076825556185,\n",
       " 3672.0830645298,\n",
       " 3817.269398791858,\n",
       " 3859.9317593459573,\n",
       " 3710.766745046743,\n",
       " 3593.662762785872,\n",
       " 3541.2957863542715,\n",
       " 3420.8935451199713,\n",
       " 3510.092145354829,\n",
       " 3620.202383023657,\n",
       " 3905.883922115114,\n",
       " 3779.8221159396567,\n",
       " 3859.8603799974576,\n",
       " 3913.0032995451,\n",
       " 3977.9812917062,\n",
       " 4047.672593081486,\n",
       " 5044.8790550419,\n",
       " 5123.084671677871,\n",
       " 5273.646384895727,\n",
       " 5253.355351577271,\n",
       " 5558.617047688201,\n",
       " 6647.705638542429,\n",
       " 7800.502762898929,\n",
       " 8167.206494515771,\n",
       " 8571.252717907872,\n",
       " 7814.48371180253,\n",
       " 8603.515134908985,\n",
       " 10005.623290684929,\n",
       " 11560.6618117288,\n",
       " 11473.037698418771,\n",
       " 11496.810546283772,\n",
       " 10301.877046744014,\n",
       " 9707.34180678787,\n",
       " 10586.570910576342,\n",
       " 11580.982884661056,\n",
       " 10454.021774754929,\n",
       " 10280.214975103314,\n",
       " 9818.797017323071,\n",
       " 10483.952294334844,\n",
       " 10269.575194714573,\n",
       " 10071.093703776456,\n",
       " 8271.3674953521,\n",
       " 8171.706864593886,\n",
       " 8374.990759212185,\n",
       " 8064.2056379057,\n",
       " 8733.6970414285,\n",
       " 9249.225458878127,\n",
       " 8958.321836880672,\n",
       " 8462.575789170929,\n",
       " 7378.252033492829,\n",
       " 7459.421913179972,\n",
       " 7397.088960857671,\n",
       " 7042.0707239835565,\n",
       " 7233.805344191971,\n",
       " 7250.295678339785,\n",
       " 7408.569355140857,\n",
       " 8149.167330618415,\n",
       " 8774.303979385371,\n",
       " 8627.6211121599,\n",
       " 9350.141793624944,\n",
       " 9891.171147785155,\n",
       " 10082.823250743115,\n",
       " 9663.622588042028,\n",
       " 8750.59148892983,\n",
       " 8555.526170428186,\n",
       " 5785.402663696414,\n",
       " 6165.904668458157,\n",
       " 6456.199928714156,\n",
       " 6897.067466348585,\n",
       " 7035.744891458628,\n",
       " 7024.871178957129,\n",
       " 7554.815265252685,\n",
       " 8875.444921340744,\n",
       " 9288.163060038287,\n",
       " 9571.100020292713,\n",
       " 9126.454596431044,\n",
       " 9563.785373111214,\n",
       " 9732.952660085071,\n",
       " 9480.438700083916,\n",
       " 9438.405713464128,\n",
       " 9170.85883360387,\n",
       " 9161.682595189855,\n",
       " 9281.020002621028,\n",
       " 9205.689365353828,\n",
       " 10070.356243450515,\n",
       " 11310.339141303813,\n",
       " 11668.715046070016,\n",
       " 11906.917613668573,\n",
       " 11661.519857526786,\n",
       " 11579.685648613628,\n",
       " 10482.850370744456,\n",
       " 10464.179050434086,\n",
       " 10837.654687845443,\n",
       " 10683.318777112258,\n",
       " 10642.454505870872,\n",
       " 11200.081321856842,\n",
       " 11542.560235802244,\n",
       " 13146.0513747528,\n",
       " 13630.250486378558,\n",
       " 15153.034505824658,\n",
       " 16382.321855826214,\n",
       " 18448.450613160985,\n",
       " 18178.690735260912,\n",
       " 19058.795086970084,\n",
       " 18811.005779150717,\n",
       " 23036.407578917195]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly_price_list = []\n",
    "closing_prices = list(df['Closing Price (USD)'])\n",
    "\n",
    "temp_list = []\n",
    "for i in range(0,len(closing_prices)):\n",
    "    if (i + 1) % 7 == 0:\n",
    "        temp_list.append(closing_prices[i])\n",
    "        weekly_price_list.append(temp_list)\n",
    "        temp_list = []\n",
    "    else:\n",
    "        temp_list.append(closing_prices[i])\n",
    "        \n",
    "    \n",
    "mean_weekly_list = []    \n",
    "for i in weekly_price_list:\n",
    "    mean_weekly_list.append(np.mean(i))\n",
    "    \n",
    "#len(mean_weekly_list)\n",
    "mean_weekly_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hafta</th>\n",
       "      <th>Bitcoin: (DÃ¼nya Genelinde)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-22</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-29</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-05</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-12</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-19</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-29</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-06</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-13</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-20</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-27</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Hafta       Bitcoin: (DÃ¼nya Genelinde)\n",
       "2017-01-22                           5\n",
       "2017-01-29                           5\n",
       "2017-02-05                           5\n",
       "2017-02-12                           5\n",
       "2017-02-19                           5\n",
       "...                                ...\n",
       "2020-11-29                          22\n",
       "2020-12-06                          18\n",
       "2020-12-13                          28\n",
       "2020-12-20                          29\n",
       "2020-12-27                          42\n",
       "\n",
       "[206 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends_weekly_df = pd.read_csv(\"weekly_chart_google_trends.csv\")\n",
    "trends_weekly_df.columns = trends_weekly_df.iloc[0]\n",
    "# trends_weekly_df = trends_weekly_df[47:256]\n",
    "trends_weekly_df = trends_weekly_df[50:256]\n",
    "trends_weekly_df['Bitcoin: (DÃ¼nya Genelinde)'] = trends_weekly_df['Bitcoin: (DÃ¼nya Genelinde)'].astype(\"int\")\n",
    "trends_weekly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Google Trends Data')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trends_weekly_df[['Bitcoin: (DÃ¼nya Genelinde)']].plot(figsize=(15, 5))\n",
    "plt.title(\"Google Trends Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hafta</th>\n",
       "      <th>Bitcoin: (DÃ¼nya Genelinde)</th>\n",
       "      <th>Mean Prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-22</th>\n",
       "      <td>5</td>\n",
       "      <td>1001.984641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-29</th>\n",
       "      <td>5</td>\n",
       "      <td>858.134107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-05</th>\n",
       "      <td>5</td>\n",
       "      <td>870.803750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-12</th>\n",
       "      <td>5</td>\n",
       "      <td>914.227679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-19</th>\n",
       "      <td>5</td>\n",
       "      <td>963.755536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-29</th>\n",
       "      <td>22</td>\n",
       "      <td>18448.450613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-06</th>\n",
       "      <td>18</td>\n",
       "      <td>18178.690735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-13</th>\n",
       "      <td>28</td>\n",
       "      <td>19058.795087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-20</th>\n",
       "      <td>29</td>\n",
       "      <td>18811.005779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-27</th>\n",
       "      <td>42</td>\n",
       "      <td>23036.407579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Hafta       Bitcoin: (DÃ¼nya Genelinde)   Mean Prices\n",
       "2017-01-22                           5   1001.984641\n",
       "2017-01-29                           5    858.134107\n",
       "2017-02-05                           5    870.803750\n",
       "2017-02-12                           5    914.227679\n",
       "2017-02-19                           5    963.755536\n",
       "...                                ...           ...\n",
       "2020-11-29                          22  18448.450613\n",
       "2020-12-06                          18  18178.690735\n",
       "2020-12-13                          28  19058.795087\n",
       "2020-12-20                          29  18811.005779\n",
       "2020-12-27                          42  23036.407579\n",
       "\n",
       "[206 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends_weekly_df['Mean Prices'] = np.array(mean_weekly_list)\n",
    "trends_weekly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Hafta', ylabel='Hafta'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(trends_weekly_df.corr(),annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hafta\n",
       "Bitcoin: (DÃ¼nya Genelinde)    0\n",
       "Mean Prices                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null values\n",
    "trends_weekly_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2400x1600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The Prices of Bitcoin over time\n",
    "plt.figure(figsize=(30, 20), dpi=80, facecolor = 'w', edgecolor = 'k')\n",
    "plt.plot(trends_weekly_df['Mean Prices'], color='red', label='Real BTC Price')\n",
    "plt.title('BTC Prices Weekly', fontsize = 40)\n",
    "plt.xlabel('Time', fontsize=40)\n",
    "plt.ylabel('BTC Price(USD)', fontsize = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hafta</th>\n",
       "      <th>Bitcoin: (DÃ¼nya Genelinde)</th>\n",
       "      <th>Mean Prices</th>\n",
       "      <th>Hafta Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-22</th>\n",
       "      <td>5</td>\n",
       "      <td>1001.984641</td>\n",
       "      <td>2017-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-29</th>\n",
       "      <td>5</td>\n",
       "      <td>858.134107</td>\n",
       "      <td>2017-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-05</th>\n",
       "      <td>5</td>\n",
       "      <td>870.803750</td>\n",
       "      <td>2017-02-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-12</th>\n",
       "      <td>5</td>\n",
       "      <td>914.227679</td>\n",
       "      <td>2017-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-19</th>\n",
       "      <td>5</td>\n",
       "      <td>963.755536</td>\n",
       "      <td>2017-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-29</th>\n",
       "      <td>22</td>\n",
       "      <td>18448.450613</td>\n",
       "      <td>2020-11-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-06</th>\n",
       "      <td>18</td>\n",
       "      <td>18178.690735</td>\n",
       "      <td>2020-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-13</th>\n",
       "      <td>28</td>\n",
       "      <td>19058.795087</td>\n",
       "      <td>2020-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-20</th>\n",
       "      <td>29</td>\n",
       "      <td>18811.005779</td>\n",
       "      <td>2020-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-27</th>\n",
       "      <td>42</td>\n",
       "      <td>23036.407579</td>\n",
       "      <td>2020-12-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Hafta       Bitcoin: (DÃ¼nya Genelinde)   Mean Prices Hafta Feature\n",
       "2017-01-22                           5   1001.984641    2017-01-22\n",
       "2017-01-29                           5    858.134107    2017-01-29\n",
       "2017-02-05                           5    870.803750    2017-02-05\n",
       "2017-02-12                           5    914.227679    2017-02-12\n",
       "2017-02-19                           5    963.755536    2017-02-19\n",
       "...                                ...           ...           ...\n",
       "2020-11-29                          22  18448.450613    2020-11-29\n",
       "2020-12-06                          18  18178.690735    2020-12-06\n",
       "2020-12-13                          28  19058.795087    2020-12-13\n",
       "2020-12-20                          29  18811.005779    2020-12-20\n",
       "2020-12-27                          42  23036.407579    2020-12-27\n",
       "\n",
       "[206 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends_weekly_df['Hafta Feature'] = trends_weekly_df.index\n",
    "trends_weekly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(trends_weekly_df)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hafta</th>\n",
       "      <th>Bitcoin: (DÃ¼nya Genelinde)</th>\n",
       "      <th>Mean Prices</th>\n",
       "      <th>Hafta Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-22</th>\n",
       "      <td>5</td>\n",
       "      <td>1001.984641</td>\n",
       "      <td>2017-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-29</th>\n",
       "      <td>5</td>\n",
       "      <td>858.134107</td>\n",
       "      <td>2017-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-05</th>\n",
       "      <td>5</td>\n",
       "      <td>870.803750</td>\n",
       "      <td>2017-02-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-12</th>\n",
       "      <td>5</td>\n",
       "      <td>914.227679</td>\n",
       "      <td>2017-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-19</th>\n",
       "      <td>5</td>\n",
       "      <td>963.755536</td>\n",
       "      <td>2017-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-29</th>\n",
       "      <td>22</td>\n",
       "      <td>18448.450613</td>\n",
       "      <td>2020-11-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-06</th>\n",
       "      <td>18</td>\n",
       "      <td>18178.690735</td>\n",
       "      <td>2020-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-13</th>\n",
       "      <td>28</td>\n",
       "      <td>19058.795087</td>\n",
       "      <td>2020-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-20</th>\n",
       "      <td>29</td>\n",
       "      <td>18811.005779</td>\n",
       "      <td>2020-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-27</th>\n",
       "      <td>42</td>\n",
       "      <td>23036.407579</td>\n",
       "      <td>2020-12-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Hafta       Bitcoin: (DÃ¼nya Genelinde)   Mean Prices Hafta Feature\n",
       "2017-01-22                           5   1001.984641    2017-01-22\n",
       "2017-01-29                           5    858.134107    2017-01-29\n",
       "2017-02-05                           5    870.803750    2017-02-05\n",
       "2017-02-12                           5    914.227679    2017-02-12\n",
       "2017-02-19                           5    963.755536    2017-02-19\n",
       "...                                ...           ...           ...\n",
       "2020-11-29                          22  18448.450613    2020-11-29\n",
       "2020-12-06                          18  18178.690735    2020-12-06\n",
       "2020-12-13                          28  19058.795087    2020-12-13\n",
       "2020-12-20                          29  18811.005779    2020-12-20\n",
       "2020-12-27                          42  23036.407579    2020-12-27\n",
       "\n",
       "[206 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting dates as a datetime object\n",
    "datelist_train = list(trends_weekly_df['Hafta Feature'])\n",
    "datelist_train = [dt.datetime.strptime(date, '%Y-%m-%d').date() for date in datelist_train]\n",
    "trends_weekly_df.index = datelist_train\n",
    "trends_weekly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hafta</th>\n",
       "      <th>Bitcoin: (DÃ¼nya Genelinde)</th>\n",
       "      <th>Mean Prices</th>\n",
       "      <th>Hafta Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-22</th>\n",
       "      <td>5</td>\n",
       "      <td>1001.984641</td>\n",
       "      <td>2017-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-29</th>\n",
       "      <td>5</td>\n",
       "      <td>858.134107</td>\n",
       "      <td>2017-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-05</th>\n",
       "      <td>5</td>\n",
       "      <td>870.803750</td>\n",
       "      <td>2017-02-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-12</th>\n",
       "      <td>5</td>\n",
       "      <td>914.227679</td>\n",
       "      <td>2017-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-19</th>\n",
       "      <td>5</td>\n",
       "      <td>963.755536</td>\n",
       "      <td>2017-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-29</th>\n",
       "      <td>22</td>\n",
       "      <td>18448.450613</td>\n",
       "      <td>2020-11-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-06</th>\n",
       "      <td>18</td>\n",
       "      <td>18178.690735</td>\n",
       "      <td>2020-12-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-13</th>\n",
       "      <td>28</td>\n",
       "      <td>19058.795087</td>\n",
       "      <td>2020-12-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-20</th>\n",
       "      <td>29</td>\n",
       "      <td>18811.005779</td>\n",
       "      <td>2020-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-27</th>\n",
       "      <td>42</td>\n",
       "      <td>23036.407579</td>\n",
       "      <td>2020-12-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Hafta       Bitcoin: (DÃ¼nya Genelinde)   Mean Prices Hafta Feature\n",
       "2017-01-22                           5   1001.984641    2017-01-22\n",
       "2017-01-29                           5    858.134107    2017-01-29\n",
       "2017-02-05                           5    870.803750    2017-02-05\n",
       "2017-02-12                           5    914.227679    2017-02-12\n",
       "2017-02-19                           5    963.755536    2017-02-19\n",
       "...                                ...           ...           ...\n",
       "2020-11-29                          22  18448.450613    2020-11-29\n",
       "2020-12-06                          18  18178.690735    2020-12-06\n",
       "2020-12-13                          28  19058.795087    2020-12-13\n",
       "2020-12-20                          29  18811.005779    2020-12-20\n",
       "2020-12-27                          42  23036.407579    2020-12-27\n",
       "\n",
       "[206 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends_weekly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data train shape (126, 3)\n",
      "Data test shape (80, 3)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data to train and test based on the prediction day we want\n",
    "\n",
    "date_string = \"21 June, 2019\"\n",
    "#possible parameter %H:%M:%S\n",
    "dt_onject_split = dt.datetime.strptime(date_string, \"%d %B, %Y\").date()\n",
    "data_train = trends_weekly_df.loc[trends_weekly_df.index <= dt_onject_split].copy()\n",
    "data_test = trends_weekly_df.loc[trends_weekly_df.index > dt_onject_split].copy()\n",
    "\n",
    "print(\"Data train shape\",data_train.shape)\n",
    "print(\"Data test shape\",data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_features_with_google_trends(df, label=None):\n",
    "    \"\"\"\n",
    "    Creates time series features from datetime index\n",
    "    \"\"\"\n",
    "    df['Hafta'] = df.index\n",
    "    df['Hafta'] = pd.to_datetime(df['Hafta'], errors='coerce')\n",
    "    df['month'] = df['Hafta'].dt.month\n",
    "    df['year'] = df['Hafta'].dt.year\n",
    "    df['dayofmonth'] = df['Hafta'].dt.day\n",
    "\n",
    "    \n",
    "    X = df[['month','year','dayofmonth','Bitcoin: (DÃ¼nya Genelinde)']]\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def create_features_without_google_trends(df, label=None):\n",
    "    \"\"\"\n",
    "    Creates time series features from datetime index\n",
    "    \"\"\"\n",
    "    df['Hafta'] = df.index\n",
    "    df['Hafta'] = pd.to_datetime(df['Hafta'], errors='coerce')\n",
    "    df['month'] = df['Hafta'].dt.month\n",
    "    df['year'] = df['Hafta'].dt.year\n",
    "    df['dayofmonth'] = df['Hafta'].dt.day\n",
    "\n",
    "    \n",
    "    X = df[['month','year','dayofmonth']]\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a train and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-fd02f2d7fc18>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['Bitcoin: (DÃ¼nya Genelinde)'] = X_train['Bitcoin: (DÃ¼nya Genelinde)'].astype(int)\n",
      "<ipython-input-16-fd02f2d7fc18>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['Bitcoin: (DÃ¼nya Genelinde)'] = X_test['Bitcoin: (DÃ¼nya Genelinde)'].astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hafta</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>Bitcoin: (DÃ¼nya Genelinde)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-22</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-29</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-05</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-12</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-19</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-19</th>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-26</th>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-02</th>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-09</th>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-16</th>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
       "2017-01-22      1  2017          22                           5\n",
       "2017-01-29      1  2017          29                           5\n",
       "2017-02-05      2  2017           5                           5\n",
       "2017-02-12      2  2017          12                           5\n",
       "2017-02-19      2  2017          19                           5\n",
       "...           ...   ...         ...                         ...\n",
       "2019-05-19      5  2019          19                          14\n",
       "2019-05-26      5  2019          26                          16\n",
       "2019-06-02      6  2019           2                          13\n",
       "2019-06-09      6  2019           9                          12\n",
       "2019-06-16      6  2019          16                          18\n",
       "\n",
       "[126 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = create_features_with_google_trends(data_train, label='Mean Prices')\n",
    "X_test, y_test = create_features_with_google_trends(data_test, label='Mean Prices')\n",
    "\n",
    "X_train['Bitcoin: (DÃ¼nya Genelinde)'] = X_train['Bitcoin: (DÃ¼nya Genelinde)'].astype(int)\n",
    "X_test['Bitcoin: (DÃ¼nya Genelinde)'] = X_test['Bitcoin: (DÃ¼nya Genelinde)'].astype(int)\n",
    "\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2017-01-22    1001.984641\n",
       "2017-01-29     858.134107\n",
       "2017-02-05     870.803750\n",
       "2017-02-12     914.227679\n",
       "2017-02-19     963.755536\n",
       "                 ...     \n",
       "2019-05-19    6647.705639\n",
       "2019-05-26    7800.502763\n",
       "2019-06-02    8167.206495\n",
       "2019-06-09    8571.252718\n",
       "2019-06-16    7814.483712\n",
       "Name: Mean Prices, Length: 126, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hafta</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofmonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-22</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-29</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-05</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-12</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-19</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-19</th>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-26</th>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-02</th>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-09</th>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-16</th>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Hafta       month  year  dayofmonth\n",
       "2017-01-22      1  2017          22\n",
       "2017-01-29      1  2017          29\n",
       "2017-02-05      2  2017           5\n",
       "2017-02-12      2  2017          12\n",
       "2017-02-19      2  2017          19\n",
       "...           ...   ...         ...\n",
       "2019-05-19      5  2019          19\n",
       "2019-05-26      5  2019          26\n",
       "2019-06-02      6  2019           2\n",
       "2019-06-09      6  2019           9\n",
       "2019-06-16      6  2019          16\n",
       "\n",
       "[126 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_second, y_train_second = create_features_without_google_trends(data_train, label='Mean Prices')\n",
    "X_test_second, y_test_second = create_features_without_google_trends(data_test, label='Mean Prices')\n",
    "\n",
    "X_train_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hafta</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofmonth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-06-23</th>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-30</th>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-07</th>\n",
       "      <td>7</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-14</th>\n",
       "      <td>7</td>\n",
       "      <td>2019</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-21</th>\n",
       "      <td>7</td>\n",
       "      <td>2019</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-29</th>\n",
       "      <td>11</td>\n",
       "      <td>2020</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-06</th>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-13</th>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-20</th>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-27</th>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Hafta       month  year  dayofmonth\n",
       "2019-06-23      6  2019          23\n",
       "2019-06-30      6  2019          30\n",
       "2019-07-07      7  2019           7\n",
       "2019-07-14      7  2019          14\n",
       "2019-07-21      7  2019          21\n",
       "...           ...   ...         ...\n",
       "2020-11-29     11  2020          29\n",
       "2020-12-06     12  2020           6\n",
       "2020-12-13     12  2020          13\n",
       "2020-12-20     12  2020          20\n",
       "2020-12-27     12  2020          27\n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an LSTM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of features X_train.shape[1]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler_y = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.7       , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.93333333, 0.        ]],\n",
       "\n",
       "       [[0.09090909, 0.        , 0.13333333, 0.        ]],\n",
       "\n",
       "       [[0.09090909, 0.        , 0.36666667, 0.        ]],\n",
       "\n",
       "       [[0.09090909, 0.        , 0.6       , 0.        ]],\n",
       "\n",
       "       [[0.09090909, 0.        , 0.83333333, 0.02352941]],\n",
       "\n",
       "       [[0.18181818, 0.        , 0.13333333, 0.02352941]],\n",
       "\n",
       "       [[0.18181818, 0.        , 0.36666667, 0.01176471]],\n",
       "\n",
       "       [[0.18181818, 0.        , 0.6       , 0.01176471]],\n",
       "\n",
       "       [[0.18181818, 0.        , 0.83333333, 0.        ]],\n",
       "\n",
       "       [[0.27272727, 0.        , 0.03333333, 0.        ]],\n",
       "\n",
       "       [[0.27272727, 0.        , 0.26666667, 0.        ]],\n",
       "\n",
       "       [[0.27272727, 0.        , 0.5       , 0.        ]],\n",
       "\n",
       "       [[0.27272727, 0.        , 0.73333333, 0.01176471]],\n",
       "\n",
       "       [[0.27272727, 0.        , 0.96666667, 0.03529412]],\n",
       "\n",
       "       [[0.36363636, 0.        , 0.2       , 0.05882353]],\n",
       "\n",
       "       [[0.36363636, 0.        , 0.43333333, 0.08235294]],\n",
       "\n",
       "       [[0.36363636, 0.        , 0.66666667, 0.16470588]],\n",
       "\n",
       "       [[0.36363636, 0.        , 0.9       , 0.09411765]],\n",
       "\n",
       "       [[0.45454545, 0.        , 0.1       , 0.09411765]],\n",
       "\n",
       "       [[0.45454545, 0.        , 0.33333333, 0.11764706]],\n",
       "\n",
       "       [[0.45454545, 0.        , 0.56666667, 0.08235294]],\n",
       "\n",
       "       [[0.45454545, 0.        , 0.8       , 0.08235294]],\n",
       "\n",
       "       [[0.54545455, 0.        , 0.03333333, 0.05882353]],\n",
       "\n",
       "       [[0.54545455, 0.        , 0.26666667, 0.07058824]],\n",
       "\n",
       "       [[0.54545455, 0.        , 0.5       , 0.09411765]],\n",
       "\n",
       "       [[0.54545455, 0.        , 0.73333333, 0.08235294]],\n",
       "\n",
       "       [[0.54545455, 0.        , 0.96666667, 0.10588235]],\n",
       "\n",
       "       [[0.63636364, 0.        , 0.16666667, 0.10588235]],\n",
       "\n",
       "       [[0.63636364, 0.        , 0.4       , 0.17647059]],\n",
       "\n",
       "       [[0.63636364, 0.        , 0.63333333, 0.12941176]],\n",
       "\n",
       "       [[0.63636364, 0.        , 0.86666667, 0.14117647]],\n",
       "\n",
       "       [[0.72727273, 0.        , 0.06666667, 0.15294118]],\n",
       "\n",
       "       [[0.72727273, 0.        , 0.3       , 0.18823529]],\n",
       "\n",
       "       [[0.72727273, 0.        , 0.53333333, 0.12941176]],\n",
       "\n",
       "       [[0.72727273, 0.        , 0.76666667, 0.08235294]],\n",
       "\n",
       "       [[0.81818182, 0.        , 0.        , 0.09411765]],\n",
       "\n",
       "       [[0.81818182, 0.        , 0.23333333, 0.16470588]],\n",
       "\n",
       "       [[0.81818182, 0.        , 0.46666667, 0.17647059]],\n",
       "\n",
       "       [[0.81818182, 0.        , 0.7       , 0.17647059]],\n",
       "\n",
       "       [[0.81818182, 0.        , 0.93333333, 0.24705882]],\n",
       "\n",
       "       [[0.90909091, 0.        , 0.13333333, 0.27058824]],\n",
       "\n",
       "       [[0.90909091, 0.        , 0.36666667, 0.28235294]],\n",
       "\n",
       "       [[0.90909091, 0.        , 0.6       , 0.25882353]],\n",
       "\n",
       "       [[0.90909091, 0.        , 0.83333333, 0.65882353]],\n",
       "\n",
       "       [[1.        , 0.        , 0.06666667, 0.96470588]],\n",
       "\n",
       "       [[1.        , 0.        , 0.3       , 0.87058824]],\n",
       "\n",
       "       [[1.        , 0.        , 0.53333333, 1.        ]],\n",
       "\n",
       "       [[1.        , 0.        , 0.76666667, 0.67058824]],\n",
       "\n",
       "       [[1.        , 0.        , 1.        , 0.48235294]],\n",
       "\n",
       "       [[0.        , 0.5       , 0.2       , 0.48235294]],\n",
       "\n",
       "       [[0.        , 0.5       , 0.43333333, 0.6       ]],\n",
       "\n",
       "       [[0.        , 0.5       , 0.66666667, 0.38823529]],\n",
       "\n",
       "       [[0.        , 0.5       , 0.9       , 0.41176471]],\n",
       "\n",
       "       [[0.09090909, 0.5       , 0.1       , 0.47058824]],\n",
       "\n",
       "       [[0.09090909, 0.5       , 0.33333333, 0.27058824]],\n",
       "\n",
       "       [[0.09090909, 0.5       , 0.56666667, 0.22352941]],\n",
       "\n",
       "       [[0.09090909, 0.5       , 0.8       , 0.17647059]],\n",
       "\n",
       "       [[0.18181818, 0.5       , 0.1       , 0.17647059]],\n",
       "\n",
       "       [[0.18181818, 0.5       , 0.33333333, 0.16470588]],\n",
       "\n",
       "       [[0.18181818, 0.5       , 0.56666667, 0.15294118]],\n",
       "\n",
       "       [[0.18181818, 0.5       , 0.8       , 0.15294118]],\n",
       "\n",
       "       [[0.27272727, 0.5       , 0.        , 0.14117647]],\n",
       "\n",
       "       [[0.27272727, 0.5       , 0.23333333, 0.12941176]],\n",
       "\n",
       "       [[0.27272727, 0.5       , 0.46666667, 0.10588235]],\n",
       "\n",
       "       [[0.27272727, 0.5       , 0.7       , 0.11764706]],\n",
       "\n",
       "       [[0.27272727, 0.5       , 0.93333333, 0.09411765]],\n",
       "\n",
       "       [[0.36363636, 0.5       , 0.16666667, 0.09411765]],\n",
       "\n",
       "       [[0.36363636, 0.5       , 0.4       , 0.07058824]],\n",
       "\n",
       "       [[0.36363636, 0.5       , 0.63333333, 0.07058824]],\n",
       "\n",
       "       [[0.36363636, 0.5       , 0.86666667, 0.07058824]],\n",
       "\n",
       "       [[0.45454545, 0.5       , 0.06666667, 0.05882353]],\n",
       "\n",
       "       [[0.45454545, 0.5       , 0.3       , 0.09411765]],\n",
       "\n",
       "       [[0.45454545, 0.5       , 0.53333333, 0.07058824]],\n",
       "\n",
       "       [[0.45454545, 0.5       , 0.76666667, 0.07058824]],\n",
       "\n",
       "       [[0.54545455, 0.5       , 0.        , 0.05882353]],\n",
       "\n",
       "       [[0.54545455, 0.5       , 0.23333333, 0.04705882]],\n",
       "\n",
       "       [[0.54545455, 0.5       , 0.46666667, 0.07058824]],\n",
       "\n",
       "       [[0.54545455, 0.5       , 0.7       , 0.08235294]],\n",
       "\n",
       "       [[0.54545455, 0.5       , 0.93333333, 0.07058824]],\n",
       "\n",
       "       [[0.63636364, 0.5       , 0.13333333, 0.07058824]],\n",
       "\n",
       "       [[0.63636364, 0.5       , 0.36666667, 0.07058824]],\n",
       "\n",
       "       [[0.63636364, 0.5       , 0.6       , 0.05882353]],\n",
       "\n",
       "       [[0.63636364, 0.5       , 0.83333333, 0.05882353]],\n",
       "\n",
       "       [[0.72727273, 0.5       , 0.03333333, 0.07058824]],\n",
       "\n",
       "       [[0.72727273, 0.5       , 0.26666667, 0.05882353]],\n",
       "\n",
       "       [[0.72727273, 0.5       , 0.5       , 0.05882353]],\n",
       "\n",
       "       [[0.72727273, 0.5       , 0.73333333, 0.04705882]],\n",
       "\n",
       "       [[0.72727273, 0.5       , 0.96666667, 0.04705882]],\n",
       "\n",
       "       [[0.81818182, 0.5       , 0.2       , 0.04705882]],\n",
       "\n",
       "       [[0.81818182, 0.5       , 0.43333333, 0.04705882]],\n",
       "\n",
       "       [[0.81818182, 0.5       , 0.66666667, 0.03529412]],\n",
       "\n",
       "       [[0.81818182, 0.5       , 0.9       , 0.03529412]],\n",
       "\n",
       "       [[0.90909091, 0.5       , 0.1       , 0.03529412]],\n",
       "\n",
       "       [[0.90909091, 0.5       , 0.33333333, 0.05882353]],\n",
       "\n",
       "       [[0.90909091, 0.5       , 0.56666667, 0.10588235]],\n",
       "\n",
       "       [[0.90909091, 0.5       , 0.8       , 0.10588235]],\n",
       "\n",
       "       [[1.        , 0.5       , 0.03333333, 0.08235294]],\n",
       "\n",
       "       [[1.        , 0.5       , 0.26666667, 0.07058824]],\n",
       "\n",
       "       [[1.        , 0.5       , 0.5       , 0.08235294]],\n",
       "\n",
       "       [[1.        , 0.5       , 0.73333333, 0.05882353]],\n",
       "\n",
       "       [[1.        , 0.5       , 0.96666667, 0.04705882]],\n",
       "\n",
       "       [[0.        , 1.        , 0.16666667, 0.04705882]],\n",
       "\n",
       "       [[0.        , 1.        , 0.4       , 0.03529412]],\n",
       "\n",
       "       [[0.        , 1.        , 0.63333333, 0.03529412]],\n",
       "\n",
       "       [[0.        , 1.        , 0.86666667, 0.03529412]],\n",
       "\n",
       "       [[0.09090909, 1.        , 0.06666667, 0.04705882]],\n",
       "\n",
       "       [[0.09090909, 1.        , 0.3       , 0.02352941]],\n",
       "\n",
       "       [[0.09090909, 1.        , 0.53333333, 0.04705882]],\n",
       "\n",
       "       [[0.09090909, 1.        , 0.76666667, 0.03529412]],\n",
       "\n",
       "       [[0.18181818, 1.        , 0.06666667, 0.03529412]],\n",
       "\n",
       "       [[0.18181818, 1.        , 0.3       , 0.02352941]],\n",
       "\n",
       "       [[0.18181818, 1.        , 0.53333333, 0.02352941]],\n",
       "\n",
       "       [[0.18181818, 1.        , 0.76666667, 0.02352941]],\n",
       "\n",
       "       [[0.18181818, 1.        , 1.        , 0.09411765]],\n",
       "\n",
       "       [[0.27272727, 1.        , 0.2       , 0.07058824]],\n",
       "\n",
       "       [[0.27272727, 1.        , 0.43333333, 0.04705882]],\n",
       "\n",
       "       [[0.27272727, 1.        , 0.66666667, 0.05882353]],\n",
       "\n",
       "       [[0.27272727, 1.        , 0.9       , 0.04705882]],\n",
       "\n",
       "       [[0.36363636, 1.        , 0.13333333, 0.09411765]],\n",
       "\n",
       "       [[0.36363636, 1.        , 0.36666667, 0.17647059]],\n",
       "\n",
       "       [[0.36363636, 1.        , 0.6       , 0.10588235]],\n",
       "\n",
       "       [[0.36363636, 1.        , 0.83333333, 0.12941176]],\n",
       "\n",
       "       [[0.45454545, 1.        , 0.03333333, 0.09411765]],\n",
       "\n",
       "       [[0.45454545, 1.        , 0.26666667, 0.08235294]],\n",
       "\n",
       "       [[0.45454545, 1.        , 0.5       , 0.15294118]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turning input X_Train to appropriate shape for LSTM    ((n_samples, time_steps, features))\n",
    "x_training_set = X_train.values\n",
    "x_training_set = x_training_set[0:len(x_training_set)]\n",
    "\n",
    "x_training_set = min_max_scaler.fit_transform(x_training_set)\n",
    "x_training_set = x_training_set.reshape((x_training_set.shape[0], 1, x_training_set.shape[1]))\n",
    "x_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)\n",
    "list(y_train['Mean Prices'])\n",
    "y_train_values = [float(i) for i in y_train['Mean Prices']]\n",
    "y_train_values = np.array(y_train_values)\n",
    "y_train_values = np.array(y_train_values)\n",
    "\n",
    "y_train_values = np.reshape(y_train_values, (len(y_train_values), 1))\n",
    "y_training_set = min_max_scaler_y.fit_transform(y_train_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "26/26 [==============================] - 2s 918us/step - loss: 0.1116\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0533\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0408\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0296\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0389\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0288\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 0s 938us/step - loss: 0.0297\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0330\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0298\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0239\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 0s 997us/step - loss: 0.0227\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0232\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 0s 997us/step - loss: 0.0256\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 0s 997us/step - loss: 0.0261\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0235\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0216\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0253\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0283\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0224\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0236\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0224\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0189\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0276\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0211\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 0s 890us/step - loss: 0.0251\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0209\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0187\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0249\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0235\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0194\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0176\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0183\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0185\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0181\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0135\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0174\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0180\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0246\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0138\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0207\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0175\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0149\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0141\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0122\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0183\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0114\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0134\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0141\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0106\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0161\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0110\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 0s 997us/step - loss: 0.0103\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0132\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0116\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 0s 997us/step - loss: 0.0122\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 0s 997us/step - loss: 0.0138\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0092\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0131\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0073\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0103\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0097\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0111\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0112\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 0s 924us/step - loss: 0.0127\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0108\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0128\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0152\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0084\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0104\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0075\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0092\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0104\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0075\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0140\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0077\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0105\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0121\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 0s 997us/step - loss: 0.0102\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0095\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0100\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0084\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0096\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0088\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0083\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0086\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0074\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0088\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0094\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0091\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0070\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0084\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0092\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0120\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0098\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0100\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0076\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0063\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 878us/step - loss: 0.0100\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a98f71b9a0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the input layer and the LSTM layer\n",
    "regressor.add(LSTM(units = 50, input_shape=(x_training_set.shape[1], x_training_set.shape[2])))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Using the training set to train the model\n",
    "regressor.fit(x_training_set, y_training_set, batch_size = 5, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the model with google trends as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.45454545, 1.        , 0.73333333, 0.25882353]],\n",
       "\n",
       "       [[0.45454545, 1.        , 0.96666667, 0.15294118]],\n",
       "\n",
       "       [[0.54545455, 1.        , 0.2       , 0.14117647]],\n",
       "\n",
       "       [[0.54545455, 1.        , 0.43333333, 0.14117647]],\n",
       "\n",
       "       [[0.54545455, 1.        , 0.66666667, 0.09411765]],\n",
       "\n",
       "       [[0.54545455, 1.        , 0.9       , 0.09411765]],\n",
       "\n",
       "       [[0.63636364, 1.        , 0.1       , 0.11764706]],\n",
       "\n",
       "       [[0.63636364, 1.        , 0.33333333, 0.09411765]],\n",
       "\n",
       "       [[0.63636364, 1.        , 0.56666667, 0.08235294]],\n",
       "\n",
       "       [[0.63636364, 1.        , 0.8       , 0.41176471]],\n",
       "\n",
       "       [[0.72727273, 1.        , 0.        , 1.11764706]],\n",
       "\n",
       "       [[0.72727273, 1.        , 0.23333333, 0.63529412]],\n",
       "\n",
       "       [[0.72727273, 1.        , 0.46666667, 0.56470588]],\n",
       "\n",
       "       [[0.72727273, 1.        , 0.7       , 0.35294118]],\n",
       "\n",
       "       [[0.72727273, 1.        , 0.93333333, 0.07058824]],\n",
       "\n",
       "       [[0.81818182, 1.        , 0.16666667, 0.05882353]],\n",
       "\n",
       "       [[0.81818182, 1.        , 0.4       , 0.05882353]],\n",
       "\n",
       "       [[0.81818182, 1.        , 0.63333333, 0.08235294]],\n",
       "\n",
       "       [[0.81818182, 1.        , 0.86666667, 0.07058824]],\n",
       "\n",
       "       [[0.90909091, 1.        , 0.06666667, 0.05882353]],\n",
       "\n",
       "       [[0.90909091, 1.        , 0.3       , 0.04705882]],\n",
       "\n",
       "       [[0.90909091, 1.        , 0.53333333, 0.05882353]],\n",
       "\n",
       "       [[0.90909091, 1.        , 0.76666667, 0.07058824]],\n",
       "\n",
       "       [[1.        , 1.        , 0.        , 0.04705882]],\n",
       "\n",
       "       [[1.        , 1.        , 0.23333333, 0.04705882]],\n",
       "\n",
       "       [[1.        , 1.        , 0.46666667, 0.05882353]],\n",
       "\n",
       "       [[1.        , 1.        , 0.7       , 0.04705882]],\n",
       "\n",
       "       [[1.        , 1.        , 0.93333333, 0.04705882]],\n",
       "\n",
       "       [[0.        , 1.5       , 0.13333333, 0.08235294]],\n",
       "\n",
       "       [[0.        , 1.5       , 0.36666667, 0.07058824]],\n",
       "\n",
       "       [[0.        , 1.5       , 0.6       , 0.05882353]],\n",
       "\n",
       "       [[0.        , 1.5       , 0.83333333, 0.07058824]],\n",
       "\n",
       "       [[0.09090909, 1.5       , 0.03333333, 0.07058824]],\n",
       "\n",
       "       [[0.09090909, 1.5       , 0.26666667, 0.09411765]],\n",
       "\n",
       "       [[0.09090909, 1.5       , 0.5       , 0.07058824]],\n",
       "\n",
       "       [[0.09090909, 1.5       , 0.73333333, 0.08235294]],\n",
       "\n",
       "       [[0.18181818, 1.5       , 0.        , 0.07058824]],\n",
       "\n",
       "       [[0.18181818, 1.5       , 0.23333333, 0.17647059]],\n",
       "\n",
       "       [[0.18181818, 1.5       , 0.46666667, 0.17647059]],\n",
       "\n",
       "       [[0.18181818, 1.5       , 0.7       , 0.12941176]],\n",
       "\n",
       "       [[0.18181818, 1.5       , 0.93333333, 0.10588235]],\n",
       "\n",
       "       [[0.27272727, 1.5       , 0.13333333, 0.10588235]],\n",
       "\n",
       "       [[0.27272727, 1.5       , 0.36666667, 0.09411765]],\n",
       "\n",
       "       [[0.27272727, 1.5       , 0.6       , 0.09411765]],\n",
       "\n",
       "       [[0.27272727, 1.5       , 0.83333333, 0.12941176]],\n",
       "\n",
       "       [[0.36363636, 1.5       , 0.06666667, 0.15294118]],\n",
       "\n",
       "       [[0.36363636, 1.5       , 0.3       , 0.18823529]],\n",
       "\n",
       "       [[0.36363636, 1.5       , 0.53333333, 0.12941176]],\n",
       "\n",
       "       [[0.36363636, 1.5       , 0.76666667, 0.09411765]],\n",
       "\n",
       "       [[0.36363636, 1.5       , 1.        , 0.09411765]],\n",
       "\n",
       "       [[0.45454545, 1.5       , 0.2       , 0.08235294]],\n",
       "\n",
       "       [[0.45454545, 1.5       , 0.43333333, 0.07058824]],\n",
       "\n",
       "       [[0.45454545, 1.5       , 0.66666667, 0.07058824]],\n",
       "\n",
       "       [[0.45454545, 1.5       , 0.9       , 0.07058824]],\n",
       "\n",
       "       [[0.54545455, 1.5       , 0.13333333, 0.05882353]],\n",
       "\n",
       "       [[0.54545455, 1.5       , 0.36666667, 0.08235294]],\n",
       "\n",
       "       [[0.54545455, 1.5       , 0.6       , 0.07058824]],\n",
       "\n",
       "       [[0.54545455, 1.5       , 0.83333333, 0.15294118]],\n",
       "\n",
       "       [[0.63636364, 1.5       , 0.03333333, 0.14117647]],\n",
       "\n",
       "       [[0.63636364, 1.5       , 0.26666667, 0.10588235]],\n",
       "\n",
       "       [[0.63636364, 1.5       , 0.5       , 0.10588235]],\n",
       "\n",
       "       [[0.63636364, 1.5       , 0.73333333, 0.08235294]],\n",
       "\n",
       "       [[0.63636364, 1.5       , 0.96666667, 0.09411765]],\n",
       "\n",
       "       [[0.72727273, 1.5       , 0.16666667, 0.08235294]],\n",
       "\n",
       "       [[0.72727273, 1.5       , 0.4       , 0.07058824]],\n",
       "\n",
       "       [[0.72727273, 1.5       , 0.63333333, 0.05882353]],\n",
       "\n",
       "       [[0.72727273, 1.5       , 0.86666667, 0.04705882]],\n",
       "\n",
       "       [[0.81818182, 1.5       , 0.1       , 0.04705882]],\n",
       "\n",
       "       [[0.81818182, 1.5       , 0.33333333, 0.05882353]],\n",
       "\n",
       "       [[0.81818182, 1.5       , 0.56666667, 0.10588235]],\n",
       "\n",
       "       [[0.81818182, 1.5       , 0.8       , 0.11764706]],\n",
       "\n",
       "       [[0.90909091, 1.5       , 0.        , 0.12941176]],\n",
       "\n",
       "       [[0.90909091, 1.5       , 0.23333333, 0.14117647]],\n",
       "\n",
       "       [[0.90909091, 1.5       , 0.46666667, 0.21176471]],\n",
       "\n",
       "       [[0.90909091, 1.5       , 0.7       , 0.23529412]],\n",
       "\n",
       "       [[0.90909091, 1.5       , 0.93333333, 0.2       ]],\n",
       "\n",
       "       [[1.        , 1.5       , 0.16666667, 0.15294118]],\n",
       "\n",
       "       [[1.        , 1.5       , 0.4       , 0.27058824]],\n",
       "\n",
       "       [[1.        , 1.5       , 0.63333333, 0.28235294]],\n",
       "\n",
       "       [[1.        , 1.5       , 0.86666667, 0.43529412]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = X_test.values\n",
    "\n",
    "# Reshaping and scaling the inputs\n",
    "inputs = np.reshape(test_set, (len(test_set), 4))\n",
    "inputs = min_max_scaler.transform(inputs)\n",
    "inputs = inputs.reshape((inputs.shape[0], 1, 4))\n",
    "predicted_price = regressor.predict(inputs)\n",
    "inputs = inputs.reshape((inputs.shape[0], 1, 4))\n",
    "predicted_price = min_max_scaler_y.inverse_transform(predicted_price)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10991.642  ],\n",
       "       [ 8051.996  ],\n",
       "       [ 7858.5884 ],\n",
       "       [ 7911.3525 ],\n",
       "       [ 6657.661  ],\n",
       "       [ 6579.2427 ],\n",
       "       [ 7376.58   ],\n",
       "       [ 6851.9434 ],\n",
       "       [ 6536.0063 ],\n",
       "       [15294.434  ],\n",
       "       [26208.879  ],\n",
       "       [18995.998  ],\n",
       "       [18231.068  ],\n",
       "       [13704.893  ],\n",
       "       [ 6299.4043 ],\n",
       "       [ 6258.4746 ],\n",
       "       [ 6271.854  ],\n",
       "       [ 6858.4478 ],\n",
       "       [ 6514.837  ],\n",
       "       [ 6359.2554 ],\n",
       "       [ 6132.1943 ],\n",
       "       [ 6425.735  ],\n",
       "       [ 6715.0337 ],\n",
       "       [ 6192.922  ],\n",
       "       [ 6258.7656 ],\n",
       "       [ 6575.29   ],\n",
       "       [ 6289.445  ],\n",
       "       [ 6251.9385 ],\n",
       "       [  913.6585 ],\n",
       "       [  406.9386 ],\n",
       "       [ -133.44922],\n",
       "       [   38.78741],\n",
       "       [  857.51184],\n",
       "       [ 1392.214  ],\n",
       "       [  543.5129 ],\n",
       "       [  733.8815 ],\n",
       "       [ 1091.0345 ],\n",
       "       [ 4005.1733 ],\n",
       "       [ 3966.262  ],\n",
       "       [ 2454.2617 ],\n",
       "       [ 1588.212  ],\n",
       "       [ 2227.842  ],\n",
       "       [ 1792.2437 ],\n",
       "       [ 1670.9674 ],\n",
       "       [ 2634.074  ],\n",
       "       [ 3755.4116 ],\n",
       "       [ 4744.5693 ],\n",
       "       [ 2994.094  ],\n",
       "       [ 1827.9037 ],\n",
       "       [ 1694.4713 ],\n",
       "       [ 1984.693  ],\n",
       "       [ 1549.3774 ],\n",
       "       [ 1425.2919 ],\n",
       "       [ 1286.7035 ],\n",
       "       [ 1588.6771 ],\n",
       "       [ 2153.7896 ],\n",
       "       [ 1713.98   ],\n",
       "       [ 4099.267  ],\n",
       "       [ 4027.9104 ],\n",
       "       [ 3067.4856 ],\n",
       "       [ 3021.262  ],\n",
       "       [ 2255.0918 ],\n",
       "       [ 2508.6323 ],\n",
       "       [ 2665.2056 ],\n",
       "       [ 2287.3997 ],\n",
       "       [ 1864.7448 ],\n",
       "       [ 1400.2432 ],\n",
       "       [ 1967.7498 ],\n",
       "       [ 2221.3013 ],\n",
       "       [ 3485.6943 ],\n",
       "       [ 3789.3096 ],\n",
       "       [ 4300.458  ],\n",
       "       [ 4664.057  ],\n",
       "       [ 6657.604  ],\n",
       "       [ 7433.262  ],\n",
       "       [ 6478.877  ],\n",
       "       [ 5150.119  ],\n",
       "       [ 8393.743  ],\n",
       "       [ 8916.473  ],\n",
       "       [13505.696  ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6957.064826461658"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test,predicted_price,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6200.279999181039"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test,predicted_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([2019-06-23, 2019-06-30, 2019-07-07, 2019-07-14, 2019-07-21, 2019-07-28,\n",
       "       2019-08-04, 2019-08-11, 2019-08-18, 2019-08-25, 2019-09-01, 2019-09-08,\n",
       "       2019-09-15, 2019-09-22, 2019-09-29, 2019-10-06, 2019-10-13, 2019-10-20,\n",
       "       2019-10-27, 2019-11-03, 2019-11-10, 2019-11-17, 2019-11-24, 2019-12-01,\n",
       "       2019-12-08, 2019-12-15, 2019-12-22, 2019-12-29, 2020-01-05, 2020-01-12,\n",
       "       2020-01-19, 2020-01-26, 2020-02-02, 2020-02-09, 2020-02-16, 2020-02-23,\n",
       "       2020-03-01, 2020-03-08, 2020-03-15, 2020-03-22, 2020-03-29, 2020-04-05,\n",
       "       2020-04-12, 2020-04-19, 2020-04-26, 2020-05-03, 2020-05-10, 2020-05-17,\n",
       "       2020-05-24, 2020-05-31, 2020-06-07, 2020-06-14, 2020-06-21, 2020-06-28,\n",
       "       2020-07-05, 2020-07-12, 2020-07-19, 2020-07-26, 2020-08-02, 2020-08-09,\n",
       "       2020-08-16, 2020-08-23, 2020-08-30, 2020-09-06, 2020-09-13, 2020-09-20,\n",
       "       2020-09-27, 2020-10-04, 2020-10-11, 2020-10-18, 2020-10-25, 2020-11-01,\n",
       "       2020-11-08, 2020-11-15, 2020-11-22, 2020-11-29, 2020-12-06, 2020-12-13,\n",
       "       2020-12-20, 2020-12-27],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions for these days are made\n",
    "X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2400x1600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test_values = [float(i) for i in y_test]\n",
    "# The Prices of Bitcoin over time\n",
    "plt.figure(figsize=(30, 20), dpi=80, facecolor = 'w', edgecolor = 'k')\n",
    "plt.plot(X_test.index,predicted_price, color='blue', label='LSTM Predictions')\n",
    "plt.plot(X_test.index,y_test_values, color='red', label='Real BTC Price')\n",
    "plt.title('BTC Prices Weekly', fontsize = 40)\n",
    "plt.xlabel('Time', fontsize=40)\n",
    "plt.ylabel('BTC Price(USD)', fontsize = 40)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the model without google trends as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous predicted prices were for feaures with google trends, now predicting for without google trends \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler_second = MinMaxScaler()\n",
    "min_max_scaler_y_second = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.7       ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.93333333]],\n",
       "\n",
       "       [[0.09090909, 0.        , 0.13333333]],\n",
       "\n",
       "       [[0.09090909, 0.        , 0.36666667]],\n",
       "\n",
       "       [[0.09090909, 0.        , 0.6       ]],\n",
       "\n",
       "       [[0.09090909, 0.        , 0.83333333]],\n",
       "\n",
       "       [[0.18181818, 0.        , 0.13333333]],\n",
       "\n",
       "       [[0.18181818, 0.        , 0.36666667]],\n",
       "\n",
       "       [[0.18181818, 0.        , 0.6       ]],\n",
       "\n",
       "       [[0.18181818, 0.        , 0.83333333]],\n",
       "\n",
       "       [[0.27272727, 0.        , 0.03333333]],\n",
       "\n",
       "       [[0.27272727, 0.        , 0.26666667]],\n",
       "\n",
       "       [[0.27272727, 0.        , 0.5       ]],\n",
       "\n",
       "       [[0.27272727, 0.        , 0.73333333]],\n",
       "\n",
       "       [[0.27272727, 0.        , 0.96666667]],\n",
       "\n",
       "       [[0.36363636, 0.        , 0.2       ]],\n",
       "\n",
       "       [[0.36363636, 0.        , 0.43333333]],\n",
       "\n",
       "       [[0.36363636, 0.        , 0.66666667]],\n",
       "\n",
       "       [[0.36363636, 0.        , 0.9       ]],\n",
       "\n",
       "       [[0.45454545, 0.        , 0.1       ]],\n",
       "\n",
       "       [[0.45454545, 0.        , 0.33333333]],\n",
       "\n",
       "       [[0.45454545, 0.        , 0.56666667]],\n",
       "\n",
       "       [[0.45454545, 0.        , 0.8       ]],\n",
       "\n",
       "       [[0.54545455, 0.        , 0.03333333]],\n",
       "\n",
       "       [[0.54545455, 0.        , 0.26666667]],\n",
       "\n",
       "       [[0.54545455, 0.        , 0.5       ]],\n",
       "\n",
       "       [[0.54545455, 0.        , 0.73333333]],\n",
       "\n",
       "       [[0.54545455, 0.        , 0.96666667]],\n",
       "\n",
       "       [[0.63636364, 0.        , 0.16666667]],\n",
       "\n",
       "       [[0.63636364, 0.        , 0.4       ]],\n",
       "\n",
       "       [[0.63636364, 0.        , 0.63333333]],\n",
       "\n",
       "       [[0.63636364, 0.        , 0.86666667]],\n",
       "\n",
       "       [[0.72727273, 0.        , 0.06666667]],\n",
       "\n",
       "       [[0.72727273, 0.        , 0.3       ]],\n",
       "\n",
       "       [[0.72727273, 0.        , 0.53333333]],\n",
       "\n",
       "       [[0.72727273, 0.        , 0.76666667]],\n",
       "\n",
       "       [[0.81818182, 0.        , 0.        ]],\n",
       "\n",
       "       [[0.81818182, 0.        , 0.23333333]],\n",
       "\n",
       "       [[0.81818182, 0.        , 0.46666667]],\n",
       "\n",
       "       [[0.81818182, 0.        , 0.7       ]],\n",
       "\n",
       "       [[0.81818182, 0.        , 0.93333333]],\n",
       "\n",
       "       [[0.90909091, 0.        , 0.13333333]],\n",
       "\n",
       "       [[0.90909091, 0.        , 0.36666667]],\n",
       "\n",
       "       [[0.90909091, 0.        , 0.6       ]],\n",
       "\n",
       "       [[0.90909091, 0.        , 0.83333333]],\n",
       "\n",
       "       [[1.        , 0.        , 0.06666667]],\n",
       "\n",
       "       [[1.        , 0.        , 0.3       ]],\n",
       "\n",
       "       [[1.        , 0.        , 0.53333333]],\n",
       "\n",
       "       [[1.        , 0.        , 0.76666667]],\n",
       "\n",
       "       [[1.        , 0.        , 1.        ]],\n",
       "\n",
       "       [[0.        , 0.5       , 0.2       ]],\n",
       "\n",
       "       [[0.        , 0.5       , 0.43333333]],\n",
       "\n",
       "       [[0.        , 0.5       , 0.66666667]],\n",
       "\n",
       "       [[0.        , 0.5       , 0.9       ]],\n",
       "\n",
       "       [[0.09090909, 0.5       , 0.1       ]],\n",
       "\n",
       "       [[0.09090909, 0.5       , 0.33333333]],\n",
       "\n",
       "       [[0.09090909, 0.5       , 0.56666667]],\n",
       "\n",
       "       [[0.09090909, 0.5       , 0.8       ]],\n",
       "\n",
       "       [[0.18181818, 0.5       , 0.1       ]],\n",
       "\n",
       "       [[0.18181818, 0.5       , 0.33333333]],\n",
       "\n",
       "       [[0.18181818, 0.5       , 0.56666667]],\n",
       "\n",
       "       [[0.18181818, 0.5       , 0.8       ]],\n",
       "\n",
       "       [[0.27272727, 0.5       , 0.        ]],\n",
       "\n",
       "       [[0.27272727, 0.5       , 0.23333333]],\n",
       "\n",
       "       [[0.27272727, 0.5       , 0.46666667]],\n",
       "\n",
       "       [[0.27272727, 0.5       , 0.7       ]],\n",
       "\n",
       "       [[0.27272727, 0.5       , 0.93333333]],\n",
       "\n",
       "       [[0.36363636, 0.5       , 0.16666667]],\n",
       "\n",
       "       [[0.36363636, 0.5       , 0.4       ]],\n",
       "\n",
       "       [[0.36363636, 0.5       , 0.63333333]],\n",
       "\n",
       "       [[0.36363636, 0.5       , 0.86666667]],\n",
       "\n",
       "       [[0.45454545, 0.5       , 0.06666667]],\n",
       "\n",
       "       [[0.45454545, 0.5       , 0.3       ]],\n",
       "\n",
       "       [[0.45454545, 0.5       , 0.53333333]],\n",
       "\n",
       "       [[0.45454545, 0.5       , 0.76666667]],\n",
       "\n",
       "       [[0.54545455, 0.5       , 0.        ]],\n",
       "\n",
       "       [[0.54545455, 0.5       , 0.23333333]],\n",
       "\n",
       "       [[0.54545455, 0.5       , 0.46666667]],\n",
       "\n",
       "       [[0.54545455, 0.5       , 0.7       ]],\n",
       "\n",
       "       [[0.54545455, 0.5       , 0.93333333]],\n",
       "\n",
       "       [[0.63636364, 0.5       , 0.13333333]],\n",
       "\n",
       "       [[0.63636364, 0.5       , 0.36666667]],\n",
       "\n",
       "       [[0.63636364, 0.5       , 0.6       ]],\n",
       "\n",
       "       [[0.63636364, 0.5       , 0.83333333]],\n",
       "\n",
       "       [[0.72727273, 0.5       , 0.03333333]],\n",
       "\n",
       "       [[0.72727273, 0.5       , 0.26666667]],\n",
       "\n",
       "       [[0.72727273, 0.5       , 0.5       ]],\n",
       "\n",
       "       [[0.72727273, 0.5       , 0.73333333]],\n",
       "\n",
       "       [[0.72727273, 0.5       , 0.96666667]],\n",
       "\n",
       "       [[0.81818182, 0.5       , 0.2       ]],\n",
       "\n",
       "       [[0.81818182, 0.5       , 0.43333333]],\n",
       "\n",
       "       [[0.81818182, 0.5       , 0.66666667]],\n",
       "\n",
       "       [[0.81818182, 0.5       , 0.9       ]],\n",
       "\n",
       "       [[0.90909091, 0.5       , 0.1       ]],\n",
       "\n",
       "       [[0.90909091, 0.5       , 0.33333333]],\n",
       "\n",
       "       [[0.90909091, 0.5       , 0.56666667]],\n",
       "\n",
       "       [[0.90909091, 0.5       , 0.8       ]],\n",
       "\n",
       "       [[1.        , 0.5       , 0.03333333]],\n",
       "\n",
       "       [[1.        , 0.5       , 0.26666667]],\n",
       "\n",
       "       [[1.        , 0.5       , 0.5       ]],\n",
       "\n",
       "       [[1.        , 0.5       , 0.73333333]],\n",
       "\n",
       "       [[1.        , 0.5       , 0.96666667]],\n",
       "\n",
       "       [[0.        , 1.        , 0.16666667]],\n",
       "\n",
       "       [[0.        , 1.        , 0.4       ]],\n",
       "\n",
       "       [[0.        , 1.        , 0.63333333]],\n",
       "\n",
       "       [[0.        , 1.        , 0.86666667]],\n",
       "\n",
       "       [[0.09090909, 1.        , 0.06666667]],\n",
       "\n",
       "       [[0.09090909, 1.        , 0.3       ]],\n",
       "\n",
       "       [[0.09090909, 1.        , 0.53333333]],\n",
       "\n",
       "       [[0.09090909, 1.        , 0.76666667]],\n",
       "\n",
       "       [[0.18181818, 1.        , 0.06666667]],\n",
       "\n",
       "       [[0.18181818, 1.        , 0.3       ]],\n",
       "\n",
       "       [[0.18181818, 1.        , 0.53333333]],\n",
       "\n",
       "       [[0.18181818, 1.        , 0.76666667]],\n",
       "\n",
       "       [[0.18181818, 1.        , 1.        ]],\n",
       "\n",
       "       [[0.27272727, 1.        , 0.2       ]],\n",
       "\n",
       "       [[0.27272727, 1.        , 0.43333333]],\n",
       "\n",
       "       [[0.27272727, 1.        , 0.66666667]],\n",
       "\n",
       "       [[0.27272727, 1.        , 0.9       ]],\n",
       "\n",
       "       [[0.36363636, 1.        , 0.13333333]],\n",
       "\n",
       "       [[0.36363636, 1.        , 0.36666667]],\n",
       "\n",
       "       [[0.36363636, 1.        , 0.6       ]],\n",
       "\n",
       "       [[0.36363636, 1.        , 0.83333333]],\n",
       "\n",
       "       [[0.45454545, 1.        , 0.03333333]],\n",
       "\n",
       "       [[0.45454545, 1.        , 0.26666667]],\n",
       "\n",
       "       [[0.45454545, 1.        , 0.5       ]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turning input X_Train to appropriate shape for LSTM    ((n_samples, time_steps, features))\n",
    "x_training_set = X_train_second.values\n",
    "x_training_set = x_training_set[0:len(x_training_set)]\n",
    "\n",
    "x_training_set = min_max_scaler_second.fit_transform(x_training_set)\n",
    "x_training_set = x_training_set.reshape((x_training_set.shape[0], 1, x_training_set.shape[1]))\n",
    "x_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train_second)\n",
    "list(y_train['Mean Prices'])\n",
    "y_train_values = [float(i) for i in y_train['Mean Prices']]\n",
    "y_train_values = np.array(y_train_values)\n",
    "y_train_values = np.array(y_train_values)\n",
    "\n",
    "y_train_values = np.reshape(y_train_values, (len(y_train_values), 1))\n",
    "y_training_set = min_max_scaler_y_second.fit_transform(y_train_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "26/26 [==============================] - 2s 1ms/step - loss: 0.1341\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0382\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0371\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0356\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0390\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0288\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0387\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0360\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0339\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0620\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0334\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0386\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0333\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0413\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 0s 997us/step - loss: 0.0530\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0337\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0406\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0408\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0370\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0416\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0307\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0332\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0374\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0429\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0409\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0384\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 0s 917us/step - loss: 0.0392\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0327\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0411\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0365\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0460\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0422\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0314\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0362\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0456\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0424\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0300\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0371\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0321\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0446\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0537\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0342\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0315\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0414\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0416\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0422\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0452\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0441\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0333\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0388\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0336\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0393\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 0s 858us/step - loss: 0.0482\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0503\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0451\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0277\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 0s 917us/step - loss: 0.0317\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 0s 997us/step - loss: 0.0348\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0371\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0352\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0364\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0475\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 0s 909us/step - loss: 0.0371\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0369\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0394\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0377\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0351\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0399\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0358\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0346\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0346\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 0s 997us/step - loss: 0.0322\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0346\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0355\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0206\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0434\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0330\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0325\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0277\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0271\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0236\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0308\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0418\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0263\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0326\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0340\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0221\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0205\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0283\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0250\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0251\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0213\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0256\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0321\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0270\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0285\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0299\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0299\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 918us/step - loss: 0.0194\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 0s 917us/step - loss: 0.0297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a99ab31190>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the input layer and the LSTM layer\n",
    "regressor.add(LSTM(units = 50, input_shape=(x_training_set.shape[1], x_training_set.shape[2])))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Using the training set to train the model\n",
    "regressor.fit(x_training_set, y_training_set, batch_size = 5, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.45454545, 1.        , 0.73333333]],\n",
       "\n",
       "       [[0.45454545, 1.        , 0.96666667]],\n",
       "\n",
       "       [[0.54545455, 1.        , 0.2       ]],\n",
       "\n",
       "       [[0.54545455, 1.        , 0.43333333]],\n",
       "\n",
       "       [[0.54545455, 1.        , 0.66666667]],\n",
       "\n",
       "       [[0.54545455, 1.        , 0.9       ]],\n",
       "\n",
       "       [[0.63636364, 1.        , 0.1       ]],\n",
       "\n",
       "       [[0.63636364, 1.        , 0.33333333]],\n",
       "\n",
       "       [[0.63636364, 1.        , 0.56666667]],\n",
       "\n",
       "       [[0.63636364, 1.        , 0.8       ]],\n",
       "\n",
       "       [[0.72727273, 1.        , 0.        ]],\n",
       "\n",
       "       [[0.72727273, 1.        , 0.23333333]],\n",
       "\n",
       "       [[0.72727273, 1.        , 0.46666667]],\n",
       "\n",
       "       [[0.72727273, 1.        , 0.7       ]],\n",
       "\n",
       "       [[0.72727273, 1.        , 0.93333333]],\n",
       "\n",
       "       [[0.81818182, 1.        , 0.16666667]],\n",
       "\n",
       "       [[0.81818182, 1.        , 0.4       ]],\n",
       "\n",
       "       [[0.81818182, 1.        , 0.63333333]],\n",
       "\n",
       "       [[0.81818182, 1.        , 0.86666667]],\n",
       "\n",
       "       [[0.90909091, 1.        , 0.06666667]],\n",
       "\n",
       "       [[0.90909091, 1.        , 0.3       ]],\n",
       "\n",
       "       [[0.90909091, 1.        , 0.53333333]],\n",
       "\n",
       "       [[0.90909091, 1.        , 0.76666667]],\n",
       "\n",
       "       [[1.        , 1.        , 0.        ]],\n",
       "\n",
       "       [[1.        , 1.        , 0.23333333]],\n",
       "\n",
       "       [[1.        , 1.        , 0.46666667]],\n",
       "\n",
       "       [[1.        , 1.        , 0.7       ]],\n",
       "\n",
       "       [[1.        , 1.        , 0.93333333]],\n",
       "\n",
       "       [[0.        , 1.5       , 0.13333333]],\n",
       "\n",
       "       [[0.        , 1.5       , 0.36666667]],\n",
       "\n",
       "       [[0.        , 1.5       , 0.6       ]],\n",
       "\n",
       "       [[0.        , 1.5       , 0.83333333]],\n",
       "\n",
       "       [[0.09090909, 1.5       , 0.03333333]],\n",
       "\n",
       "       [[0.09090909, 1.5       , 0.26666667]],\n",
       "\n",
       "       [[0.09090909, 1.5       , 0.5       ]],\n",
       "\n",
       "       [[0.09090909, 1.5       , 0.73333333]],\n",
       "\n",
       "       [[0.18181818, 1.5       , 0.        ]],\n",
       "\n",
       "       [[0.18181818, 1.5       , 0.23333333]],\n",
       "\n",
       "       [[0.18181818, 1.5       , 0.46666667]],\n",
       "\n",
       "       [[0.18181818, 1.5       , 0.7       ]],\n",
       "\n",
       "       [[0.18181818, 1.5       , 0.93333333]],\n",
       "\n",
       "       [[0.27272727, 1.5       , 0.13333333]],\n",
       "\n",
       "       [[0.27272727, 1.5       , 0.36666667]],\n",
       "\n",
       "       [[0.27272727, 1.5       , 0.6       ]],\n",
       "\n",
       "       [[0.27272727, 1.5       , 0.83333333]],\n",
       "\n",
       "       [[0.36363636, 1.5       , 0.06666667]],\n",
       "\n",
       "       [[0.36363636, 1.5       , 0.3       ]],\n",
       "\n",
       "       [[0.36363636, 1.5       , 0.53333333]],\n",
       "\n",
       "       [[0.36363636, 1.5       , 0.76666667]],\n",
       "\n",
       "       [[0.36363636, 1.5       , 1.        ]],\n",
       "\n",
       "       [[0.45454545, 1.5       , 0.2       ]],\n",
       "\n",
       "       [[0.45454545, 1.5       , 0.43333333]],\n",
       "\n",
       "       [[0.45454545, 1.5       , 0.66666667]],\n",
       "\n",
       "       [[0.45454545, 1.5       , 0.9       ]],\n",
       "\n",
       "       [[0.54545455, 1.5       , 0.13333333]],\n",
       "\n",
       "       [[0.54545455, 1.5       , 0.36666667]],\n",
       "\n",
       "       [[0.54545455, 1.5       , 0.6       ]],\n",
       "\n",
       "       [[0.54545455, 1.5       , 0.83333333]],\n",
       "\n",
       "       [[0.63636364, 1.5       , 0.03333333]],\n",
       "\n",
       "       [[0.63636364, 1.5       , 0.26666667]],\n",
       "\n",
       "       [[0.63636364, 1.5       , 0.5       ]],\n",
       "\n",
       "       [[0.63636364, 1.5       , 0.73333333]],\n",
       "\n",
       "       [[0.63636364, 1.5       , 0.96666667]],\n",
       "\n",
       "       [[0.72727273, 1.5       , 0.16666667]],\n",
       "\n",
       "       [[0.72727273, 1.5       , 0.4       ]],\n",
       "\n",
       "       [[0.72727273, 1.5       , 0.63333333]],\n",
       "\n",
       "       [[0.72727273, 1.5       , 0.86666667]],\n",
       "\n",
       "       [[0.81818182, 1.5       , 0.1       ]],\n",
       "\n",
       "       [[0.81818182, 1.5       , 0.33333333]],\n",
       "\n",
       "       [[0.81818182, 1.5       , 0.56666667]],\n",
       "\n",
       "       [[0.81818182, 1.5       , 0.8       ]],\n",
       "\n",
       "       [[0.90909091, 1.5       , 0.        ]],\n",
       "\n",
       "       [[0.90909091, 1.5       , 0.23333333]],\n",
       "\n",
       "       [[0.90909091, 1.5       , 0.46666667]],\n",
       "\n",
       "       [[0.90909091, 1.5       , 0.7       ]],\n",
       "\n",
       "       [[0.90909091, 1.5       , 0.93333333]],\n",
       "\n",
       "       [[1.        , 1.5       , 0.16666667]],\n",
       "\n",
       "       [[1.        , 1.5       , 0.4       ]],\n",
       "\n",
       "       [[1.        , 1.5       , 0.63333333]],\n",
       "\n",
       "       [[1.        , 1.5       , 0.86666667]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = X_test_second.values\n",
    "\n",
    "# Reshaping and scaling the inputs\n",
    "inputs = np.reshape(test_set, (len(test_set), 3))\n",
    "inputs = min_max_scaler_second.transform(inputs)\n",
    "inputs = inputs.reshape((inputs.shape[0], 1, 3))\n",
    "predicted_price = regressor.predict(inputs)\n",
    "inputs = inputs.reshape((inputs.shape[0], 1, 3))\n",
    "predicted_price = min_max_scaler_y_second.inverse_transform(predicted_price)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4533.5645 ],\n",
       "       [ 4570.9277 ],\n",
       "       [ 4023.982  ],\n",
       "       [ 4115.7456 ],\n",
       "       [ 4193.4053 ],\n",
       "       [ 4256.356  ],\n",
       "       [ 3619.8916 ],\n",
       "       [ 3736.3958 ],\n",
       "       [ 3839.6575 ],\n",
       "       [ 3929.0276 ],\n",
       "       [ 3212.6948 ],\n",
       "       [ 3352.531  ],\n",
       "       [ 3479.9836 ],\n",
       "       [ 3594.362  ],\n",
       "       [ 3695.008  ],\n",
       "       [ 2987.6245 ],\n",
       "       [ 3136.1694 ],\n",
       "       [ 3272.3552 ],\n",
       "       [ 3395.495  ],\n",
       "       [ 2601.5244 ],\n",
       "       [ 2771.6265 ],\n",
       "       [ 2930.1746 ],\n",
       "       [ 3076.4534 ],\n",
       "       [ 2242.1777 ],\n",
       "       [ 2431.0183 ],\n",
       "       [ 2608.994  ],\n",
       "       [ 2775.3657 ],\n",
       "       [ 2929.4211 ],\n",
       "       [  944.8216 ],\n",
       "       [  923.08746],\n",
       "       [  896.8765 ],\n",
       "       [  866.0027 ],\n",
       "       [  393.84433],\n",
       "       [  391.439  ],\n",
       "       [  385.2249 ],\n",
       "       [  375.00635],\n",
       "       [ -146.37189],\n",
       "       [ -131.3916 ],\n",
       "       [ -119.64361],\n",
       "       [ -111.34539],\n",
       "       [ -106.75644],\n",
       "       [ -647.28076],\n",
       "       [ -617.51654],\n",
       "       [ -590.61664],\n",
       "       [ -566.85004],\n",
       "       [-1149.881  ],\n",
       "       [-1103.4373 ],\n",
       "       [-1059.3506 ],\n",
       "       [-1017.89514],\n",
       "       [ -979.38995],\n",
       "       [-1587.2079 ],\n",
       "       [-1526.5284 ],\n",
       "       [-1467.9722 ],\n",
       "       [-1411.8552 ],\n",
       "       [-2053.023  ],\n",
       "       [-1976.9519 ],\n",
       "       [-1902.5724 ],\n",
       "       [-1830.2035 ],\n",
       "       [-2520.0989 ],\n",
       "       [-2429.2898 ],\n",
       "       [-2339.7334 ],\n",
       "       [-2251.7427 ],\n",
       "       [-2165.6719 ],\n",
       "       [-2874.374  ],\n",
       "       [-2770.569  ],\n",
       "       [-2667.9204 ],\n",
       "       [-2566.768  ],\n",
       "       [-3294.8394 ],\n",
       "       [-3177.7642 ],\n",
       "       [-3061.5093 ],\n",
       "       [-2946.4104 ],\n",
       "       [-3721.9648 ],\n",
       "       [-3592.3147 ],\n",
       "       [-3463.1309 ],\n",
       "       [-3334.7437 ],\n",
       "       [-3207.521  ],\n",
       "       [-3978.7915 ],\n",
       "       [-3837.551  ],\n",
       "       [-3696.8232 ],\n",
       "       [-3556.967  ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11214.937845316701"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test,predicted_price,squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10072.232338637496"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test,predicted_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2400x1600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test_values = [float(i) for i in y_test]\n",
    "# The Prices of Bitcoin over time\n",
    "plt.figure(figsize=(30, 20), dpi=80, facecolor = 'w', edgecolor = 'k')\n",
    "plt.plot(X_test.index,predicted_price, color='blue', label='LSTM Predictions')\n",
    "plt.plot(X_test.index,y_test_values, color='red', label='Real BTC Price')\n",
    "plt.title('BTC Prices Weekly', fontsize = 40)\n",
    "plt.xlabel('Time', fontsize=40)\n",
    "plt.ylabel('BTC Price(USD)', fontsize = 40)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-642bf5ccaf3c>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dummy_x['Mean Prices'] = y_train['Mean Prices']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Hafta', ylabel='Hafta'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To see the correlation values\n",
    "dummy_x = X_train\n",
    "dummy_x['Mean Prices'] = y_train['Mean Prices']\n",
    "sns.heatmap(dummy_x.corr(),annot = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A huge difference could be seen by predicting with google trends and predicting without google trends\n",
    "\n",
    "Now prices will be shifted and yesterdays price will be used as a feature on the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now k fold cross validation will be used, using rolling forecast origin and sliding window methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling Forecast Origin Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: [2300, 5000, 6000, 9000, 2323, 5644, 7878, 4444, 2342, 3000, 3500, 5000, 7000, 8000]\n",
      "Test Set: [9000, 8000]\n",
      "\n",
      "\n",
      "Count[1]\n",
      "Train:\t[[2300, 5000, 6000]]\n",
      "Val:\t[[9000, 2323]]\n",
      "-------\n",
      "Count[2]\n",
      "Train:\t[[2300, 5000, 6000, 9000]]\n",
      "Val:\t[[2323, 5644]]\n",
      "-------\n",
      "Count[3]\n",
      "Train:\t[[2300, 5000, 6000, 9000, 2323]]\n",
      "Val:\t[[5644, 7878]]\n",
      "-------\n",
      "Count[4]\n",
      "Train:\t[[2300, 5000, 6000, 9000, 2323, 5644]]\n",
      "Val:\t[[7878, 4444]]\n",
      "-------\n",
      "Count[5]\n",
      "Train:\t[[2300, 5000, 6000, 9000, 2323, 5644, 7878]]\n",
      "Val:\t[[4444, 2342]]\n",
      "-------\n",
      "Count[6]\n",
      "Train:\t[[2300, 5000, 6000, 9000, 2323, 5644, 7878, 4444]]\n",
      "Val:\t[[2342, 3000]]\n",
      "-------\n",
      "Count[7]\n",
      "Train:\t[[2300, 5000, 6000, 9000, 2323, 5644, 7878, 4444, 2342]]\n",
      "Val:\t[[3000, 3500]]\n",
      "-------\n",
      "Count[8]\n",
      "Train:\t[[2300, 5000, 6000, 9000, 2323, 5644, 7878, 4444, 2342, 3000]]\n",
      "Val:\t[[3500, 5000]]\n",
      "-------\n",
      "Count[9]\n",
      "Train:\t[[2300, 5000, 6000, 9000, 2323, 5644, 7878, 4444, 2342, 3000, 3500]]\n",
      "Val:\t[[5000, 7000]]\n",
      "-------\n",
      "Count[10]\n",
      "Train:\t[[2300, 5000, 6000, 9000, 2323, 5644, 7878, 4444, 2342, 3000, 3500, 5000]]\n",
      "Val:\t[[7000, 8000]]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "def roll_forecast_orig(train,min_train_size,horizon):\n",
    "    for i in range(len(train)-min_train_size-horizon+1):\n",
    "        split_train = train[:min_train_size+i]\n",
    "        split_val = train[min_train_size+i:min_train_size+i+horizon]\n",
    "        yield split_train,split_val\n",
    "        \n",
    "# Simple example implementation\n",
    "series_ex = [2300,5000,6000,9000,2323,5644,7878,4444,2342,3000,3500,5000,7000,8000,9000,8000]\n",
    "test=series_ex[-2:]\n",
    "train = series_ex[:-2]\n",
    "\n",
    "rolling_data = roll_forecast_orig(train,min_train_size=3,horizon=2)\n",
    "\n",
    "print('Training Set: {0}'.format(train))\n",
    "print('Test Set: {0}'.format(test))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "i= 0\n",
    "for train_c, val_c in rolling_data:\n",
    "    print(f'Count[{i+1}]')\n",
    "    print(f'Train:\\t[{train_c}]')\n",
    "    print(f'Val:\\t[{val_c}]')\n",
    "    print('-------')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data train shape (126, 3)\n",
      "Data test shape (80, 3)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data to train and test based on the prediction day we want\n",
    "\n",
    "date_string = \"21 June, 2019\"\n",
    "#possible parameter %H:%M:%S\n",
    "dt_onject_split = dt.datetime.strptime(date_string, \"%d %B, %Y\").date()\n",
    "data_train = trends_weekly_df.loc[trends_weekly_df.index <= dt_onject_split].copy()\n",
    "data_test = trends_weekly_df.loc[trends_weekly_df.index > dt_onject_split].copy()\n",
    "\n",
    "print(\"Data train shape\",data_train.shape)\n",
    "print(\"Data test shape\",data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hafta</th>\n",
       "      <th>Bitcoin: (DÃ¼nya Genelinde)</th>\n",
       "      <th>Mean Prices</th>\n",
       "      <th>Hafta Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-22</th>\n",
       "      <td>5</td>\n",
       "      <td>1001.984641</td>\n",
       "      <td>2017-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-29</th>\n",
       "      <td>5</td>\n",
       "      <td>858.134107</td>\n",
       "      <td>2017-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-05</th>\n",
       "      <td>5</td>\n",
       "      <td>870.803750</td>\n",
       "      <td>2017-02-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-12</th>\n",
       "      <td>5</td>\n",
       "      <td>914.227679</td>\n",
       "      <td>2017-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-19</th>\n",
       "      <td>5</td>\n",
       "      <td>963.755536</td>\n",
       "      <td>2017-02-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-19</th>\n",
       "      <td>14</td>\n",
       "      <td>6647.705639</td>\n",
       "      <td>2019-05-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-26</th>\n",
       "      <td>16</td>\n",
       "      <td>7800.502763</td>\n",
       "      <td>2019-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-02</th>\n",
       "      <td>13</td>\n",
       "      <td>8167.206495</td>\n",
       "      <td>2019-06-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-09</th>\n",
       "      <td>12</td>\n",
       "      <td>8571.252718</td>\n",
       "      <td>2019-06-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-16</th>\n",
       "      <td>18</td>\n",
       "      <td>7814.483712</td>\n",
       "      <td>2019-06-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Hafta       Bitcoin: (DÃ¼nya Genelinde)  Mean Prices Hafta Feature\n",
       "2017-01-22                           5  1001.984641    2017-01-22\n",
       "2017-01-29                           5   858.134107    2017-01-29\n",
       "2017-02-05                           5   870.803750    2017-02-05\n",
       "2017-02-12                           5   914.227679    2017-02-12\n",
       "2017-02-19                           5   963.755536    2017-02-19\n",
       "...                                ...          ...           ...\n",
       "2019-05-19                          14  6647.705639    2019-05-19\n",
       "2019-05-26                          16  7800.502763    2019-05-26\n",
       "2019-06-02                          13  8167.206495    2019-06-02\n",
       "2019-06-09                          12  8571.252718    2019-06-09\n",
       "2019-06-16                          18  7814.483712    2019-06-16\n",
       "\n",
       "[126 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_with_google_trends_for_df(df, label=None):\n",
    "    \"\"\"\n",
    "    Creates time series features from datetime index\n",
    "    \"\"\"\n",
    "    df['Hafta'] = df.index\n",
    "    df['Hafta'] = pd.to_datetime(df['Hafta'], errors='coerce')\n",
    "    df['month'] = df['Hafta'].dt.month\n",
    "    df['year'] = df['Hafta'].dt.year\n",
    "    df['dayofmonth'] = df['Hafta'].dt.day\n",
    "\n",
    "    \n",
    "    X = df[['month','year','dayofmonth','Bitcoin: (DÃ¼nya Genelinde)']]\n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X\n",
    "\n",
    "df_x, df_y = create_features_with_google_trends_for_df(data_train,\"Mean Prices\")\n",
    "df_x_test, df_y_test = create_features_with_google_trends_for_df(data_test,\"Mean Prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hafta</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>Bitcoin: (DÃ¼nya Genelinde)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-22</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-29</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-05</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-12</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-19</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-19</th>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-26</th>\n",
       "      <td>5</td>\n",
       "      <td>2019</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-02</th>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-09</th>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-16</th>\n",
       "      <td>6</td>\n",
       "      <td>2019</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
       "2017-01-22      1  2017          22                           5\n",
       "2017-01-29      1  2017          29                           5\n",
       "2017-02-05      2  2017           5                           5\n",
       "2017-02-12      2  2017          12                           5\n",
       "2017-02-19      2  2017          19                           5\n",
       "...           ...   ...         ...                         ...\n",
       "2019-05-19      5  2019          19                          14\n",
       "2019-05-26      5  2019          26                          16\n",
       "2019-06-02      6  2019           2                          13\n",
       "2019-06-09      6  2019           9                          12\n",
       "2019-06-16      6  2019          16                          18\n",
       "\n",
       "[126 rows x 4 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turning input X_Train to appropriate shape for LSTM    ((n_samples, time_steps, features))\n",
    "x_training_set = df_x\n",
    "x_training_set = df_x[0:len(x_training_set)]\n",
    "\n",
    "y_train = pd.DataFrame(df_y)\n",
    "list(y_train['Mean Prices'])\n",
    "y_train_values = [float(i) for i in y_train['Mean Prices']]\n",
    "y_train_values = np.array(y_train_values)\n",
    "y_train_values = np.array(y_train_values)\n",
    "\n",
    "y_train_values = np.reshape(y_train_values, (len(y_train_values), 1))\n",
    "y_training_set = y_train_values\n",
    "\n",
    "x_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count[1]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5]\n",
      "-------\n",
      "Count[2]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7]\n",
      "-------\n",
      "Count[3]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7]\n",
      "-------\n",
      "Count[4]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6]\n",
      "-------\n",
      "Count[5]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6]\n",
      "-------\n",
      "Count[6]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5]\n",
      "-------\n",
      "Count[7]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5]\n",
      "-------\n",
      "Count[8]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5]\n",
      "-------\n",
      "Count[9]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5]\n",
      "-------\n",
      "Count[10]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6]\n",
      "-------\n",
      "Count[11]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8]\n",
      "-------\n",
      "Count[12]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10]\n",
      "-------\n",
      "Count[13]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12]\n",
      "-------\n",
      "Count[14]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19]\n",
      "-------\n",
      "Count[15]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13]\n",
      "-------\n",
      "Count[16]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13]\n",
      "-------\n",
      "Count[17]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15]\n",
      "-------\n",
      "Count[18]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12]\n",
      "-------\n",
      "Count[19]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12]\n",
      "-------\n",
      "Count[20]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10]\n",
      "-------\n",
      "Count[21]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11]\n",
      "-------\n",
      "Count[22]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13]\n",
      "-------\n",
      "Count[23]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12]\n",
      "-------\n",
      "Count[24]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14]\n",
      "-------\n",
      "Count[25]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14]\n",
      "-------\n",
      "Count[26]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20]\n",
      "-------\n",
      "Count[27]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16]\n",
      "-------\n",
      "Count[28]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17]\n",
      "-------\n",
      "Count[29]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18]\n",
      "-------\n",
      "Count[30]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21]\n",
      "-------\n",
      "Count[31]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16]\n",
      "-------\n",
      "Count[32]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12]\n",
      "-------\n",
      "Count[33]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13]\n",
      "-------\n",
      "Count[34]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19]\n",
      "-------\n",
      "Count[35]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20]\n",
      "-------\n",
      "Count[36]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20]\n",
      "-------\n",
      "Count[37]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26]\n",
      "-------\n",
      "Count[38]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-22     10  2017          22                          20]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28]\n",
      "-------\n",
      "Count[39]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29]\n",
      "-------\n",
      "Count[40]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27]\n",
      "-------\n",
      "Count[41]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61]\n",
      "-------\n",
      "Count[42]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87]\n",
      "-------\n",
      "Count[43]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79]\n",
      "-------\n",
      "Count[44]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90]\n",
      "-------\n",
      "Count[45]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-12-17     12  2017          17                          90\n",
      "2017-12-24     12  2017          24                          62]\n",
      "-------\n",
      "Count[46]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-12-24     12  2017          24                          62\n",
      "2017-12-31     12  2017          31                          46]\n",
      "-------\n",
      "Count[47]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90\n",
      "2017-12-24     12  2017          24                          62]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-12-31     12  2017          31                          46\n",
      "2018-01-07      1  2018           7                          46]\n",
      "-------\n",
      "Count[48]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90\n",
      "2017-12-24     12  2017          24                          62\n",
      "2017-12-31     12  2017          31                          46]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-01-07      1  2018           7                          46\n",
      "2018-01-14      1  2018          14                          56]\n",
      "-------\n",
      "Count[49]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90\n",
      "2017-12-24     12  2017          24                          62\n",
      "2017-12-31     12  2017          31                          46\n",
      "2018-01-07      1  2018           7                          46]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-01-14      1  2018          14                          56\n",
      "2018-01-21      1  2018          21                          38]\n",
      "-------\n",
      "Count[50]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90\n",
      "2017-12-24     12  2017          24                          62\n",
      "2017-12-31     12  2017          31                          46\n",
      "2018-01-07      1  2018           7                          46\n",
      "2018-01-14      1  2018          14                          56]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-01-21      1  2018          21                          38\n",
      "2018-01-28      1  2018          28                          40]\n",
      "-------\n",
      "Count[51]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90\n",
      "2017-12-24     12  2017          24                          62\n",
      "2017-12-31     12  2017          31                          46\n",
      "2018-01-07      1  2018           7                          46\n",
      "2018-01-14      1  2018          14                          56\n",
      "2018-01-21      1  2018          21                          38]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-01-28      1  2018          28                          40\n",
      "2018-02-04      2  2018           4                          45]\n",
      "-------\n",
      "Count[52]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90\n",
      "2017-12-24     12  2017          24                          62\n",
      "2017-12-31     12  2017          31                          46\n",
      "2018-01-07      1  2018           7                          46\n",
      "2018-01-14      1  2018          14                          56\n",
      "2018-01-21      1  2018          21                          38\n",
      "2018-01-28      1  2018          28                          40]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-02-04      2  2018           4                          45\n",
      "2018-02-11      2  2018          11                          28]\n",
      "-------\n",
      "Count[53]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90\n",
      "2017-12-24     12  2017          24                          62\n",
      "2017-12-31     12  2017          31                          46\n",
      "2018-01-07      1  2018           7                          46\n",
      "2018-01-14      1  2018          14                          56\n",
      "2018-01-21      1  2018          21                          38\n",
      "2018-01-28      1  2018          28                          40\n",
      "2018-02-04      2  2018           4                          45]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-02-11      2  2018          11                          28\n",
      "2018-02-18      2  2018          18                          24]\n",
      "-------\n",
      "Count[54]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90\n",
      "2017-12-24     12  2017          24                          62\n",
      "2017-12-31     12  2017          31                          46\n",
      "2018-01-07      1  2018           7                          46\n",
      "2018-01-14      1  2018          14                          56\n",
      "2018-01-21      1  2018          21                          38\n",
      "2018-01-28      1  2018          28                          40\n",
      "2018-02-04      2  2018           4                          45\n",
      "2018-02-11      2  2018          11                          28]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-02-18      2  2018          18                          24\n",
      "2018-02-25      2  2018          25                          20]\n",
      "-------\n",
      "Count[55]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90\n",
      "2017-12-24     12  2017          24                          62\n",
      "2017-12-31     12  2017          31                          46\n",
      "2018-01-07      1  2018           7                          46\n",
      "2018-01-14      1  2018          14                          56\n",
      "2018-01-21      1  2018          21                          38\n",
      "2018-01-28      1  2018          28                          40\n",
      "2018-02-04      2  2018           4                          45\n",
      "2018-02-11      2  2018          11                          28\n",
      "2018-02-18      2  2018          18                          24]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-02-25      2  2018          25                          20\n",
      "2018-03-04      3  2018           4                          20]\n",
      "-------\n",
      "Count[56]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90\n",
      "2017-12-24     12  2017          24                          62\n",
      "2017-12-31     12  2017          31                          46\n",
      "2018-01-07      1  2018           7                          46\n",
      "2018-01-14      1  2018          14                          56\n",
      "2018-01-21      1  2018          21                          38\n",
      "2018-01-28      1  2018          28                          40\n",
      "2018-02-04      2  2018           4                          45\n",
      "2018-02-11      2  2018          11                          28\n",
      "2018-02-18      2  2018          18                          24\n",
      "2018-02-25      2  2018          25                          20]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-03-04      3  2018           4                          20\n",
      "2018-03-11      3  2018          11                          19]\n",
      "-------\n",
      "Count[57]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90\n",
      "2017-12-24     12  2017          24                          62\n",
      "2017-12-31     12  2017          31                          46\n",
      "2018-01-07      1  2018           7                          46\n",
      "2018-01-14      1  2018          14                          56\n",
      "2018-01-21      1  2018          21                          38\n",
      "2018-01-28      1  2018          28                          40\n",
      "2018-02-04      2  2018           4                          45\n",
      "2018-02-11      2  2018          11                          28\n",
      "2018-02-18      2  2018          18                          24\n",
      "2018-02-25      2  2018          25                          20\n",
      "2018-03-04      3  2018           4                          20]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-03-11      3  2018          11                          19\n",
      "2018-03-18      3  2018          18                          18]\n",
      "-------\n",
      "Count[58]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "2017-02-26      2  2017          26                           7\n",
      "2017-03-05      3  2017           5                           7\n",
      "2017-03-12      3  2017          12                           6\n",
      "2017-03-19      3  2017          19                           6\n",
      "2017-03-26      3  2017          26                           5\n",
      "2017-04-02      4  2017           2                           5\n",
      "2017-04-09      4  2017           9                           5\n",
      "2017-04-16      4  2017          16                           5\n",
      "2017-04-23      4  2017          23                           6\n",
      "2017-04-30      4  2017          30                           8\n",
      "2017-05-07      5  2017           7                          10\n",
      "2017-05-14      5  2017          14                          12\n",
      "2017-05-21      5  2017          21                          19\n",
      "2017-05-28      5  2017          28                          13\n",
      "2017-06-04      6  2017           4                          13\n",
      "2017-06-11      6  2017          11                          15\n",
      "2017-06-18      6  2017          18                          12\n",
      "2017-06-25      6  2017          25                          12\n",
      "2017-07-02      7  2017           2                          10\n",
      "2017-07-09      7  2017           9                          11\n",
      "2017-07-16      7  2017          16                          13\n",
      "2017-07-23      7  2017          23                          12\n",
      "2017-07-30      7  2017          30                          14\n",
      "2017-08-06      8  2017           6                          14\n",
      "2017-08-13      8  2017          13                          20\n",
      "2017-08-20      8  2017          20                          16\n",
      "2017-08-27      8  2017          27                          17\n",
      "2017-09-03      9  2017           3                          18\n",
      "2017-09-10      9  2017          10                          21\n",
      "2017-09-17      9  2017          17                          16\n",
      "2017-09-24      9  2017          24                          12\n",
      "2017-10-01     10  2017           1                          13\n",
      "2017-10-08     10  2017           8                          19\n",
      "2017-10-15     10  2017          15                          20\n",
      "2017-10-22     10  2017          22                          20\n",
      "2017-10-29     10  2017          29                          26\n",
      "2017-11-05     11  2017           5                          28\n",
      "2017-11-12     11  2017          12                          29\n",
      "2017-11-19     11  2017          19                          27\n",
      "2017-11-26     11  2017          26                          61\n",
      "2017-12-03     12  2017           3                          87\n",
      "2017-12-10     12  2017          10                          79\n",
      "2017-12-17     12  2017          17                          90\n",
      "2017-12-24     12  2017          24                          62\n",
      "2017-12-31     12  2017          31                          46\n",
      "2018-01-07      1  2018           7                          46\n",
      "2018-01-14      1  2018          14                          56\n",
      "2018-01-21      1  2018          21                          38\n",
      "2018-01-28      1  2018          28                          40\n",
      "2018-02-04      2  2018           4                          45\n",
      "2018-02-11      2  2018          11                          28\n",
      "2018-02-18      2  2018          18                          24\n",
      "2018-02-25      2  2018          25                          20\n",
      "2018-03-04      3  2018           4                          20\n",
      "2018-03-11      3  2018          11                          19]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-03-18      3  2018          18                          18\n",
      "2018-03-25      3  2018          25                          18]\n",
      "-------\n",
      "Count[59]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-02-18      2  2018          18                          24\n",
      "2018-02-25      2  2018          25                          20\n",
      "2018-03-04      3  2018           4                          20\n",
      "2018-03-11      3  2018          11                          19\n",
      "2018-03-18      3  2018          18                          18\n",
      "\n",
      "[61 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-03-25      3  2018          25                          18\n",
      "2018-04-01      4  2018           1                          17]\n",
      "-------\n",
      "Count[60]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-02-25      2  2018          25                          20\n",
      "2018-03-04      3  2018           4                          20\n",
      "2018-03-11      3  2018          11                          19\n",
      "2018-03-18      3  2018          18                          18\n",
      "2018-03-25      3  2018          25                          18\n",
      "\n",
      "[62 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-04-01      4  2018           1                          17\n",
      "2018-04-08      4  2018           8                          16]\n",
      "-------\n",
      "Count[61]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-03-04      3  2018           4                          20\n",
      "2018-03-11      3  2018          11                          19\n",
      "2018-03-18      3  2018          18                          18\n",
      "2018-03-25      3  2018          25                          18\n",
      "2018-04-01      4  2018           1                          17\n",
      "\n",
      "[63 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-04-08      4  2018           8                          16\n",
      "2018-04-15      4  2018          15                          14]\n",
      "-------\n",
      "Count[62]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-03-11      3  2018          11                          19\n",
      "2018-03-18      3  2018          18                          18\n",
      "2018-03-25      3  2018          25                          18\n",
      "2018-04-01      4  2018           1                          17\n",
      "2018-04-08      4  2018           8                          16\n",
      "\n",
      "[64 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-04-15      4  2018          15                          14\n",
      "2018-04-22      4  2018          22                          15]\n",
      "-------\n",
      "Count[63]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-03-18      3  2018          18                          18\n",
      "2018-03-25      3  2018          25                          18\n",
      "2018-04-01      4  2018           1                          17\n",
      "2018-04-08      4  2018           8                          16\n",
      "2018-04-15      4  2018          15                          14\n",
      "\n",
      "[65 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-04-22      4  2018          22                          15\n",
      "2018-04-29      4  2018          29                          13]\n",
      "-------\n",
      "Count[64]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-03-25      3  2018          25                          18\n",
      "2018-04-01      4  2018           1                          17\n",
      "2018-04-08      4  2018           8                          16\n",
      "2018-04-15      4  2018          15                          14\n",
      "2018-04-22      4  2018          22                          15\n",
      "\n",
      "[66 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-04-29      4  2018          29                          13\n",
      "2018-05-06      5  2018           6                          13]\n",
      "-------\n",
      "Count[65]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-04-01      4  2018           1                          17\n",
      "2018-04-08      4  2018           8                          16\n",
      "2018-04-15      4  2018          15                          14\n",
      "2018-04-22      4  2018          22                          15\n",
      "2018-04-29      4  2018          29                          13\n",
      "\n",
      "[67 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-05-06      5  2018           6                          13\n",
      "2018-05-13      5  2018          13                          11]\n",
      "-------\n",
      "Count[66]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-04-08      4  2018           8                          16\n",
      "2018-04-15      4  2018          15                          14\n",
      "2018-04-22      4  2018          22                          15\n",
      "2018-04-29      4  2018          29                          13\n",
      "2018-05-06      5  2018           6                          13\n",
      "\n",
      "[68 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-05-13      5  2018          13                          11\n",
      "2018-05-20      5  2018          20                          11]\n",
      "-------\n",
      "Count[67]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-04-15      4  2018          15                          14\n",
      "2018-04-22      4  2018          22                          15\n",
      "2018-04-29      4  2018          29                          13\n",
      "2018-05-06      5  2018           6                          13\n",
      "2018-05-13      5  2018          13                          11\n",
      "\n",
      "[69 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-05-20      5  2018          20                          11\n",
      "2018-05-27      5  2018          27                          11]\n",
      "-------\n",
      "Count[68]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-04-22      4  2018          22                          15\n",
      "2018-04-29      4  2018          29                          13\n",
      "2018-05-06      5  2018           6                          13\n",
      "2018-05-13      5  2018          13                          11\n",
      "2018-05-20      5  2018          20                          11\n",
      "\n",
      "[70 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-05-27      5  2018          27                          11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-03      6  2018           3                          10]\n",
      "-------\n",
      "Count[69]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-04-29      4  2018          29                          13\n",
      "2018-05-06      5  2018           6                          13\n",
      "2018-05-13      5  2018          13                          11\n",
      "2018-05-20      5  2018          20                          11\n",
      "2018-05-27      5  2018          27                          11\n",
      "\n",
      "[71 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-06-03      6  2018           3                          10\n",
      "2018-06-10      6  2018          10                          13]\n",
      "-------\n",
      "Count[70]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-05-06      5  2018           6                          13\n",
      "2018-05-13      5  2018          13                          11\n",
      "2018-05-20      5  2018          20                          11\n",
      "2018-05-27      5  2018          27                          11\n",
      "2018-06-03      6  2018           3                          10\n",
      "\n",
      "[72 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-06-10      6  2018          10                          13\n",
      "2018-06-17      6  2018          17                          11]\n",
      "-------\n",
      "Count[71]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-05-13      5  2018          13                          11\n",
      "2018-05-20      5  2018          20                          11\n",
      "2018-05-27      5  2018          27                          11\n",
      "2018-06-03      6  2018           3                          10\n",
      "2018-06-10      6  2018          10                          13\n",
      "\n",
      "[73 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-06-17      6  2018          17                          11\n",
      "2018-06-24      6  2018          24                          11]\n",
      "-------\n",
      "Count[72]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-05-20      5  2018          20                          11\n",
      "2018-05-27      5  2018          27                          11\n",
      "2018-06-03      6  2018           3                          10\n",
      "2018-06-10      6  2018          10                          13\n",
      "2018-06-17      6  2018          17                          11\n",
      "\n",
      "[74 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-06-24      6  2018          24                          11\n",
      "2018-07-01      7  2018           1                          10]\n",
      "-------\n",
      "Count[73]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-05-27      5  2018          27                          11\n",
      "2018-06-03      6  2018           3                          10\n",
      "2018-06-10      6  2018          10                          13\n",
      "2018-06-17      6  2018          17                          11\n",
      "2018-06-24      6  2018          24                          11\n",
      "\n",
      "[75 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-07-01      7  2018           1                          10\n",
      "2018-07-08      7  2018           8                           9]\n",
      "-------\n",
      "Count[74]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-06-03      6  2018           3                          10\n",
      "2018-06-10      6  2018          10                          13\n",
      "2018-06-17      6  2018          17                          11\n",
      "2018-06-24      6  2018          24                          11\n",
      "2018-07-01      7  2018           1                          10\n",
      "\n",
      "[76 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-07-08      7  2018           8                           9\n",
      "2018-07-15      7  2018          15                          11]\n",
      "-------\n",
      "Count[75]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-06-10      6  2018          10                          13\n",
      "2018-06-17      6  2018          17                          11\n",
      "2018-06-24      6  2018          24                          11\n",
      "2018-07-01      7  2018           1                          10\n",
      "2018-07-08      7  2018           8                           9\n",
      "\n",
      "[77 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-07-15      7  2018          15                          11\n",
      "2018-07-22      7  2018          22                          12]\n",
      "-------\n",
      "Count[76]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-06-17      6  2018          17                          11\n",
      "2018-06-24      6  2018          24                          11\n",
      "2018-07-01      7  2018           1                          10\n",
      "2018-07-08      7  2018           8                           9\n",
      "2018-07-15      7  2018          15                          11\n",
      "\n",
      "[78 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-07-22      7  2018          22                          12\n",
      "2018-07-29      7  2018          29                          11]\n",
      "-------\n",
      "Count[77]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-06-24      6  2018          24                          11\n",
      "2018-07-01      7  2018           1                          10\n",
      "2018-07-08      7  2018           8                           9\n",
      "2018-07-15      7  2018          15                          11\n",
      "2018-07-22      7  2018          22                          12\n",
      "\n",
      "[79 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-07-29      7  2018          29                          11\n",
      "2018-08-05      8  2018           5                          11]\n",
      "-------\n",
      "Count[78]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-07-01      7  2018           1                          10\n",
      "2018-07-08      7  2018           8                           9\n",
      "2018-07-15      7  2018          15                          11\n",
      "2018-07-22      7  2018          22                          12\n",
      "2018-07-29      7  2018          29                          11\n",
      "\n",
      "[80 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-08-05      8  2018           5                          11\n",
      "2018-08-12      8  2018          12                          11]\n",
      "-------\n",
      "Count[79]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-07-08      7  2018           8                           9\n",
      "2018-07-15      7  2018          15                          11\n",
      "2018-07-22      7  2018          22                          12\n",
      "2018-07-29      7  2018          29                          11\n",
      "2018-08-05      8  2018           5                          11\n",
      "\n",
      "[81 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-08-12      8  2018          12                          11\n",
      "2018-08-19      8  2018          19                          10]\n",
      "-------\n",
      "Count[80]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-07-15      7  2018          15                          11\n",
      "2018-07-22      7  2018          22                          12\n",
      "2018-07-29      7  2018          29                          11\n",
      "2018-08-05      8  2018           5                          11\n",
      "2018-08-12      8  2018          12                          11\n",
      "\n",
      "[82 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-08-19      8  2018          19                          10\n",
      "2018-08-26      8  2018          26                          10]\n",
      "-------\n",
      "Count[81]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-07-22      7  2018          22                          12\n",
      "2018-07-29      7  2018          29                          11\n",
      "2018-08-05      8  2018           5                          11\n",
      "2018-08-12      8  2018          12                          11\n",
      "2018-08-19      8  2018          19                          10\n",
      "\n",
      "[83 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-08-26      8  2018          26                          10\n",
      "2018-09-02      9  2018           2                          11]\n",
      "-------\n",
      "Count[82]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-07-29      7  2018          29                          11\n",
      "2018-08-05      8  2018           5                          11\n",
      "2018-08-12      8  2018          12                          11\n",
      "2018-08-19      8  2018          19                          10\n",
      "2018-08-26      8  2018          26                          10\n",
      "\n",
      "[84 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-09-02      9  2018           2                          11\n",
      "2018-09-09      9  2018           9                          10]\n",
      "-------\n",
      "Count[83]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-08-05      8  2018           5                          11\n",
      "2018-08-12      8  2018          12                          11\n",
      "2018-08-19      8  2018          19                          10\n",
      "2018-08-26      8  2018          26                          10\n",
      "2018-09-02      9  2018           2                          11\n",
      "\n",
      "[85 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-09-09      9  2018           9                          10\n",
      "2018-09-16      9  2018          16                          10]\n",
      "-------\n",
      "Count[84]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-08-12      8  2018          12                          11\n",
      "2018-08-19      8  2018          19                          10\n",
      "2018-08-26      8  2018          26                          10\n",
      "2018-09-02      9  2018           2                          11\n",
      "2018-09-09      9  2018           9                          10\n",
      "\n",
      "[86 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-09-16      9  2018          16                          10\n",
      "2018-09-23      9  2018          23                           9]\n",
      "-------\n",
      "Count[85]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-08-19      8  2018          19                          10\n",
      "2018-08-26      8  2018          26                          10\n",
      "2018-09-02      9  2018           2                          11\n",
      "2018-09-09      9  2018           9                          10\n",
      "2018-09-16      9  2018          16                          10\n",
      "\n",
      "[87 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-09-23      9  2018          23                           9\n",
      "2018-09-30      9  2018          30                           9]\n",
      "-------\n",
      "Count[86]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-08-26      8  2018          26                          10\n",
      "2018-09-02      9  2018           2                          11\n",
      "2018-09-09      9  2018           9                          10\n",
      "2018-09-16      9  2018          16                          10\n",
      "2018-09-23      9  2018          23                           9\n",
      "\n",
      "[88 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-09-30      9  2018          30                           9\n",
      "2018-10-07     10  2018           7                           9]\n",
      "-------\n",
      "Count[87]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-09-02      9  2018           2                          11\n",
      "2018-09-09      9  2018           9                          10\n",
      "2018-09-16      9  2018          16                          10\n",
      "2018-09-23      9  2018          23                           9\n",
      "2018-09-30      9  2018          30                           9\n",
      "\n",
      "[89 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-10-07     10  2018           7                           9\n",
      "2018-10-14     10  2018          14                           9]\n",
      "-------\n",
      "Count[88]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-09-09      9  2018           9                          10\n",
      "2018-09-16      9  2018          16                          10\n",
      "2018-09-23      9  2018          23                           9\n",
      "2018-09-30      9  2018          30                           9\n",
      "2018-10-07     10  2018           7                           9\n",
      "\n",
      "[90 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-10-14     10  2018          14                           9\n",
      "2018-10-21     10  2018          21                           8]\n",
      "-------\n",
      "Count[89]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-09-16      9  2018          16                          10\n",
      "2018-09-23      9  2018          23                           9\n",
      "2018-09-30      9  2018          30                           9\n",
      "2018-10-07     10  2018           7                           9\n",
      "2018-10-14     10  2018          14                           9\n",
      "\n",
      "[91 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-10-21     10  2018          21                           8\n",
      "2018-10-28     10  2018          28                           8]\n",
      "-------\n",
      "Count[90]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-09-23      9  2018          23                           9\n",
      "2018-09-30      9  2018          30                           9\n",
      "2018-10-07     10  2018           7                           9\n",
      "2018-10-14     10  2018          14                           9\n",
      "2018-10-21     10  2018          21                           8\n",
      "\n",
      "[92 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-10-28     10  2018          28                           8\n",
      "2018-11-04     11  2018           4                           8]\n",
      "-------\n",
      "Count[91]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-09-30      9  2018          30                           9\n",
      "2018-10-07     10  2018           7                           9\n",
      "2018-10-14     10  2018          14                           9\n",
      "2018-10-21     10  2018          21                           8\n",
      "2018-10-28     10  2018          28                           8\n",
      "\n",
      "[93 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-11-04     11  2018           4                           8\n",
      "2018-11-11     11  2018          11                          10]\n",
      "-------\n",
      "Count[92]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-10-07     10  2018           7                           9\n",
      "2018-10-14     10  2018          14                           9\n",
      "2018-10-21     10  2018          21                           8\n",
      "2018-10-28     10  2018          28                           8\n",
      "2018-11-04     11  2018           4                           8\n",
      "\n",
      "[94 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-11-11     11  2018          11                          10\n",
      "2018-11-18     11  2018          18                          14]\n",
      "-------\n",
      "Count[93]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-10-14     10  2018          14                           9\n",
      "2018-10-21     10  2018          21                           8\n",
      "2018-10-28     10  2018          28                           8\n",
      "2018-11-04     11  2018           4                           8\n",
      "2018-11-11     11  2018          11                          10\n",
      "\n",
      "[95 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-11-18     11  2018          18                          14\n",
      "2018-11-25     11  2018          25                          14]\n",
      "-------\n",
      "Count[94]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-10-21     10  2018          21                           8\n",
      "2018-10-28     10  2018          28                           8\n",
      "2018-11-04     11  2018           4                           8\n",
      "2018-11-11     11  2018          11                          10\n",
      "2018-11-18     11  2018          18                          14\n",
      "\n",
      "[96 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-11-25     11  2018          25                          14\n",
      "2018-12-02     12  2018           2                          12]\n",
      "-------\n",
      "Count[95]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-10-28     10  2018          28                           8\n",
      "2018-11-04     11  2018           4                           8\n",
      "2018-11-11     11  2018          11                          10\n",
      "2018-11-18     11  2018          18                          14\n",
      "2018-11-25     11  2018          25                          14\n",
      "\n",
      "[97 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-12-02     12  2018           2                          12\n",
      "2018-12-09     12  2018           9                          11]\n",
      "-------\n",
      "Count[96]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-11-04     11  2018           4                           8\n",
      "2018-11-11     11  2018          11                          10\n",
      "2018-11-18     11  2018          18                          14\n",
      "2018-11-25     11  2018          25                          14\n",
      "2018-12-02     12  2018           2                          12\n",
      "\n",
      "[98 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-12-09     12  2018           9                          11\n",
      "2018-12-16     12  2018          16                          12]\n",
      "-------\n",
      "Count[97]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-11-11     11  2018          11                          10\n",
      "2018-11-18     11  2018          18                          14\n",
      "2018-11-25     11  2018          25                          14\n",
      "2018-12-02     12  2018           2                          12\n",
      "2018-12-09     12  2018           9                          11\n",
      "\n",
      "[99 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-12-16     12  2018          16                          12\n",
      "2018-12-23     12  2018          23                          10]\n",
      "-------\n",
      "Count[98]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-11-18     11  2018          18                          14\n",
      "2018-11-25     11  2018          25                          14\n",
      "2018-12-02     12  2018           2                          12\n",
      "2018-12-09     12  2018           9                          11\n",
      "2018-12-16     12  2018          16                          12\n",
      "\n",
      "[100 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-12-23     12  2018          23                          10\n",
      "2018-12-30     12  2018          30                           9]\n",
      "-------\n",
      "Count[99]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-11-25     11  2018          25                          14\n",
      "2018-12-02     12  2018           2                          12\n",
      "2018-12-09     12  2018           9                          11\n",
      "2018-12-16     12  2018          16                          12\n",
      "2018-12-23     12  2018          23                          10\n",
      "\n",
      "[101 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2018-12-30     12  2018          30                           9\n",
      "2019-01-06      1  2019           6                           9]\n",
      "-------\n",
      "Count[100]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-12-02     12  2018           2                          12\n",
      "2018-12-09     12  2018           9                          11\n",
      "2018-12-16     12  2018          16                          12\n",
      "2018-12-23     12  2018          23                          10\n",
      "2018-12-30     12  2018          30                           9\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-01-06      1  2019           6                           9\n",
      "2019-01-13      1  2019          13                           8]\n",
      "-------\n",
      "Count[101]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-12-09     12  2018           9                          11\n",
      "2018-12-16     12  2018          16                          12\n",
      "2018-12-23     12  2018          23                          10\n",
      "2018-12-30     12  2018          30                           9\n",
      "2019-01-06      1  2019           6                           9\n",
      "\n",
      "[103 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-01-13      1  2019          13                           8\n",
      "2019-01-20      1  2019          20                           8]\n",
      "-------\n",
      "Count[102]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-12-16     12  2018          16                          12\n",
      "2018-12-23     12  2018          23                          10\n",
      "2018-12-30     12  2018          30                           9\n",
      "2019-01-06      1  2019           6                           9\n",
      "2019-01-13      1  2019          13                           8\n",
      "\n",
      "[104 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-01-20      1  2019          20                           8\n",
      "2019-01-27      1  2019          27                           8]\n",
      "-------\n",
      "Count[103]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-12-23     12  2018          23                          10\n",
      "2018-12-30     12  2018          30                           9\n",
      "2019-01-06      1  2019           6                           9\n",
      "2019-01-13      1  2019          13                           8\n",
      "2019-01-20      1  2019          20                           8\n",
      "\n",
      "[105 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-01-27      1  2019          27                           8\n",
      "2019-02-03      2  2019           3                           9]\n",
      "-------\n",
      "Count[104]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2018-12-30     12  2018          30                           9\n",
      "2019-01-06      1  2019           6                           9\n",
      "2019-01-13      1  2019          13                           8\n",
      "2019-01-20      1  2019          20                           8\n",
      "2019-01-27      1  2019          27                           8\n",
      "\n",
      "[106 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-02-03      2  2019           3                           9\n",
      "2019-02-10      2  2019          10                           7]\n",
      "-------\n",
      "Count[105]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-01-06      1  2019           6                           9\n",
      "2019-01-13      1  2019          13                           8\n",
      "2019-01-20      1  2019          20                           8\n",
      "2019-01-27      1  2019          27                           8\n",
      "2019-02-03      2  2019           3                           9\n",
      "\n",
      "[107 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-02-10      2  2019          10                           7\n",
      "2019-02-17      2  2019          17                           9]\n",
      "-------\n",
      "Count[106]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-01-13      1  2019          13                           8\n",
      "2019-01-20      1  2019          20                           8\n",
      "2019-01-27      1  2019          27                           8\n",
      "2019-02-03      2  2019           3                           9\n",
      "2019-02-10      2  2019          10                           7\n",
      "\n",
      "[108 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-02-17      2  2019          17                           9\n",
      "2019-02-24      2  2019          24                           8]\n",
      "-------\n",
      "Count[107]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-01-20      1  2019          20                           8\n",
      "2019-01-27      1  2019          27                           8\n",
      "2019-02-03      2  2019           3                           9\n",
      "2019-02-10      2  2019          10                           7\n",
      "2019-02-17      2  2019          17                           9\n",
      "\n",
      "[109 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-02-24      2  2019          24                           8\n",
      "2019-03-03      3  2019           3                           8]\n",
      "-------\n",
      "Count[108]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-01-27      1  2019          27                           8\n",
      "2019-02-03      2  2019           3                           9\n",
      "2019-02-10      2  2019          10                           7\n",
      "2019-02-17      2  2019          17                           9\n",
      "2019-02-24      2  2019          24                           8\n",
      "\n",
      "[110 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-03-03      3  2019           3                           8\n",
      "2019-03-10      3  2019          10                           7]\n",
      "-------\n",
      "Count[109]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-02-03      2  2019           3                           9\n",
      "2019-02-10      2  2019          10                           7\n",
      "2019-02-17      2  2019          17                           9\n",
      "2019-02-24      2  2019          24                           8\n",
      "2019-03-03      3  2019           3                           8\n",
      "\n",
      "[111 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-03-10      3  2019          10                           7\n",
      "2019-03-17      3  2019          17                           7]\n",
      "-------\n",
      "Count[110]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-02-10      2  2019          10                           7\n",
      "2019-02-17      2  2019          17                           9\n",
      "2019-02-24      2  2019          24                           8\n",
      "2019-03-03      3  2019           3                           8\n",
      "2019-03-10      3  2019          10                           7\n",
      "\n",
      "[112 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-03-17      3  2019          17                           7\n",
      "2019-03-24      3  2019          24                           7]\n",
      "-------\n",
      "Count[111]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-02-17      2  2019          17                           9\n",
      "2019-02-24      2  2019          24                           8\n",
      "2019-03-03      3  2019           3                           8\n",
      "2019-03-10      3  2019          10                           7\n",
      "2019-03-17      3  2019          17                           7\n",
      "\n",
      "[113 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-03-24      3  2019          24                           7\n",
      "2019-03-31      3  2019          31                          13]\n",
      "-------\n",
      "Count[112]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-02-24      2  2019          24                           8\n",
      "2019-03-03      3  2019           3                           8\n",
      "2019-03-10      3  2019          10                           7\n",
      "2019-03-17      3  2019          17                           7\n",
      "2019-03-24      3  2019          24                           7\n",
      "\n",
      "[114 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-03-31      3  2019          31                          13\n",
      "2019-04-07      4  2019           7                          11]\n",
      "-------\n",
      "Count[113]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-03-03      3  2019           3                           8\n",
      "2019-03-10      3  2019          10                           7\n",
      "2019-03-17      3  2019          17                           7\n",
      "2019-03-24      3  2019          24                           7\n",
      "2019-03-31      3  2019          31                          13\n",
      "\n",
      "[115 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-04-07      4  2019           7                          11\n",
      "2019-04-14      4  2019          14                           9]\n",
      "-------\n",
      "Count[114]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-03-10      3  2019          10                           7\n",
      "2019-03-17      3  2019          17                           7\n",
      "2019-03-24      3  2019          24                           7\n",
      "2019-03-31      3  2019          31                          13\n",
      "2019-04-07      4  2019           7                          11\n",
      "\n",
      "[116 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-04-14      4  2019          14                           9\n",
      "2019-04-21      4  2019          21                          10]\n",
      "-------\n",
      "Count[115]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-03-17      3  2019          17                           7\n",
      "2019-03-24      3  2019          24                           7\n",
      "2019-03-31      3  2019          31                          13\n",
      "2019-04-07      4  2019           7                          11\n",
      "2019-04-14      4  2019          14                           9\n",
      "\n",
      "[117 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-04-21      4  2019          21                          10\n",
      "2019-04-28      4  2019          28                           9]\n",
      "-------\n",
      "Count[116]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-03-24      3  2019          24                           7\n",
      "2019-03-31      3  2019          31                          13\n",
      "2019-04-07      4  2019           7                          11\n",
      "2019-04-14      4  2019          14                           9\n",
      "2019-04-21      4  2019          21                          10\n",
      "\n",
      "[118 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-04-28      4  2019          28                           9\n",
      "2019-05-05      5  2019           5                          13]\n",
      "-------\n",
      "Count[117]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-03-31      3  2019          31                          13\n",
      "2019-04-07      4  2019           7                          11\n",
      "2019-04-14      4  2019          14                           9\n",
      "2019-04-21      4  2019          21                          10\n",
      "2019-04-28      4  2019          28                           9\n",
      "\n",
      "[119 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-05-05      5  2019           5                          13\n",
      "2019-05-12      5  2019          12                          20]\n",
      "-------\n",
      "Count[118]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-04-07      4  2019           7                          11\n",
      "2019-04-14      4  2019          14                           9\n",
      "2019-04-21      4  2019          21                          10\n",
      "2019-04-28      4  2019          28                           9\n",
      "2019-05-05      5  2019           5                          13\n",
      "\n",
      "[120 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-05-12      5  2019          12                          20\n",
      "2019-05-19      5  2019          19                          14]\n",
      "-------\n",
      "Count[119]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-04-14      4  2019          14                           9\n",
      "2019-04-21      4  2019          21                          10\n",
      "2019-04-28      4  2019          28                           9\n",
      "2019-05-05      5  2019           5                          13\n",
      "2019-05-12      5  2019          12                          20\n",
      "\n",
      "[121 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-05-19      5  2019          19                          14\n",
      "2019-05-26      5  2019          26                          16]\n",
      "-------\n",
      "Count[120]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-04-21      4  2019          21                          10\n",
      "2019-04-28      4  2019          28                           9\n",
      "2019-05-05      5  2019           5                          13\n",
      "2019-05-12      5  2019          12                          20\n",
      "2019-05-19      5  2019          19                          14\n",
      "\n",
      "[122 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-05-26      5  2019          26                          16\n",
      "2019-06-02      6  2019           2                          13]\n",
      "-------\n",
      "Count[121]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-04-28      4  2019          28                           9\n",
      "2019-05-05      5  2019           5                          13\n",
      "2019-05-12      5  2019          12                          20\n",
      "2019-05-19      5  2019          19                          14\n",
      "2019-05-26      5  2019          26                          16\n",
      "\n",
      "[123 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-06-02      6  2019           2                          13\n",
      "2019-06-09      6  2019           9                          12]\n",
      "-------\n",
      "Count[122]\n",
      "Train:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2017-01-22      1  2017          22                           5\n",
      "2017-01-29      1  2017          29                           5\n",
      "2017-02-05      2  2017           5                           5\n",
      "2017-02-12      2  2017          12                           5\n",
      "2017-02-19      2  2017          19                           5\n",
      "...           ...   ...         ...                         ...\n",
      "2019-05-05      5  2019           5                          13\n",
      "2019-05-12      5  2019          12                          20\n",
      "2019-05-19      5  2019          19                          14\n",
      "2019-05-26      5  2019          26                          16\n",
      "2019-06-02      6  2019           2                          13\n",
      "\n",
      "[124 rows x 4 columns]]\n",
      "Val:\t[Hafta       month  year  dayofmonth  Bitcoin: (DÃ¼nya Genelinde)\n",
      "2019-06-09      6  2019           9                          12\n",
      "2019-06-16      6  2019          16                          18]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "# Initializing rolling data objects\n",
    "rolling_data_x = roll_forecast_orig(x_training_set,min_train_size=3,horizon=2)\n",
    "rolling_data_y = roll_forecast_orig(list(y_training_set),min_train_size=3,horizon=2)\n",
    "\n",
    "\n",
    "cv_x_train_list = []\n",
    "cv_x_test_list = []\n",
    "i= 0\n",
    "for train_c, val_c in rolling_data_x:\n",
    "    print(f'Count[{i+1}]')\n",
    "    print(f'Train:\\t[{train_c}]')\n",
    "    print(f'Val:\\t[{val_c}]')\n",
    "    print('-------')\n",
    "    cv_x_train_list.append(train_c)\n",
    "    cv_x_test_list.append(val_c)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count[1]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375])]]\n",
      "Val:\t[[array([914.22767857]), array([963.75553571])]]\n",
      "-------\n",
      "Count[2]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857])]]\n",
      "Val:\t[[array([963.75553571]), array([1027.17464143])]]\n",
      "-------\n",
      "Count[3]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571])]]\n",
      "Val:\t[[array([1027.17464143]), array([1017.61857143])]]\n",
      "-------\n",
      "Count[4]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143])]]\n",
      "Val:\t[[array([1017.61857143]), array([1112.91392714])]]\n",
      "-------\n",
      "Count[5]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143])]]\n",
      "Val:\t[[array([1112.91392714]), array([1212.71057])]]\n",
      "-------\n",
      "Count[6]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714])]]\n",
      "Val:\t[[array([1212.71057]), array([1217.40271143])]]\n",
      "-------\n",
      "Count[7]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057])]]\n",
      "Val:\t[[array([1217.40271143]), array([1192.34642571])]]\n",
      "-------\n",
      "Count[8]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143])]]\n",
      "Val:\t[[array([1192.34642571]), array([1023.57671286])]]\n",
      "-------\n",
      "Count[9]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571])]]\n",
      "Val:\t[[array([1023.57671286]), array([1020.84085429])]]\n",
      "-------\n",
      "Count[10]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286])]]\n",
      "Val:\t[[array([1020.84085429]), array([1147.88914286])]]\n",
      "-------\n",
      "Count[11]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429])]]\n",
      "Val:\t[[array([1147.88914286]), array([1203.42628286])]]\n",
      "-------\n",
      "Count[12]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286])]]\n",
      "Val:\t[[array([1203.42628286]), array([1221.29832])]]\n",
      "-------\n",
      "Count[13]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286])]]\n",
      "Val:\t[[array([1221.29832]), array([1277.53339286])]]\n",
      "-------\n",
      "Count[14]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832])]]\n",
      "Val:\t[[array([1277.53339286]), array([1447.72999857])]]\n",
      "-------\n",
      "Count[15]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286])]]\n",
      "Val:\t[[array([1447.72999857]), array([1659.52982143])]]\n",
      "-------\n",
      "Count[16]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857])]]\n",
      "Val:\t[[array([1659.52982143]), array([1803.61946429])]]\n",
      "-------\n",
      "Count[17]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143])]]\n",
      "Val:\t[[array([1803.61946429]), array([2287.14303286])]]\n",
      "-------\n",
      "Count[18]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429])]]\n",
      "Val:\t[[array([2287.14303286]), array([2324.56160429])]]\n",
      "-------\n",
      "Count[19]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286])]]\n",
      "Val:\t[[array([2324.56160429]), array([2756.75232143])]]\n",
      "-------\n",
      "Count[20]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429])]]\n",
      "Val:\t[[array([2756.75232143]), array([2696.07982])]]\n",
      "-------\n",
      "Count[21]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143])]]\n",
      "Val:\t[[array([2696.07982]), array([2710.04499857])]]\n",
      "-------\n",
      "Count[22]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982])]]\n",
      "Val:\t[[array([2710.04499857]), array([2559.42803143])]]\n",
      "-------\n",
      "Count[23]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857])]]\n",
      "Val:\t[[array([2559.42803143]), array([2559.18375])]]\n",
      "-------\n",
      "Count[24]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143])]]\n",
      "Val:\t[[array([2559.18375]), array([2377.13624857])]]\n",
      "-------\n",
      "Count[25]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375])]]\n",
      "Val:\t[[array([2377.13624857]), array([2343.62089143])]]\n",
      "-------\n",
      "Count[26]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857])]]\n",
      "Val:\t[[array([2343.62089143]), array([2700.38053429])]]\n",
      "-------\n",
      "Count[27]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143])]]\n",
      "Val:\t[[array([2700.38053429]), array([2816.55375])]]\n",
      "-------\n",
      "Count[28]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429])]]\n",
      "Val:\t[[array([2816.55375]), array([3420.28410714])]]\n",
      "-------\n",
      "Count[29]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375])]]\n",
      "Val:\t[[array([3420.28410714]), array([4219.52803571])]]\n",
      "-------\n",
      "Count[30]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714])]]\n",
      "Val:\t[[array([4219.52803571]), array([4179.86874857])]]\n",
      "-------\n",
      "Count[31]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571])]]\n",
      "Val:\t[[array([4179.86874857]), array([4593.25071429])]]\n",
      "-------\n",
      "Count[32]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857])]]\n",
      "Val:\t[[array([4593.25071429]), array([4485.21946429])]]\n",
      "-------\n",
      "Count[33]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429])]]\n",
      "Val:\t[[array([4485.21946429]), array([3955.74857])]]\n",
      "-------\n",
      "Count[34]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429])]]\n",
      "Val:\t[[array([3955.74857]), array([3763.375])]]\n",
      "-------\n",
      "Count[35]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857])]]\n",
      "Val:\t[[array([3763.375]), array([3982.4425])]]\n",
      "-------\n",
      "Count[36]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375])]]\n",
      "Val:\t[[array([3982.4425]), array([4332.70642714])]]\n",
      "-------\n",
      "Count[37]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425])]]\n",
      "Val:\t[[array([4332.70642714]), array([4979.38035429])]]\n",
      "-------\n",
      "Count[38]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714])]]\n",
      "Val:\t[[array([4979.38035429]), array([5705.20053571])]]\n",
      "-------\n",
      "Count[39]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429])]]\n",
      "Val:\t[[array([5705.20053571]), array([5793.775])]]\n",
      "-------\n",
      "Count[40]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571])]]\n",
      "Val:\t[[array([5793.775]), array([6533.88535714])]]\n",
      "-------\n",
      "Count[41]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775])]]\n",
      "Val:\t[[array([6533.88535714]), array([7208.05160714])]]\n",
      "-------\n",
      "Count[42]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714])]]\n",
      "Val:\t[[array([7208.05160714]), array([6908.55553571])]]\n",
      "-------\n",
      "Count[43]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714])]]\n",
      "Val:\t[[array([6908.55553571]), array([8037.99803571])]]\n",
      "-------\n",
      "Count[44]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571])]]\n",
      "Val:\t[[array([8037.99803571]), array([9943.70839286])]]\n",
      "-------\n",
      "Count[45]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571])]]\n",
      "Val:\t[[array([9943.70839286]), array([13150.48428571])]]\n",
      "-------\n",
      "Count[46]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286])]]\n",
      "Val:\t[[array([13150.48428571]), array([16201.34267714])]]\n",
      "-------\n",
      "Count[47]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571])]]\n",
      "Val:\t[[array([16201.34267714]), array([17065.23499714])]]\n",
      "-------\n",
      "Count[48]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714])]]\n",
      "Val:\t[[array([17065.23499714]), array([14408.95428571])]]\n",
      "-------\n",
      "Count[49]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714])]]\n",
      "Val:\t[[array([14408.95428571]), array([14479.38964286])]]\n",
      "-------\n",
      "Count[50]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571])]]\n",
      "Val:\t[[array([14479.38964286]), array([14686.02089286])]]\n",
      "-------\n",
      "Count[51]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286])]]\n",
      "Val:\t[[array([14686.02089286]), array([12249.07625])]]\n",
      "-------\n",
      "Count[52]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286])]]\n",
      "Val:\t[[array([12249.07625]), array([11408.26446429])]]\n",
      "-------\n",
      "Count[53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625])]]\n",
      "Val:\t[[array([11408.26446429]), array([10095.43606857])]]\n",
      "-------\n",
      "Count[54]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429])]]\n",
      "Val:\t[[array([10095.43606857]), array([7990.78642857])]]\n",
      "-------\n",
      "Count[55]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857])]]\n",
      "Val:\t[[array([7990.78642857]), array([9175.62392714])]]\n",
      "-------\n",
      "Count[56]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857])]]\n",
      "Val:\t[[array([9175.62392714]), array([10669.57785714])]]\n",
      "-------\n",
      "Count[57]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714])]]\n",
      "Val:\t[[array([10669.57785714]), array([10449.48339286])]]\n",
      "-------\n",
      "Count[58]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714])]]\n",
      "Val:\t[[array([10449.48339286]), array([10356.73857])]]\n",
      "-------\n",
      "Count[59]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286])]]\n",
      "Val:\t[[array([10356.73857]), array([8652.06410571])]]\n",
      "-------\n",
      "Count[60]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857])]]\n",
      "Val:\t[[array([8652.06410571]), array([8542.92464286])]]\n",
      "-------\n",
      "Count[61]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571])]]\n",
      "Val:\t[[array([8542.92464286]), array([7770.58357])]]\n",
      "-------\n",
      "Count[62]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286])]]\n",
      "Val:\t[[array([7770.58357]), array([6968.95964286])]]\n",
      "-------\n",
      "Count[63]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357])]]\n",
      "Val:\t[[array([6968.95964286]), array([7215.68732143])]]\n",
      "-------\n",
      "Count[64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286])]]\n",
      "Val:\t[[array([7215.68732143]), array([8228.76696286])]]\n",
      "-------\n",
      "Count[65]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143])]]\n",
      "Val:\t[[array([8228.76696286]), array([9052.69749857])]]\n",
      "-------\n",
      "Count[66]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286])]]\n",
      "Val:\t[[array([9052.69749857]), array([9330.19482])]]\n",
      "-------\n",
      "Count[67]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857])]]\n",
      "Val:\t[[array([9330.19482]), array([9184.56999714])]]\n",
      "-------\n",
      "Count[68]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482])]]\n",
      "Val:\t[[array([9184.56999714]), array([8329.44839143])]]\n",
      "-------\n",
      "Count[69]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714])]]\n",
      "Val:\t[[array([8329.44839143]), array([7952.97375])]]\n",
      "-------\n",
      "Count[70]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143])]]\n",
      "Val:\t[[array([7952.97375]), array([7380.32565413])]]\n",
      "-------\n",
      "Count[71]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375])]]\n",
      "Val:\t[[array([7380.32565413]), array([7157.66452182])]]\n",
      "-------\n",
      "Count[72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413])]]\n",
      "Val:\t[[array([7157.66452182]), array([6598.77222534])]]\n",
      "-------\n",
      "Count[73]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182])]]\n",
      "Val:\t[[array([6598.77222534]), array([6243.34389536])]]\n",
      "-------\n",
      "Count[74]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534])]]\n",
      "Val:\t[[array([6243.34389536]), array([6366.17412419])]]\n",
      "-------\n",
      "Count[75]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536])]]\n",
      "Val:\t[[array([6366.17412419]), array([6488.3248472])]]\n",
      "-------\n",
      "Count[76]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419])]]\n",
      "Val:\t[[array([6488.3248472]), array([6972.86105117])]]\n",
      "-------\n",
      "Count[77]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472])]]\n",
      "Val:\t[[array([6972.86105117]), array([7906.15806285])]]\n",
      "-------\n",
      "Count[78]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117])]]\n",
      "Val:\t[[array([7906.15806285]), array([7830.39766037])]]\n",
      "-------\n",
      "Count[79]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285])]]\n",
      "Val:\t[[array([7830.39766037]), array([6856.28489232])]]\n",
      "-------\n",
      "Count[80]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037])]]\n",
      "Val:\t[[array([6856.28489232]), array([6326.72415194])]]\n",
      "-------\n",
      "Count[81]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232])]]\n",
      "Val:\t[[array([6326.72415194]), array([6459.83121488])]]\n",
      "-------\n",
      "Count[82]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194])]]\n",
      "Val:\t[[array([6459.83121488]), array([6897.4815259])]]\n",
      "-------\n",
      "Count[83]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488])]]\n",
      "Val:\t[[array([6897.4815259]), array([6983.90257273])]]\n",
      "-------\n",
      "Count[84]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259])]]\n",
      "Val:\t[[array([6983.90257273]), array([6261.87074343])]]\n",
      "-------\n",
      "Count[85]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273])]]\n",
      "Val:\t[[array([6261.87074343]), array([6285.98794474])]]\n",
      "-------\n",
      "Count[86]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343])]]\n",
      "Val:\t[[array([6285.98794474]), array([6450.38844454])]]\n",
      "-------\n",
      "Count[87]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474])]]\n",
      "Val:\t[[array([6450.38844454]), array([6587.26681294])]]\n",
      "-------\n",
      "Count[88]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454])]]\n",
      "Val:\t[[array([6587.26681294]), array([6532.48431201])]]\n",
      "-------\n",
      "Count[89]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294])]]\n",
      "Val:\t[[array([6532.48431201]), array([6475.86449153])]]\n",
      "-------\n",
      "Count[90]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201])]]\n",
      "Val:\t[[array([6475.86449153]), array([6402.64132748])]]\n",
      "-------\n",
      "Count[91]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153])]]\n",
      "Val:\t[[array([6402.64132748]), array([6334.29000324])]]\n",
      "-------\n",
      "Count[92]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748])]]\n",
      "Val:\t[[array([6334.29000324]), array([6409.10159387])]]\n",
      "-------\n",
      "Count[93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324])]]\n",
      "Val:\t[[array([6409.10159387]), array([5845.57712265])]]\n",
      "-------\n",
      "Count[94]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387])]]\n",
      "Val:\t[[array([5845.57712265]), array([4506.07200357])]]\n",
      "-------\n",
      "Count[95]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265])]]\n",
      "Val:\t[[array([4506.07200357]), array([3986.84780238])]]\n",
      "-------\n",
      "Count[96]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357])]]\n",
      "Val:\t[[array([3986.84780238]), array([3713.01620529])]]\n",
      "-------\n",
      "Count[97]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238])]]\n",
      "Val:\t[[array([3713.01620529]), array([3339.07682556])]]\n",
      "-------\n",
      "Count[98]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529])]]\n",
      "Val:\t[[array([3339.07682556]), array([3672.08306453])]]\n",
      "-------\n",
      "Count[99]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556])]]\n",
      "Val:\t[[array([3672.08306453]), array([3817.26939879])]]\n",
      "-------\n",
      "Count[100]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453])]]\n",
      "Val:\t[[array([3817.26939879]), array([3859.93175935])]]\n",
      "-------\n",
      "Count[101]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879])]]\n",
      "Val:\t[[array([3859.93175935]), array([3710.76674505])]]\n",
      "-------\n",
      "Count[102]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935])]]\n",
      "Val:\t[[array([3710.76674505]), array([3593.66276279])]]\n",
      "-------\n",
      "Count[103]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505])]]\n",
      "Val:\t[[array([3593.66276279]), array([3541.29578635])]]\n",
      "-------\n",
      "Count[104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279])]]\n",
      "Val:\t[[array([3541.29578635]), array([3420.89354512])]]\n",
      "-------\n",
      "Count[105]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635])]]\n",
      "Val:\t[[array([3420.89354512]), array([3510.09214535])]]\n",
      "-------\n",
      "Count[106]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512])]]\n",
      "Val:\t[[array([3510.09214535]), array([3620.20238302])]]\n",
      "-------\n",
      "Count[107]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535])]]\n",
      "Val:\t[[array([3620.20238302]), array([3905.88392212])]]\n",
      "-------\n",
      "Count[108]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302])]]\n",
      "Val:\t[[array([3905.88392212]), array([3779.82211594])]]\n",
      "-------\n",
      "Count[109]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212])]]\n",
      "Val:\t[[array([3779.82211594]), array([3859.86038])]]\n",
      "-------\n",
      "Count[110]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212]), array([3779.82211594])]]\n",
      "Val:\t[[array([3859.86038]), array([3913.00329955])]]\n",
      "-------\n",
      "Count[111]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212]), array([3779.82211594]), array([3859.86038])]]\n",
      "Val:\t[[array([3913.00329955]), array([3977.98129171])]]\n",
      "-------\n",
      "Count[112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212]), array([3779.82211594]), array([3859.86038]), array([3913.00329955])]]\n",
      "Val:\t[[array([3977.98129171]), array([4047.67259308])]]\n",
      "-------\n",
      "Count[113]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212]), array([3779.82211594]), array([3859.86038]), array([3913.00329955]), array([3977.98129171])]]\n",
      "Val:\t[[array([4047.67259308]), array([5044.87905504])]]\n",
      "-------\n",
      "Count[114]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212]), array([3779.82211594]), array([3859.86038]), array([3913.00329955]), array([3977.98129171]), array([4047.67259308])]]\n",
      "Val:\t[[array([5044.87905504]), array([5123.08467168])]]\n",
      "-------\n",
      "Count[115]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212]), array([3779.82211594]), array([3859.86038]), array([3913.00329955]), array([3977.98129171]), array([4047.67259308]), array([5044.87905504])]]\n",
      "Val:\t[[array([5123.08467168]), array([5273.6463849])]]\n",
      "-------\n",
      "Count[116]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212]), array([3779.82211594]), array([3859.86038]), array([3913.00329955]), array([3977.98129171]), array([4047.67259308]), array([5044.87905504]), array([5123.08467168])]]\n",
      "Val:\t[[array([5273.6463849]), array([5253.35535158])]]\n",
      "-------\n",
      "Count[117]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212]), array([3779.82211594]), array([3859.86038]), array([3913.00329955]), array([3977.98129171]), array([4047.67259308]), array([5044.87905504]), array([5123.08467168]), array([5273.6463849])]]\n",
      "Val:\t[[array([5253.35535158]), array([5558.61704769])]]\n",
      "-------\n",
      "Count[118]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212]), array([3779.82211594]), array([3859.86038]), array([3913.00329955]), array([3977.98129171]), array([4047.67259308]), array([5044.87905504]), array([5123.08467168]), array([5273.6463849]), array([5253.35535158])]]\n",
      "Val:\t[[array([5558.61704769]), array([6647.70563854])]]\n",
      "-------\n",
      "Count[119]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212]), array([3779.82211594]), array([3859.86038]), array([3913.00329955]), array([3977.98129171]), array([4047.67259308]), array([5044.87905504]), array([5123.08467168]), array([5273.6463849]), array([5253.35535158]), array([5558.61704769])]]\n",
      "Val:\t[[array([6647.70563854]), array([7800.5027629])]]\n",
      "-------\n",
      "Count[120]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212]), array([3779.82211594]), array([3859.86038]), array([3913.00329955]), array([3977.98129171]), array([4047.67259308]), array([5044.87905504]), array([5123.08467168]), array([5273.6463849]), array([5253.35535158]), array([5558.61704769]), array([6647.70563854])]]\n",
      "Val:\t[[array([7800.5027629]), array([8167.20649452])]]\n",
      "-------\n",
      "Count[121]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212]), array([3779.82211594]), array([3859.86038]), array([3913.00329955]), array([3977.98129171]), array([4047.67259308]), array([5044.87905504]), array([5123.08467168]), array([5273.6463849]), array([5253.35535158]), array([5558.61704769]), array([6647.70563854]), array([7800.5027629])]]\n",
      "Val:\t[[array([8167.20649452]), array([8571.25271791])]]\n",
      "-------\n",
      "Count[122]\n",
      "Train:\t[[array([1001.98464143]), array([858.13410714]), array([870.80375]), array([914.22767857]), array([963.75553571]), array([1027.17464143]), array([1017.61857143]), array([1112.91392714]), array([1212.71057]), array([1217.40271143]), array([1192.34642571]), array([1023.57671286]), array([1020.84085429]), array([1147.88914286]), array([1203.42628286]), array([1221.29832]), array([1277.53339286]), array([1447.72999857]), array([1659.52982143]), array([1803.61946429]), array([2287.14303286]), array([2324.56160429]), array([2756.75232143]), array([2696.07982]), array([2710.04499857]), array([2559.42803143]), array([2559.18375]), array([2377.13624857]), array([2343.62089143]), array([2700.38053429]), array([2816.55375]), array([3420.28410714]), array([4219.52803571]), array([4179.86874857]), array([4593.25071429]), array([4485.21946429]), array([3955.74857]), array([3763.375]), array([3982.4425]), array([4332.70642714]), array([4979.38035429]), array([5705.20053571]), array([5793.775]), array([6533.88535714]), array([7208.05160714]), array([6908.55553571]), array([8037.99803571]), array([9943.70839286]), array([13150.48428571]), array([16201.34267714]), array([17065.23499714]), array([14408.95428571]), array([14479.38964286]), array([14686.02089286]), array([12249.07625]), array([11408.26446429]), array([10095.43606857]), array([7990.78642857]), array([9175.62392714]), array([10669.57785714]), array([10449.48339286]), array([10356.73857]), array([8652.06410571]), array([8542.92464286]), array([7770.58357]), array([6968.95964286]), array([7215.68732143]), array([8228.76696286]), array([9052.69749857]), array([9330.19482]), array([9184.56999714]), array([8329.44839143]), array([7952.97375]), array([7380.32565413]), array([7157.66452182]), array([6598.77222534]), array([6243.34389536]), array([6366.17412419]), array([6488.3248472]), array([6972.86105117]), array([7906.15806285]), array([7830.39766037]), array([6856.28489232]), array([6326.72415194]), array([6459.83121488]), array([6897.4815259]), array([6983.90257273]), array([6261.87074343]), array([6285.98794474]), array([6450.38844454]), array([6587.26681294]), array([6532.48431201]), array([6475.86449153]), array([6402.64132748]), array([6334.29000324]), array([6409.10159387]), array([5845.57712265]), array([4506.07200357]), array([3986.84780238]), array([3713.01620529]), array([3339.07682556]), array([3672.08306453]), array([3817.26939879]), array([3859.93175935]), array([3710.76674505]), array([3593.66276279]), array([3541.29578635]), array([3420.89354512]), array([3510.09214535]), array([3620.20238302]), array([3905.88392212]), array([3779.82211594]), array([3859.86038]), array([3913.00329955]), array([3977.98129171]), array([4047.67259308]), array([5044.87905504]), array([5123.08467168]), array([5273.6463849]), array([5253.35535158]), array([5558.61704769]), array([6647.70563854]), array([7800.5027629]), array([8167.20649452])]]\n",
      "Val:\t[[array([8571.25271791]), array([7814.4837118])]]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "cv_y_train_list = []\n",
    "cv_y_test_list = []\n",
    "i= 0\n",
    "for train_c, val_c in rolling_data_y:\n",
    "    print(f'Count[{i+1}]')\n",
    "    print(f'Train:\\t[{train_c}]')\n",
    "    print(f'Val:\\t[{val_c}]')\n",
    "    print('-------')\n",
    "    cv_y_train_list.append(train_c)\n",
    "    cv_y_test_list.append(val_c)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_least_error_hyperparameters(dictionary):\n",
    "    min_index = 0\n",
    "    key_list = []\n",
    "    value_list = []\n",
    "    for key,item in dictionary.items():\n",
    "        key_list.append(key)\n",
    "        value_list.append(item)\n",
    "    temp_index = 0\n",
    "    temp = value_list[0]\n",
    "    for i in range(0,len(value_list)):\n",
    "        if temp > value_list[i]:\n",
    "            temp_index = i\n",
    "            temp = value_list[i]\n",
    "    return key_list[temp_index], temp\n",
    "\n",
    "def root_mean_squared_sum_of_list(list_value):\n",
    "    sum_of_list_as_root_mean_squared = 0\n",
    "    for i in list_value:\n",
    "        sum_of_list_as_root_mean_squared += np.sqrt(i)\n",
    "    return sum_of_list_as_root_mean_squared\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_cv_scores(cv_x_test_list_param ,cv_y_test_param, metric,cv_x_train_list_param,cv_y_train_param,unit_count,batch_count,epoch_count):\n",
    "    cv_scores = []\n",
    "    i=0\n",
    "    for i in range(len(cv_x_train_list_param)):\n",
    "        \n",
    "        fold_min_max_scaler = MinMaxScaler()\n",
    "        fold_min_max_scaler_y = MinMaxScaler()\n",
    "        \n",
    "        \n",
    "        cv_x_train_list_param[i] = fold_min_max_scaler.fit_transform(cv_x_train_list_param[i])\n",
    "        cv_y_train_param[i] = fold_min_max_scaler_y.fit_transform(cv_y_train_param[i])\n",
    "        \n",
    "        cv_x_train_list_param[i] = cv_x_train_list_param[i].reshape((cv_x_train_list_param[i].shape[0], 1, cv_x_train_list_param[i].shape[1]))\n",
    "        \n",
    "        # Initialize the RNN\n",
    "        regressor = Sequential()\n",
    "\n",
    "        # Adding the input layer and the LSTM layer\n",
    "        regressor.add(LSTM(units = unit_count, input_shape=(cv_x_train_list_param[i].shape[1], cv_x_train_list_param[i].shape[2])))\n",
    "\n",
    "        # Adding the output layer\n",
    "        regressor.add(Dense(units = 1))\n",
    "\n",
    "        # Compiling the RNN\n",
    "        regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "        # Using the training set to train the model\n",
    "        regressor.fit(cv_x_train_list_param[i], cv_y_train_param[i], batch_size = batch_count, epochs = epoch_count)\n",
    "        \n",
    "        \n",
    "        inputs = cv_x_test_list_param[i]\n",
    "        # Reshaping and scaling the inputs\n",
    "        inputs = np.reshape(inputs, (len(inputs), 4))\n",
    "        inputs = fold_min_max_scaler.transform(inputs)\n",
    "        inputs = inputs.reshape((inputs.shape[0], 1, 4))\n",
    "        predicted_price = regressor.predict(inputs)\n",
    "        predicted_price = fold_min_max_scaler_y.inverse_transform(predicted_price)\n",
    "        \n",
    "        \n",
    "        score = metric(y_true = cv_y_test_param[i], y_pred = predicted_price)\n",
    "        cv_scores.append(score)\n",
    "        i += 1\n",
    "    return root_mean_squared_sum_of_list(np.array(cv_scores)) \n",
    "    \n",
    "\n",
    "# Cross Validation function\n",
    "def cross_val_score(cv_x_train_list, cv_x_test_list ,cv_y_train,cv_y_test, metric):\n",
    "\n",
    "    parameters_dict = {\"batch_size\"    : [3,4 ] ,\n",
    "     \"epochs\"        : [ 50,60],\n",
    "     \"units\" : [ 30,40 ]\n",
    "                      }\n",
    "    \n",
    "    cv_scores_dict = {}\n",
    "    i_num = 0\n",
    "    for batch_count in parameters_dict['batch_size']:\n",
    "        for epoch_count in parameters_dict['epochs']:\n",
    "            for unit_count in parameters_dict['units']:\n",
    "                # Various param variables\n",
    "                cv_x_test_list_temp = cv_x_test_list\n",
    "                cv_y_test_temp = cv_y_test\n",
    "                cv_x_train_list_temp = cv_x_train_list\n",
    "                cv_y_train_temp = cv_y_train\n",
    "                \n",
    "                hyper_parameter_string = \"batch_size \"+str(batch_count)+\" epochs \"+str(epoch_count)+\" units \"+str(unit_count)\n",
    "                cv_score_of_model = calculate_cv_scores(cv_x_test_list_temp,cv_y_test_temp, metric,cv_x_train_list_temp,cv_y_train_temp,unit_count,batch_count,epoch_count)\n",
    "                i_num = i_num + 1\n",
    "                print(str(i_num)+\" Error of the model \"+ hyper_parameter_string ,cv_score_of_model)\n",
    "                cv_scores_dict[hyper_parameter_string] = cv_score_of_model\n",
    "    return cv_scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3595\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.3557\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3520\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3483\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3447\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.3411\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3376\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3341\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3307\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3274\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.3241\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3209\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3177\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.3146\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3115\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3085\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.3056\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3027\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2999\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2971\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2944\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2917\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2891\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2865\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2840\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2815\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.2791\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2767\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2744\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2721\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2699\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2677\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2656\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2635\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2615\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2595\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2575\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2556\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2537\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2519\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2501\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2483\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.2466\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2449\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2433\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2417\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2401\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2386\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2371\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2356\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 2s 2ms/step - loss: 0.2310\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3451\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2206\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2160\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.2114\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2069\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.3003\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1983\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2902\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2981\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.2931\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1836\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2842\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.1775\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1746\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2717\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1689\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.2632\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1638\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2562\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2520\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2425\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1553\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2374\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2347\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2365\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2336\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1467\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2288\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2239\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1427\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2202\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2193\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.2185\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2166\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2135\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1367\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2108\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2100\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2064\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2048\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1332\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2044\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2035\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1316\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2005\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1304\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1948\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1961\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.1949\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 2s 2ms/step - loss: 0.4516\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4273\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3117\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3042\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4170\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.2775\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2716\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3274\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3328\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2146\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2590\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3522\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3081\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 996us/step - loss: 0.3025\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3407\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3339\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3277\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2218\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.3100\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.1756\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1726\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2048\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2903\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2888\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.2802\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.1891\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2741\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.2412\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1796\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2595\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.1727\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1461\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1673\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1424\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2408\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2358\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2124\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1544\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1348\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1336\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1478\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2186\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2021\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.2081\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1397\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2079\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2055\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1973\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2014\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1997\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE435D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 1s 1ms/step - loss: 0.3573\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4333\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.4183\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.3257\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.3076\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3070\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.3242\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.3136\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 996us/step - loss: 0.2401\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.2660\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2885\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2772\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.3301\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.1979\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2533\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2445\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2514\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 998us/step - loss: 0.2310\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2235\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2258\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1582\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.2097\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2523\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1972\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2355\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1746\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2143\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.2084\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1543\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1903\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1571\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1874\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1103\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1519\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1476\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.1672\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1391\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1035\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.1539\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1249\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 996us/step - loss: 0.1461\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1167\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1129\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 994us/step - loss: 0.1410\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.0960\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1070\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1092\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.1190\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.1027\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 997us/step - loss: 0.0923\n",
      "WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE571EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 2s 1ms/step - loss: 0.4030\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.4346\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4126\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4486\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.3049\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.4022\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 999us/step - loss: 0.4059\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.3420\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.3045\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2881\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.3378\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.3101\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2589\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2663\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2416\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2319\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2522\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2163\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.1732\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2104\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1938\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2287\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2102\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1973\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1815\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1713\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.1512\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1586\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1140\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1340\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1188\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1312\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0984\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1420\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.1180\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1057\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.1324\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1198\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1179\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1148\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0978\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0849\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0898\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0777\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.0808\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0963\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0852\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0951\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1020\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0884\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B641D3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 2s 2ms/step - loss: 0.3777\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4050\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2635\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2390\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3440\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2226\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3523\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.2187\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2154\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2768\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2168\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2758\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1676\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1340\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1818\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1387\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1526\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1489\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.1183\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0827\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1236\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0999\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0950\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0932\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0875\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0813\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0712\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0655\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0715\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0969\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0891\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0599\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0635\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0878\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0658\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0728\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0820\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0796\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0721\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 994us/step - loss: 0.0653\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0742\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0608\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0527\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0788\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0814\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0714\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0521\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0636\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0654\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0468\n",
      "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B61FAB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "3/3 [==============================] - 2s 1ms/step - loss: 0.2329\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.2663\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1653\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.2915\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.1716\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2418\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.2390\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.1376\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.1792\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1162\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1569\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.0933\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1154\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.1302\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.1383\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0835\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0897\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.1186\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.1312\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.1281\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0702\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0959\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0416\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0873\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0541\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.0869\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0600\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.0584\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0810\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0676\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0775\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0713\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0777\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.0747\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.0375\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0445\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0613\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0474\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.0478\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0665\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.0410\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.0492\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0565\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0492\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0696\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0452\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0645\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0527\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 998us/step - loss: 0.0470\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 997us/step - loss: 0.0670\n",
      "WARNING:tensorflow:9 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B8A46280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "4/4 [==============================] - 2s 2ms/step - loss: 0.1615\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1881\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1376\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1782\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.2189\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1536\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1406\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1372\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0944\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1546\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1532\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.1097\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0773\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1450\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.1172\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0921\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.1002\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0855\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0716\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0808\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0611\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0631\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0754\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0725\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0608\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0783\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0616\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0808\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0703\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0565\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0699\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0577\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0652\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0509\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.0647\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0636\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0669\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0504\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.0611\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0630\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0759\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0613\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0556\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0435\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0612\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0461\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.0450\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 665us/step - loss: 0.0543\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0512\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0522\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE1C61F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "4/4 [==============================] - 2s 1ms/step - loss: 0.4321\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3012\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3557\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2616\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2990\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2935\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2351\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2856\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2120\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1668\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1542\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1857\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1677\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1351\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1241\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1464\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1879\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1304\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1083\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1206\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1358\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1084\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0979\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0983\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1001\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0860\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0736\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0689\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0734\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0792\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0791\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0951\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0789\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0813\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0702\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0741\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0900\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0718\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0653\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0613\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0684\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0572\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0685\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0701\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0691\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0669\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0770\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0664\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0679\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0760\n",
      "WARNING:tensorflow:10 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AEBA9E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "4/4 [==============================] - 1s 998us/step - loss: 0.4198\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.3929\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.2205\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.3450\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2906\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.2560\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.2406\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.2048\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2639\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.2560\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1603\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.2086\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1920\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1886\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.2090\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1467\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1787\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.1005\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 665us/step - loss: 0.1510\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1305\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0874\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1118\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0871\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1103\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1020\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0925\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0928\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.0946\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.1313\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0754\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0561\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0933\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0961\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0860\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0733\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0868\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0892\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0613\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0746\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.0896\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0819\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0611\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0598\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 998us/step - loss: 0.0847\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0570\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0731\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0727\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0812\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 997us/step - loss: 0.0573\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0840\n",
      "WARNING:tensorflow:11 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B8A97CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 2s 1ms/step - loss: 0.3692\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.3940\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.4158\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.3624\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.3711\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2183\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.3125\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.3307\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1646\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.2413\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1877\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.2474\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 998us/step - loss: 0.1372\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1519\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1716\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1037\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1668\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1159\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 998us/step - loss: 0.0963\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0960\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0902\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0965\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0636\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.1174\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0965\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0973\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0976\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0616\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0784\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0916\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0835\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0875\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0738\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0711\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0682\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0636\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0865\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0836\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0668\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0739\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0805\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0761\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 748us/step - loss: 0.0789\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0608\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0678\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0520\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0748\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0699\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0756\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B4976E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 2s 1ms/step - loss: 0.3184\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.3438\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.2219\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.2793\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.3299\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1552\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.2212\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1385\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.2000\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1580\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1638\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1454\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0867\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0699\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1375\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0879\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0668\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0616\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0745\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0880\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0676\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0814\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0702\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0809\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0779\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0488\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0825\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0561\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0534\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0550\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0699\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0437\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0675\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0726\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0659\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0765\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0659\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0674\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0565\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0645\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0834\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0554\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0511\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0664\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0696\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0702\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0551\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0733\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0420\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0695\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B1D49B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 997us/step - loss: 0.3891\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 748us/step - loss: 0.3585\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.2322\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.2663\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 998us/step - loss: 0.2762\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.2080\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1126\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.1611\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.1367\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1751\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.1424\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0966\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.1321\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0780\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0719\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0745\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 748us/step - loss: 0.0369\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0614\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0601\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0773\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 748us/step - loss: 0.0713\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0695\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0704\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0792\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0678\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 748us/step - loss: 0.0686\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 748us/step - loss: 0.0509\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0634\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0652\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0473\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0690\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0508\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0711\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 984us/step - loss: 0.0609\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0598\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0695\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0539\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.0691\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0527\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0614\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0732\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0555\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 748us/step - loss: 0.0536\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0516\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0560\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0689\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0538\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0609\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0626\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 997us/step - loss: 0.0658\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE1C6940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "6/6 [==============================] - 2s 997us/step - loss: 0.4383\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.2342\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3217\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3298\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2400\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.2750\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 998us/step - loss: 0.2453\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2066\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2215\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.1252\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1554\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.1275\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.1433\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0994\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0944\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0913\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0922\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0570\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 998us/step - loss: 0.0538\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0585\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0809\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0729\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0617\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0624\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0632\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0540\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0665\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0502\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0526\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0586\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0477\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0711\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0595\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0441\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0618\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0629\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0515\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0535\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0577\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0518\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0462\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0454\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0652\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0646\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0626\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0432\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0450\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0392\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0577\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0591\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B60F8820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "6/6 [==============================] - 2s 1ms/step - loss: 0.3485\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2684\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2430\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2294\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2213\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1497\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1807\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.1665\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1070\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1117\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1027\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1253\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0772\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0682\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0685\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0468\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0459\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0447\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0430\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0426\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0439\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0432\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0457\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0547\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0373\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0361\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0257\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0550\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0409\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0475\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0368\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0311\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0422\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0334\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0378\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0285\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0324\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0445\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 998us/step - loss: 0.0322\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0334\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0453\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0354\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0412\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0452\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0405\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0394\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0264\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0396\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0453\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0367\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B75460D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "6/6 [==============================] - 1s 913us/step - loss: 0.2373\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.2527\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.1763\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.1182\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1589\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1040\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.1374\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0758\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.1013\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0971\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0639\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0481\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0477\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0499\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0379\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0326\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0334\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0250\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0380\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0319\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0237\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0270\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0292\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0287\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0215\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0339\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0289\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0289\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0203\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0277\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0165\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0270\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0210\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0258\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0230\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 908us/step - loss: 0.0235\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0248\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0206\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0162\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0286\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0196\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0174\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0217\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0197\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0197\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0140\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0164\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 997us/step - loss: 0.0124\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0205\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 798us/step - loss: 0.0227\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE61FCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 2s 1ms/step - loss: 0.1620\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2236\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.2097\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1272\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1134\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1388\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0744\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0757\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0439\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0407\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0288\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0757\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0580\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0349\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0225\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0301\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0272\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0260\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0374\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0261\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0228\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0221\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0232\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0195\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0269\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0199\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0270\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0222\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0188\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0137\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0164\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 999us/step - loss: 0.0186\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0332\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0202\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0161\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0207\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0141\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0280\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0151\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0195\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0210\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0128\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0117\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0177\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0181\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0157\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0159\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0224\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0161\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE1C6A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 2s 1ms/step - loss: 0.1922\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1780\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.1152\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.1671\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.1106\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0520\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0986\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0496\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0919\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0509\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0236\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0518\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0429\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0269\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0392\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0323\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0420\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0459\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0252\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0287\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0200\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0575\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0268\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0370\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0227\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0226\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0245\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0242\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0212\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0416\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0217\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0266\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0262\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0161\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0158\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0157\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0141\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0145\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0176\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0250\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0156\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0209\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0157\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0310\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0148\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 998us/step - loss: 0.0211\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0225\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0144\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B5C650D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "7/7 [==============================] - 2s 997us/step - loss: 0.2018\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.1533\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 996us/step - loss: 0.1723\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0800\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.1190\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0446\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0420\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0390\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0888\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0799\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0190\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0441\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0174\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0366\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 797us/step - loss: 0.0186\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0262\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0589\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0313\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0545\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 665us/step - loss: 0.0515\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0167\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0300\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 998us/step - loss: 0.0258\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0454\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0165\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0471\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0176\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0284\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0178\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0317\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0133\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0202\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0390\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0215\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0172\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0173\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0361\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0213\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.014 - 0s 997us/step - loss: 0.0128\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 863us/step - loss: 0.0333\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0218\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0109\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0159\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0139\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0139\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 831us/step - loss: 0.0141\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0151\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0119\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.0130\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 997us/step - loss: 0.0128\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE5AACA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 2s 997us/step - loss: 0.2213\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0668\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1105\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0951\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0708\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0633\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0555\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.1219\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0562\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0624\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0608\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0465\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0318\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.0281\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0230\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 996us/step - loss: 0.0432\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0384\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0356\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0391\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0218\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0440\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0273\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0337\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0370\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0568\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.0280\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0265\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0320\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0206\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0323\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0232\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0309\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0404\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0380\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0356\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0218\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0286\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0253\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0242\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0242\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0317\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0277\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0217\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0369\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0318\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0210\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0478\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0335\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0241\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B2F30F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 2s 1ms/step - loss: 0.1779\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0712\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1036\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0283\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0626\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0282\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0480\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0353\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0282\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0206\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0309\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0348\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0559\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0327\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0296\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0173\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0414\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0333\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0262\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0290\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0314\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0289\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0240\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0310\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0470\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0140\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0454\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0216\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0240\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0189\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0175\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0230\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0216\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0165\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0202\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0218\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0207\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0146\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0209\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0157\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0380\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0327\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.0292\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.0221\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 996us/step - loss: 0.0256\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0197\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.006 - 0s 997us/step - loss: 0.0302\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0217\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0280\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0187\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B5DEFC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 855us/step - loss: 0.1956\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 996us/step - loss: 0.1177\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0808\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0462\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0414\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.1115\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0802\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0525\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.1112\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0637\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0802\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0573\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0505\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0692\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.0433\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 855us/step - loss: 0.0558\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0600\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0901\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0601\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 854us/step - loss: 0.0601\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0504\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0381\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0627\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0647\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.0553\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0276\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0461\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0350\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0472\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0412\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0424\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0234\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.0324\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0501\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0455\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 927us/step - loss: 0.0281\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0310\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0352\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0258\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.0332\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0310\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0384\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0228\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0246\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 997us/step - loss: 0.0294\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 874us/step - loss: 0.0417\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0291\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0416\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.0525\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0363\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B456AF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 2s 997us/step - loss: 0.2637\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.2046\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.2407\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.2269\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.1142\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0873\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0913\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0903\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0746\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0826\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0901\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0685\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0476\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0514\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0560\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0572\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0484\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0470\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0649\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0540\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0525\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0492\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 0s 947us/step - loss: 0.0366\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0357\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0386\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0560\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0514\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0379\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0606\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0521\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0339\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0405\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0411\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0392\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0465\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0422\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0353\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 0s 872us/step - loss: 0.0374\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0382\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0362\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0326\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0423\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0529\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0537\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0335\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0366\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0265\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0501\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0385\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0297\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE571700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9/9 [==============================] - 2s 1ms/step - loss: 0.3462\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.2527\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.2075\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.1453\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.1197\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.1080\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 0s 998us/step - loss: 0.1100\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0990\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.1241\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0923\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0853\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0740\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0668\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0644\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 0s 998us/step - loss: 0.0580\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0581\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0600\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0534\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0714\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0486\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0537\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0661\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0632\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0579\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0581\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0481\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0740\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0356\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0593\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0401\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0374\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0508\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0376\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0283\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0660\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0573\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0468\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0397\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0316\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0306\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0460\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0472\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0376\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0351\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0442\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0322\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0438\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0209\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0392\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0279\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B456A0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 1s 873us/step - loss: 0.3146\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.2990\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.2199\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.1325\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.1324\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 0s 872us/step - loss: 0.1551\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.1150\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 0s 748us/step - loss: 0.0779\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.1152\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0870\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.1020\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 0s 748us/step - loss: 0.0894\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 0s 818us/step - loss: 0.0641\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0731\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0612\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0699\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 0s 748us/step - loss: 0.0732\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 0s 872us/step - loss: 0.0587\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 0s 748us/step - loss: 0.0595\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0669\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0739\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0594\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.0648\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 0s 872us/step - loss: 0.0469\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0629\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0448\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0536\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0495\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0444\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0563\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0519\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0613\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0459\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0406\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0412\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0488\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0413\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0304\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0514\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0338\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0275\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0426\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0531\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0332\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 748us/step - loss: 0.0400\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 0s 748us/step - loss: 0.0276\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 0s 997us/step - loss: 0.0270\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0549\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 0s 873us/step - loss: 0.0302\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 0s 872us/step - loss: 0.0343\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B1D494C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 2s 997us/step - loss: 0.1692\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.1827\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.1248\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0881\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.1007\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0705\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0640\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0787\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0761\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0629\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0717\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0954\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0612\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0615\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0666\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0673\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0646\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0542\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0546\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0465\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0504\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0452\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0647\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0481\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0420\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0447\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0499\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0505\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0537\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0499\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0421\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0328\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0325\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0458\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0386\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0397\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0283\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0380\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0343\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0338\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0340\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0346\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0221\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0362\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0406\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0357\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0443\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0456\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 886us/step - loss: 0.0294\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0372\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE67BEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 1s 1ms/step - loss: 0.2859\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.2126\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 998us/step - loss: 0.2146\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.1638\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.1262\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0826\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0693\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0651\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0823\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 951us/step - loss: 0.0603\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0562\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0711\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0695\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0529\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0529\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0521\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0684\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0542\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 975us/step - loss: 0.0553\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0717\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0475\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0478\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0418\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0410\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0467\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0463\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0511\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0357\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0476\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0473\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0405\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 998us/step - loss: 0.0297\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0354\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0528\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0296\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0364\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0355\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0409\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0425\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0311\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0390\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0355\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0510\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0280\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0417\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0429\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0399\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0267\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0332\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0458\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B5E12B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 1s 776us/step - loss: 0.2816\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 886us/step - loss: 0.1996\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.1942\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 885us/step - loss: 0.1505\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.1442\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.1230\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.1735\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0945\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 886us/step - loss: 0.0977\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 886us/step - loss: 0.0758\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0904\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0768\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 802us/step - loss: 0.0642\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0647\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0649\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0579\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 886us/step - loss: 0.0466\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0471\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 886us/step - loss: 0.0523\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 886us/step - loss: 0.0493\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0470\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0503\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 886us/step - loss: 0.0508\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0492\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0539\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 886us/step - loss: 0.0455\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0455\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0477\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0363\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0393\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0450\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0293\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0348\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 997us/step - loss: 0.0458\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.0264\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 886us/step - loss: 0.0345\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 886us/step - loss: 0.0254\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0406\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0271\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0357\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 886us/step - loss: 0.0331\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0433\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 887us/step - loss: 0.0311\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0262\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0373\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0221\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0274\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0364\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0455\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 776us/step - loss: 0.0445\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE46BCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "11/11 [==============================] - 2s 1ms/step - loss: 0.2496\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.1372\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.1613\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.1298\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.1232\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.1328\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.1072\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0846\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0702\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0702\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0693\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0787\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0562\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0685\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0577\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0623\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0558\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0575\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0465\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0547\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0509\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0460\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0504\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0438\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.032 - 0s 997us/step - loss: 0.0347\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0480\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0499\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0434\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0463\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0449\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0327\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 0s 897us/step - loss: 0.0368\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0393\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0365\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0306\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0257\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0418\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0255\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0344\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0344\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0292\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0268\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0399\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0229\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0192\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0231\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0352\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 0s 948us/step - loss: 0.0301\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0290\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0306\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B61FB430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "11/11 [==============================] - 2s 1ms/step - loss: 0.1788\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0943\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0899\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0404\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0496\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0448\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0630\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0415\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0488\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0406\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0347\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0406\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0408\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0350\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0351\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0242\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0278\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.042 - 0s 1ms/step - loss: 0.0290\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0275\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0305\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0209\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0250\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0214\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0220\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0232\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0178\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0161\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0155\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0109\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0176\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0157\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0200\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0163\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0159\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0188\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0173\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0141\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0160\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0174\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0149\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0175\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0221\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0142\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0177\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0168\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0119\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE5AA0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "11/11 [==============================] - 1s 898us/step - loss: 0.1596\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 0s 897us/step - loss: 0.0985\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0822\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0714\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0400\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0380\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0357\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0339\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0272\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0277\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0480\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 0s 821us/step - loss: 0.0273\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0345\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0256\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0396\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0315\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0313\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0229\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0181\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0183\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 0s 849us/step - loss: 0.0210\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0229\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0184\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0213\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0205\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0188\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0226\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0195\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0216\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0154\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0154\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0170\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0153\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0086\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 0s 698us/step - loss: 0.0173\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0163\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0119\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0124\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0168\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0134\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0162\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0187\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0160\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0170\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0152\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 0s 997us/step - loss: 0.0124\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0119\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0134\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 0s 798us/step - loss: 0.0139\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 0s 898us/step - loss: 0.0110\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B43C2E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 2s 995us/step - loss: 0.1487\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 966us/step - loss: 0.1011\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.1039\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0642\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0507\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0398\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0544\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0415\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0388\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0387\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0329\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0466\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0298\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0383\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0296\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.022 - 0s 1ms/step - loss: 0.0264\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0233\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0344\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0232\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0198\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.045 - 0s 1ms/step - loss: 0.0241\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0213\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0210\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0155\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0155\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0164\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0194\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0142\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0168\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0145\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0149\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0157\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0148\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0190\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0145\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0135\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0164\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0123\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0136\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0142\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0179\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0129\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0206\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0138\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0151\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0093\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B321D430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 2s 1ms/step - loss: 0.1574\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 983us/step - loss: 0.1330\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.1203\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.1188\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0620\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0662\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0494\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0507\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0389\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0332\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0367\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0487\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0491\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0243\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0338\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0351\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0424\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0288\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0373\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0254\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0233\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0204\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0199\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0207\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0193\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0185\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0165\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0169\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0167\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0180\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0153\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0222\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0203\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0227\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0128\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0121\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0117\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0157\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0139\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0245\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0203\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0111\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0130\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0173\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0163\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0144\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0121\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0190\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0105\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0165\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE461AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 1s 907us/step - loss: 0.2957\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 725us/step - loss: 0.1446\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 896us/step - loss: 0.1729\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0989\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 725us/step - loss: 0.0898\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0468\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0486\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 725us/step - loss: 0.0697\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0427\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0501\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0459\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 887us/step - loss: 0.0476\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0490\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 725us/step - loss: 0.0414\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0354\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0506\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0612\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0342\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0520\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 896us/step - loss: 0.0295\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0475\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0248\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0296\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0340\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0317\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0236\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0257\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.013 - 0s 816us/step - loss: 0.0282\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0264\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0190\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0237\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0215\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0244\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0182\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0130\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0215\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0236\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.0268\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0262\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0231\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 997us/step - loss: 0.0151\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0268\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0315\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0132\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0168\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0149\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0280\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 816us/step - loss: 0.0250\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0142\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 907us/step - loss: 0.0217\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE46BC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "13/13 [==============================] - 2s 997us/step - loss: 0.1633\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.1968\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.1267\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.1124\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0709\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0753\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0548\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0481\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0604\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0601\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0476\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0469\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0552\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0470\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0391\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0464\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0322\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0395\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0288\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0280\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0246\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 972us/step - loss: 0.0382\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0254\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0245\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0322\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0233\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0204\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0388\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0255\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0274\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0216\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0163\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0346\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0228\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0215\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0258\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0191\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0129\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0209\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0187\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0156\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0169\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0147\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0188\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0198\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0259\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0208\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0216\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0150\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B4AE34C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "13/13 [==============================] - 2s 997us/step - loss: 0.1977\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 912us/step - loss: 0.0903\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0890\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0912\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0932\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0551\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0426\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0453\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0443\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0501\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0529\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0436\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0349\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0397\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0375\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0341\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 958us/step - loss: 0.0337\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0283\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0239\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0276\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0323\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0286\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0207\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0248\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0226\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0224\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0215\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0153\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0236\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0113\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0196\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0243\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0161\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0218\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0145\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0168\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0230\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0138\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0171\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0174\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0187\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0113\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0119\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0132\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 0.0194\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0157\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0118\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0262\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 997us/step - loss: 0.0153\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AD8664C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "13/13 [==============================] - 1s 748us/step - loss: 0.2481\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.2152\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.1185\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 859us/step - loss: 0.0756\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0761\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0641\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0530\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0415\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0388\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0624\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0340\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 814us/step - loss: 0.0462\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0386\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0349\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0388\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0400\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0276\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0222\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0309\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0216\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0229\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0215\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0341\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0287\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0209\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0223\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0229\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0122\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 842us/step - loss: 0.0198\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0313\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0180\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0226\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0155\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0166\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0184\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0192\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 914us/step - loss: 0.0216\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0241\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0189\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0221\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0165\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0147\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.004 - 0s 831us/step - loss: 0.0168\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0234\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0197\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0231\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0112\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0260\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 748us/step - loss: 0.0188\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 831us/step - loss: 0.0167\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AD81FE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 2s 1ms/step - loss: 0.2226\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.1491\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.1358\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.1130\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0753\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0737\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0787\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0491\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0669\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0454\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0502\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0354\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0365\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0322\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0341\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0303\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0231\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0335\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0327\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0346\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0294\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 920us/step - loss: 0.0227\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0192\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0175\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0347\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0258\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0217\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0241\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0179\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0257\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0179\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0271\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0136\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0275\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0310\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0207\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0186\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0153\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0189\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0149\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0185\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0140\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0210\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0250\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0256\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0215\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0225\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0148\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0238\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE46BDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 2s 921us/step - loss: 0.2600\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2005\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.1294\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0553\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0574\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0516\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0437\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0406\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0368\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0373\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0372\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0272\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0306\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0352\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0279\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0307\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0240\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0184\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0303\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 960us/step - loss: 0.0188\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0250\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0166\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 997us/step - loss: 0.0235\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0143\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0125\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0195\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0121\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0149\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0136\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0206\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0107\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0138\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0122\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0135\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0221\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0207\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0102\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0198\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 960us/step - loss: 0.0098\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0203\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0203\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0162\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 997us/step - loss: 0.0108\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0149\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0097\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.0120\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AD866940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 2s 767us/step - loss: 0.1261\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0789\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0716\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0715\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0690\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0591\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0401\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0396\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0439\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0319\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0284\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0203\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0272\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0257\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0222\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0159\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 823us/step - loss: 0.0127\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0113\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0177\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0141\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0131\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0164\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0134\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0119\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0081\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0150\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0117\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0129\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0121\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0087\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0132\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0108\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0089\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0111\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0103\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0120\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0131\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0091\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0086\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0092\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0134\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0073\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 767us/step - loss: 0.0099\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0083\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0081\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0122\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0086\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0097\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 921us/step - loss: 0.0085\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 844us/step - loss: 0.0101\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B323A550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 2s 926us/step - loss: 0.0767\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0938\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0752\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0575\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0382\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 0s 962us/step - loss: 0.0389\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0338\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0318\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0270\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0287\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0228\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0252\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0183\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0169\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0176\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0118\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0121\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0155\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0107\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0163\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0122\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0071\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0112\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0070\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0087\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0080\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0139\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0101\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0113\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0095\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0123\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0136\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0138\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0086\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0082\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0121\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0139\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0110\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0092\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0096\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0102\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0099\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0103\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0085\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0068\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0095\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0071\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0098\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B60F83A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 2s 997us/step - loss: 0.1388\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.1411\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0821\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0481\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0538\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0406\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0324\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0404\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0258\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0268\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0224\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0308\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0262\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0230\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0246\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 0s 917us/step - loss: 0.0197\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0158\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0211\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0142\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0130\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0120\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0100\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0132\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0138\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0093\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0092\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0079\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0090\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0126\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0099\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0112\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0082\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0091\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0079\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0103\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0079\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0105\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0069\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0081\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0057\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0097\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0070\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0060\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0063\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0059\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0080\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0106\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0067\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0092\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0082\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B3137C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 1s 784us/step - loss: 0.1505\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0891\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0933\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0445\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0671\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 0s 783us/step - loss: 0.0359\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 0s 712us/step - loss: 0.0325\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0325\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0495\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0255\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0247\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0255\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0234\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0301\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 0s 915us/step - loss: 0.0196\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0205\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0218\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0140\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0173\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0113\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0115\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 0s 810us/step - loss: 0.0112\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0086\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 0s 784us/step - loss: 0.0089\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0096\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 0s 784us/step - loss: 0.0071\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0073\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0063\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 0s 813us/step - loss: 0.0067\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0061\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 0s 784us/step - loss: 0.0069\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0072\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0073\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0081\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 0s 997us/step - loss: 0.0045\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0061\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0047\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 0s 784us/step - loss: 0.0049\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 0s 784us/step - loss: 0.0070\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0057\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 0s 784us/step - loss: 0.0049\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0064\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0054\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 0s 784us/step - loss: 0.0082\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 0s 784us/step - loss: 0.0060\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 0s 784us/step - loss: 0.0038\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 0s 855us/step - loss: 0.0054\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0048\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 0s 926us/step - loss: 0.0051\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 0s 784us/step - loss: 0.0058\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B321D790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 2s 931us/step - loss: 0.2292\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1167\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1393\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1273\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0906\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0605\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0648\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0580\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0510\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.0647\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0512\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0410\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0472\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0487\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0403\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0358\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0470\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0308\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0342\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0312\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0318\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0178\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0213\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0199\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0131\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0164\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0130\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0111\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0123\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0148\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0097\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0119\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0074\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0092\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0103\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0087\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0090\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0082\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0087\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.0066\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0072\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0055\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0090\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0071\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0071\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.0053\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0071\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.0079\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0077\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0061\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B4AE3700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 2s 931us/step - loss: 0.1372\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0874\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0868\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.0562\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0389\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0390\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0511\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0381\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0320\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0338\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0261\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0203\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 980us/step - loss: 0.0247\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0216\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0170\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0174\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0161\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0157\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0096\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0104\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.0116\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0120\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0094\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0107\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0057\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0067\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0066\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0081\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0060\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0086\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 930us/step - loss: 0.0050\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0071\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0062\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0052\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0064\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0060\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0066\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0053\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0053\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.0059\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0069\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0065\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0061\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0067\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0056\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0050\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 997us/step - loss: 0.0050\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 946us/step - loss: 0.0060\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0060\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0042\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B456A280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 798us/step - loss: 0.1259\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0822\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 832us/step - loss: 0.0532\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.0454\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0330\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.0304\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 931us/step - loss: 0.0315\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0218\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.0226\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0294\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 798us/step - loss: 0.0247\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 864us/step - loss: 0.0241\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0169\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0140\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0155\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0113\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 766us/step - loss: 0.0104\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0084\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0070\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0058\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0037\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0069\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0070\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0046\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 772us/step - loss: 0.0044\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0048\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0055\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0059\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0042\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0036\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0033\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0038\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0032\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0032\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0027\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0034\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0036\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0044\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0031\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0034\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0039\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0040\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0035\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0023\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 798us/step - loss: 0.0045\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0040\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0035\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0032\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0028\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 731us/step - loss: 0.0034\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE46BD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 935us/step - loss: 0.0679\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0342\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0280\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0395\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0219\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0153\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0221\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0311\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0116\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0149\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0072\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0149\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 940us/step - loss: 0.0131\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0059\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0069\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0052\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0055\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 987us/step - loss: 0.0053\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0036\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0025\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0032\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0053\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0037\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0033\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0056\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0133\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0082\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0033\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0073\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0062\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0044\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0043\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0080\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0078\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0028\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0039\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0032\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0076\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0083\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0048\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0082\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0049\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0042\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0047\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0077\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0056\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0050\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0040\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0024\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0060\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B2FFBAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 935us/step - loss: 0.0436\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0553\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0306\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0277\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0189\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0222\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 971us/step - loss: 0.0131\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0131\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0232\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0160\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0331\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0073\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0346\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0109\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0204\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0098\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0138\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0094\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0120\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0043\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0099\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0081\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0042\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 982us/step - loss: 0.0080\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0048\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0128\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0192\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0047\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0055\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0087\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0054\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0088\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0099\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0052\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 967us/step - loss: 0.0126\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0060\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0056\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0088\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0090\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0069\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0070\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0048\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0084\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0099\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0114\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0050\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0131\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0172\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.0136\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 997us/step - loss: 0.0041\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE571F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 800us/step - loss: 0.0624\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0205\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0398\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0549\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0469\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0407\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 935us/step - loss: 0.0293\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 905us/step - loss: 0.0472\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0258\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0223\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0388\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0315\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0265\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - ETA: 0s - loss: 0.003 - 0s 843us/step - loss: 0.0083\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0188\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0219\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0261\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0294\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0199\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0240\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 774us/step - loss: 0.0211\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0115\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0081\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0086\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0096\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0087\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0294\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0129\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0109\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0049\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0233\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0154\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0101\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0111\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0179\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0083\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0092\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0074\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0091\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0104\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0089\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0197\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0114\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0044\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0043\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0120\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0056\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 810us/step - loss: 0.0059\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 873us/step - loss: 0.0042\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 748us/step - loss: 0.0046\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE46BCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 2s 997us/step - loss: 0.0807\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0929\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0384\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0576\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0481\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0299\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0519\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0498\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0206\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0453\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0214\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0250\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0306\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0155\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0182\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0236\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0249\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0140\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0127\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0220\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0251\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0125\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0063\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.001 - 0s 997us/step - loss: 0.0117\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0056\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0081\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0153\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0144\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0223\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 880us/step - loss: 0.0225\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0084\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0121\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0130\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0089\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0129\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 880us/step - loss: 0.0059\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0214\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0096\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0101\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0089\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0065\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0146\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0072\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0089\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0132\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0047\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0062\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0078\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0085\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0116\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AEB119D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 2s 921us/step - loss: 0.0798\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0827\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0264\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0413\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0587\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0428\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0426\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0286\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 880us/step - loss: 0.0301\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0444\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0247\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 971us/step - loss: 0.0362\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0188\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0394\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0255\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0199\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 951us/step - loss: 0.0300\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0096\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0107\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0196\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0074\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 910us/step - loss: 0.0052\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 880us/step - loss: 0.0059\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0190\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0054\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0069\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 910us/step - loss: 0.0070\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0219\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0058\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0064\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0111\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0197\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0076\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0063\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0188\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0178\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 880us/step - loss: 0.0059\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0046\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0140\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0090\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0083\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0049\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0046\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.0037\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0066\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0077\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0042\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0049\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 880us/step - loss: 0.0160\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE1C6A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 821us/step - loss: 0.1609\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 762us/step - loss: 0.1371\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.1145\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0505\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 880us/step - loss: 0.0744\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0567\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0333\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0449\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0629\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 704us/step - loss: 0.0422\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0273\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 741us/step - loss: 0.0432\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0363\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 704us/step - loss: 0.0324\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0237\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 704us/step - loss: 0.0405\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0300\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0150\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 803us/step - loss: 0.0103\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0232\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0190\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0086\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0180\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0124\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0104\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 880us/step - loss: 0.0091\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0147\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0062\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0093\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0053\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 880us/step - loss: 0.0071\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.0134\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.0144\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 880us/step - loss: 0.0062\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 880us/step - loss: 0.0068\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 880us/step - loss: 0.0109\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 795us/step - loss: 0.0048\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0050\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 704us/step - loss: 0.0049\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0063\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0082\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0051\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 853us/step - loss: 0.0050\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0141\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0103\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0054\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 763us/step - loss: 0.0050\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 704us/step - loss: 0.0092\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0070\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 821us/step - loss: 0.0050\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE9E8EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 2s 997us/step - loss: 0.2005\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.1252\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0794\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0680\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0492\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0288\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 943us/step - loss: 0.0494\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0333\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0251\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0346\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0208\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0180\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0158\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0177\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0121\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0195\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0184\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0138\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.0225\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0101\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0168\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0043\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0178\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0082\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0074\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0062\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0083\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0055\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0144\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0056\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0077\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0133\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0057\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0044\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0061\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0071\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0087\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0112\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0111\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.0094\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0115\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0132\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0062\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0095\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B63D1E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 2s 942us/step - loss: 0.0840\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0702\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0923\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0493\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0623\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0425\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0413\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0605\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0427\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0354\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0240\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0204\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0213\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.0160\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0146\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0123\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0166\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0165\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0083\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 943us/step - loss: 0.0064\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0060\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0059\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0130\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0089\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0070\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0063\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0067\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0064\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0076\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0067\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0118\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0189\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0079\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0055\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 970us/step - loss: 0.0078\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.0041\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0058\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0088\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0052\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0178\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0091\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0137\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0047\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0060\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0067\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0062\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0082\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 942us/step - loss: 0.0061\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 997us/step - loss: 0.0063\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE8F93A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 1s 776us/step - loss: 0.1478\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.1416\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.1221\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0551\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0487\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0493\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0346\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0326\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0643\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0259\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.0369\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0262\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.0187\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0243\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0158\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0106\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.0279\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0093\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0061\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0144\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0138\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0143\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0053\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0182\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0100\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0078\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0044\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0136\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0092\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0069\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.0129\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0079\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0133\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0080\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0195\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0137\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.0142\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.0053\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.0084\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.0102\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.0100\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0120\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0125\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.0081\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 720us/step - loss: 0.0075\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.0062\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 831us/step - loss: 0.0071\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 776us/step - loss: 0.0057\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 886us/step - loss: 0.0047\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 886us/step - loss: 0.0117\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE435EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 2s 945us/step - loss: 0.1149\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0860\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0771\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0505\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0516\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 925us/step - loss: 0.0504\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0428\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0329\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0321\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0340\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0336\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0203\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0196\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0145\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0231\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0186\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0125\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0146\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0266\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0198\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 904us/step - loss: 0.0132\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0134\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0221\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0084\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0051\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0065\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0134\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0209\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0128\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0120\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0108\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0129\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0129\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0166\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0220\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0167\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0138\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0098\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0076\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0043\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0145\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 840us/step - loss: 0.0109\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0140\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0095\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0068\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0087\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0108\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0083\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0103\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0126\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B88E24C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 2s 945us/step - loss: 0.1244\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0918\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0633\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0646\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0592\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0728\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0356\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0421\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0286\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0317\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0320\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0158\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0192\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0149\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0128\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0244\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0124\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0062\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0125\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0195\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0078\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0149\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 840us/step - loss: 0.0110\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0112\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0073\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0073\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0112\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0115\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0095\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0146\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0078\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0204\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0053\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0127\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0122\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0097\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0087\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0061\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 840us/step - loss: 0.0122\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0083\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0073\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0093\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0069\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0060\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0124\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0093\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0101\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 945us/step - loss: 0.0131\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 892us/step - loss: 0.0089\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0061\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B46659D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 1s 735us/step - loss: 0.1344\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 729us/step - loss: 0.0721\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 787us/step - loss: 0.0972\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 682us/step - loss: 0.0398\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0436\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0492\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0397\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 770us/step - loss: 0.0340\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0341\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0234\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 735us/step - loss: 0.0286\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0252\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 682us/step - loss: 0.0167\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0141\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0136\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 787us/step - loss: 0.0092\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 787us/step - loss: 0.0117\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0114\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 682us/step - loss: 0.0146\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 713us/step - loss: 0.0152\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 787us/step - loss: 0.0135\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0188\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0052\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 787us/step - loss: 0.0167\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0118\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 765us/step - loss: 0.0128\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0056\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0159\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 787us/step - loss: 0.0127\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 840us/step - loss: 0.0110\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0077\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 737us/step - loss: 0.0068\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0083\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 787us/step - loss: 0.0128\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0057\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0076\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 787us/step - loss: 0.0071\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 762us/step - loss: 0.0105\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 787us/step - loss: 0.0110\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 787us/step - loss: 0.0124\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0113\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 840us/step - loss: 0.0134\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 787us/step - loss: 0.0076\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0143\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0101\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0141\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0099\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0144\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 787us/step - loss: 0.0052\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 735us/step - loss: 0.0098\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B4AE3310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 1s 898us/step - loss: 0.1012\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.1063\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0562\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0462\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0436\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0359\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0294\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0242\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0229\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0300\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0235\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0340\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0194\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0148\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 947us/step - loss: 0.0152\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0104\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0117\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0111\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0118\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0181\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0083\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0093\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0102\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0164\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0181\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0080\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0072\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0141\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0126\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 947us/step - loss: 0.0089\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 947us/step - loss: 0.0117\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0100\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0133\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0065\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 947us/step - loss: 0.0115\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0065\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0155\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0083\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0086\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0106\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 997us/step - loss: 0.0093\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 947us/step - loss: 0.0215\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 947us/step - loss: 0.0047\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0068\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0136\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0138\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0167\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0194\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0117\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 947us/step - loss: 0.0095\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B5C21E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21/21 [==============================] - 2s 898us/step - loss: 0.1204\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0964\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0637\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0678\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0432\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0650\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0275\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 861us/step - loss: 0.0261\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0245\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0308\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0345\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0346\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0226\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0126\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0148\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0168\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0099\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0145\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0084\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0173\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0062\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0168\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0122\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0089\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0062\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0088\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0156\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0112\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0063\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 947us/step - loss: 0.0098\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0106\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 947us/step - loss: 0.0133\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0132\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0053\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0061\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0076\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0069\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 947us/step - loss: 0.0052\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0080\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 948us/step - loss: 0.0080\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0147\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0202\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0068\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0066\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0128\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0059\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0129\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0103\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 898us/step - loss: 0.0063\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0111\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE8F9A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 1s 798us/step - loss: 0.1114\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 798us/step - loss: 0.1007\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 798us/step - loss: 0.0650\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0446\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0289\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0383\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0395\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 798us/step - loss: 0.0260\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0247\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 798us/step - loss: 0.0214\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0145\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0129\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 698us/step - loss: 0.0151\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0146\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 798us/step - loss: 0.0153\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 698us/step - loss: 0.0127\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0095\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 848us/step - loss: 0.0105\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 798us/step - loss: 0.0092\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 798us/step - loss: 0.0150\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 798us/step - loss: 0.0147\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 723us/step - loss: 0.0099\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 698us/step - loss: 0.0056\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0153\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 698us/step - loss: 0.0099\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0064\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 798us/step - loss: 0.0082\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 798us/step - loss: 0.0080\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 698us/step - loss: 0.0131\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 698us/step - loss: 0.0078\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0112\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 798us/step - loss: 0.0105\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 698us/step - loss: 0.0135\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0167\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0112\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 698us/step - loss: 0.0066\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0090\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0137\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 698us/step - loss: 0.0058\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0087\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 798us/step - loss: 0.0075\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 698us/step - loss: 0.0081\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0101\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0116\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 698us/step - loss: 0.0093\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0110\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 698us/step - loss: 0.0098\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 698us/step - loss: 0.0061\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 748us/step - loss: 0.0063\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 698us/step - loss: 0.0163\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE461160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 2s 906us/step - loss: 0.1331\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0819\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0642\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0524\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0410\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0362\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 926us/step - loss: 0.0249\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0342\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0293\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0196\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0166\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0149\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0140\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0239\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0161\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0081\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0107\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0099\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0160\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0103\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0134\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0131\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0062\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0144\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0112\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0092\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0051\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0054\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0083\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0093\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 997us/step - loss: 0.0162\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0077\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0104\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0122\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0063\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0064\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0187\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0073\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0102\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0067\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0050\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0063\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0080\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 997us/step - loss: 0.0060\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0105\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0047\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0081\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0050\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0203\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B456AE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 2s 1ms/step - loss: 0.1230\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0551\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0586\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0516\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0563\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0430\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0409\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0267\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0218\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0175\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0221\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0263\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0123\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0153\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0126\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0193\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0099\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0270\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0143\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0158\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0123\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0150\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0103\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0151\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0072\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0087\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0100\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0162\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0141\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 997us/step - loss: 0.0079\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0085\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0113\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0111\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 997us/step - loss: 0.0121\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0095\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0081\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0134\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.020 - 0s 855us/step - loss: 0.0105\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0062\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0101\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0056\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 902us/step - loss: 0.0105\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 997us/step - loss: 0.0087\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0103\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 997us/step - loss: 0.0089\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 950us/step - loss: 0.0081\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0066\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0049\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0068\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0117\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE67DA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 762us/step - loss: 0.1507\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 712us/step - loss: 0.1232\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0613\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0808\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 712us/step - loss: 0.0622\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0354\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 729us/step - loss: 0.0263\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0343\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 712us/step - loss: 0.0284\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 712us/step - loss: 0.0189\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0332\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0175\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0165\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0211\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0177\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0154\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0099\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 712us/step - loss: 0.0278\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0128\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 807us/step - loss: 0.0103\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0070\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0053\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0103\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0123\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0108\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 807us/step - loss: 0.0221\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 807us/step - loss: 0.0153\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0107\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0064\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0075\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0076\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 807us/step - loss: 0.0054\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0155\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 712us/step - loss: 0.0071\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0190\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 807us/step - loss: 0.0063\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0067\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0138\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 807us/step - loss: 0.0075\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 807us/step - loss: 0.0079\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 807us/step - loss: 0.0083\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 807us/step - loss: 0.0157\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 807us/step - loss: 0.0118\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 807us/step - loss: 0.0055\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0177\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0093\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0145\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0089\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 855us/step - loss: 0.0072\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 760us/step - loss: 0.0119\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230ADF588B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 2s 952us/step - loss: 0.1449\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0650\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0665\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0549\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.0480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0298\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0344\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0231\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0186\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0224\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.0220\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0167\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0127\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0238\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.0083\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0123\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0105\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0166\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0090\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0154\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0120\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0120\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0140\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0075\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0110\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.0114\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0121\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0151\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0174\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0108\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0154\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0141\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0103\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0113\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0170\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.0144\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0149\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0089\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0074\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0146\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0132\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0086\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0071\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0097\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0145\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0123\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0082\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0092\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0150\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0092\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE1C6CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 2s 952us/step - loss: 0.1443\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0867\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0567\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0391\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0601\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 853us/step - loss: 0.0400\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0314\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0384\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0389\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.0299\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0160\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0161\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.0169\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0097\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0149\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0093\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0070\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.0108\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.0120\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0093\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0113\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0125\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0101\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0074\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0130\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0088\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0086\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0135\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0162\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0124\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0069\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0115\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0090\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0143\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0115\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.0096\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0062\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0151\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0074\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.0109\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0126\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0080\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 880us/step - loss: 0.0099\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0169\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0080\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0090\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0096\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0061\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 907us/step - loss: 0.0092\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE46B9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 801us/step - loss: 0.1292\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.1297\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0820\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0519\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0468\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0368\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 705us/step - loss: 0.0335\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0311\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0155\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0358\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0167\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0168\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 751us/step - loss: 0.0244\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0235\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0107\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0068\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0153\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 778us/step - loss: 0.0117\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0102\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0086\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0156\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0175\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0116\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0104\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0108\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0162\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0188\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0096\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0131\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0067\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0122\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0102\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0071\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0069\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0095\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 816us/step - loss: 0.0073\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0060\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 861us/step - loss: 0.0106\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 816us/step - loss: 0.0065\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0065\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0093\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0110\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0110\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0079\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0090\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0087\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 725us/step - loss: 0.0083\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 816us/step - loss: 0.0141\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0102\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 771us/step - loss: 0.0047\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B321D9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "24/24 [==============================] - 1s 911us/step - loss: 0.1608\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0712\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0737\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0525\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0411\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0431\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0442\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0259\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0180\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 0s 901us/step - loss: 0.0255\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0156\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 0s 824us/step - loss: 0.0165\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0246\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0130\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.0171\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 0s 997us/step - loss: 0.0088\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0122\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0088\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 0s 837us/step - loss: 0.0088\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0106\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.0078\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0087\n",
      "Epoch 23/50\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.0074\n",
      "Epoch 24/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0117\n",
      "Epoch 25/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0114\n",
      "Epoch 26/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0108\n",
      "Epoch 27/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0134\n",
      "Epoch 28/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0098\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 867us/step - loss: 0.0086\n",
      "Epoch 30/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0134\n",
      "Epoch 31/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0169\n",
      "Epoch 32/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0116\n",
      "Epoch 33/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0201\n",
      "Epoch 34/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0157\n",
      "Epoch 35/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0140\n",
      "Epoch 36/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0090\n",
      "Epoch 37/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0077\n",
      "Epoch 38/50\n",
      "24/24 [==============================] - 0s 824us/step - loss: 0.0060\n",
      "Epoch 39/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0083\n",
      "Epoch 40/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0089\n",
      "Epoch 41/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0095\n",
      "Epoch 42/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0116\n",
      "Epoch 43/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0050\n",
      "Epoch 44/50\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.0084\n",
      "Epoch 45/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0057\n",
      "Epoch 46/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0076\n",
      "Epoch 47/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0103\n",
      "Epoch 48/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0116\n",
      "Epoch 49/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0106\n",
      "Epoch 50/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0080\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B43C2C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "24/24 [==============================] - 1s 911us/step - loss: 0.0917\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0883\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0459\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0457\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0318\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0325\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0356\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 0s 997us/step - loss: 0.0363\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.0320\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 0s 885us/step - loss: 0.0142\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0245\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.0164\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.0185\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 0s 824us/step - loss: 0.0088\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0098\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0158\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0154\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 0s 870us/step - loss: 0.0120\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0163\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0176\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0052\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0106\n",
      "Epoch 23/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0117\n",
      "Epoch 24/50\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.0190\n",
      "Epoch 25/50\n",
      "24/24 [==============================] - 0s 824us/step - loss: 0.0093\n",
      "Epoch 26/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0104\n",
      "Epoch 27/50\n",
      "24/24 [==============================] - 0s 896us/step - loss: 0.0065\n",
      "Epoch 28/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0135\n",
      "Epoch 29/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0106\n",
      "Epoch 30/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0068\n",
      "Epoch 31/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0080\n",
      "Epoch 32/50\n",
      "24/24 [==============================] - 0s 983us/step - loss: 0.0122\n",
      "Epoch 33/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0118\n",
      "Epoch 34/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0078\n",
      "Epoch 35/50\n",
      "24/24 [==============================] - 0s 824us/step - loss: 0.0155\n",
      "Epoch 36/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0067\n",
      "Epoch 37/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0105\n",
      "Epoch 38/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0054\n",
      "Epoch 39/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0076\n",
      "Epoch 40/50\n",
      "24/24 [==============================] - 0s 997us/step - loss: 0.0048\n",
      "Epoch 41/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0106\n",
      "Epoch 42/50\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.0064\n",
      "Epoch 43/50\n",
      "24/24 [==============================] - 0s 997us/step - loss: 0.0086\n",
      "Epoch 44/50\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.0085\n",
      "Epoch 45/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0079\n",
      "Epoch 46/50\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.0075\n",
      "Epoch 47/50\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.0090\n",
      "Epoch 48/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0087\n",
      "Epoch 49/50\n",
      "24/24 [==============================] - 0s 911us/step - loss: 0.0083\n",
      "Epoch 50/50\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0128\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B6329F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "24/24 [==============================] - 1s 737us/step - loss: 0.1238\n",
      "Epoch 2/50\n",
      "24/24 [==============================] - 0s 867us/step - loss: 0.0967\n",
      "Epoch 3/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0664\n",
      "Epoch 4/50\n",
      "24/24 [==============================] - 0s 694us/step - loss: 0.0531\n",
      "Epoch 5/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0436\n",
      "Epoch 6/50\n",
      "24/24 [==============================] - 0s 759us/step - loss: 0.0540\n",
      "Epoch 7/50\n",
      "24/24 [==============================] - 0s 694us/step - loss: 0.0321\n",
      "Epoch 8/50\n",
      "24/24 [==============================] - 0s 781us/step - loss: 0.0312\n",
      "Epoch 9/50\n",
      "24/24 [==============================] - 0s 694us/step - loss: 0.0269\n",
      "Epoch 10/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0337\n",
      "Epoch 11/50\n",
      "24/24 [==============================] - 0s 803us/step - loss: 0.0205\n",
      "Epoch 12/50\n",
      "24/24 [==============================] - 0s 824us/step - loss: 0.0231\n",
      "Epoch 13/50\n",
      "24/24 [==============================] - 0s 781us/step - loss: 0.0176\n",
      "Epoch 14/50\n",
      "24/24 [==============================] - 0s 781us/step - loss: 0.0175\n",
      "Epoch 15/50\n",
      "24/24 [==============================] - 0s 781us/step - loss: 0.0119\n",
      "Epoch 16/50\n",
      "24/24 [==============================] - 0s 780us/step - loss: 0.0110\n",
      "Epoch 17/50\n",
      "24/24 [==============================] - 0s 781us/step - loss: 0.0070\n",
      "Epoch 18/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0074\n",
      "Epoch 19/50\n",
      "24/24 [==============================] - 0s 780us/step - loss: 0.0145\n",
      "Epoch 20/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0120\n",
      "Epoch 21/50\n",
      "24/24 [==============================] - 0s 780us/step - loss: 0.0102\n",
      "Epoch 22/50\n",
      "24/24 [==============================] - 0s 781us/step - loss: 0.0107\n",
      "Epoch 23/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0088\n",
      "Epoch 24/50\n",
      "24/24 [==============================] - 0s 780us/step - loss: 0.0095\n",
      "Epoch 25/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0147\n",
      "Epoch 26/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0103\n",
      "Epoch 27/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0069\n",
      "Epoch 28/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0177\n",
      "Epoch 29/50\n",
      "24/24 [==============================] - 0s 694us/step - loss: 0.0101\n",
      "Epoch 30/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0159\n",
      "Epoch 31/50\n",
      "24/24 [==============================] - 0s 694us/step - loss: 0.0111\n",
      "Epoch 32/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0114\n",
      "Epoch 33/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0109\n",
      "Epoch 34/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0057\n",
      "Epoch 35/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0114\n",
      "Epoch 36/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0099\n",
      "Epoch 37/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0093\n",
      "Epoch 38/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0066\n",
      "Epoch 39/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0094\n",
      "Epoch 40/50\n",
      "24/24 [==============================] - 0s 694us/step - loss: 0.0076\n",
      "Epoch 41/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0075\n",
      "Epoch 42/50\n",
      "24/24 [==============================] - 0s 694us/step - loss: 0.0064\n",
      "Epoch 43/50\n",
      "24/24 [==============================] - 0s 824us/step - loss: 0.0140\n",
      "Epoch 44/50\n",
      "24/24 [==============================] - 0s 781us/step - loss: 0.0128\n",
      "Epoch 45/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0084\n",
      "Epoch 46/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0075\n",
      "Epoch 47/50\n",
      "24/24 [==============================] - 0s 694us/step - loss: 0.0134\n",
      "Epoch 48/50\n",
      "24/24 [==============================] - 0s 694us/step - loss: 0.0097\n",
      "Epoch 49/50\n",
      "24/24 [==============================] - 0s 694us/step - loss: 0.0046\n",
      "Epoch 50/50\n",
      "24/24 [==============================] - 0s 737us/step - loss: 0.0059\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B46653A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 2s 937us/step - loss: 0.1426\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0798\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0621\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0305\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0284\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0264\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0258\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0344\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0167\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0163\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0203\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0095\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0171\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 879us/step - loss: 0.0238\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 882us/step - loss: 0.0090\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0130\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0106\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0084\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0129\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0142\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0126\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0117\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0065\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0126\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0096\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0104\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0101\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0094\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0073\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0144\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0154\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0140\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0197\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.0165\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.0088\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.0095\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0075\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0082\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0071\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0068\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0080\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0063\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0112\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0074\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.0101\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0093\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0169\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0122\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0064\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0078\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE8F98B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25/25 [==============================] - 2s 997us/step - loss: 0.1492\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0784\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0639\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0412\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 908us/step - loss: 0.0378\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0421\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0244\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0280\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0242\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0231\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0166\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0105\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0221\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0216\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0117\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 877us/step - loss: 0.0131\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0148\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0116\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.0216\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0061\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.0071\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0087\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0147\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0199\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0098\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 831us/step - loss: 0.0087\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0119\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0140\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0091\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 831us/step - loss: 0.0043\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0083\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 831us/step - loss: 0.0136\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0136\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 831us/step - loss: 0.0098\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 831us/step - loss: 0.0088\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 831us/step - loss: 0.0093\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0095\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 790us/step - loss: 0.0103\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 790us/step - loss: 0.0190\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 831us/step - loss: 0.0060\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0105\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0144\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.0071\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0065\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0083\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0090\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0069\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0092\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0111\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B456ADC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "25/25 [==============================] - 1s 790us/step - loss: 0.1907\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 789us/step - loss: 0.0650\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 790us/step - loss: 0.0416\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 748us/step - loss: 0.0342\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0401\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 824us/step - loss: 0.0343\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 706us/step - loss: 0.0333\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 790us/step - loss: 0.0333\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 790us/step - loss: 0.0165\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 831us/step - loss: 0.0190\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 790us/step - loss: 0.0172\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 831us/step - loss: 0.0133\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 790us/step - loss: 0.0129\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 831us/step - loss: 0.0147\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 777us/step - loss: 0.0258\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 748us/step - loss: 0.0124\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 748us/step - loss: 0.0089\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 748us/step - loss: 0.0078\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 790us/step - loss: 0.0240\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 695us/step - loss: 0.0083\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 665us/step - loss: 0.0107\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 665us/step - loss: 0.0116\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 706us/step - loss: 0.0107\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 706us/step - loss: 0.0130\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 706us/step - loss: 0.0080\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 748us/step - loss: 0.0146\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 790us/step - loss: 0.0195\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 748us/step - loss: 0.0134\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 790us/step - loss: 0.0152\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 790us/step - loss: 0.0144\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 790us/step - loss: 0.0080\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0052\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0081\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 873us/step - loss: 0.0053\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.0091\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.0090\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 831us/step - loss: 0.0177\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 748us/step - loss: 0.0166\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 748us/step - loss: 0.0111\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 790us/step - loss: 0.0069\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 748us/step - loss: 0.0129\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 831us/step - loss: 0.0160\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 706us/step - loss: 0.0091\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 748us/step - loss: 0.0084\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 665us/step - loss: 0.0075\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 665us/step - loss: 0.0056\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 748us/step - loss: 0.0097\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 748us/step - loss: 0.0074\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 706us/step - loss: 0.0092\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE46B820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "26/26 [==============================] - 2s 878us/step - loss: 0.1606\n",
      "Epoch 2/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0824\n",
      "Epoch 3/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0458\n",
      "Epoch 4/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0406\n",
      "Epoch 5/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0383\n",
      "Epoch 6/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0280\n",
      "Epoch 7/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0275\n",
      "Epoch 8/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0234\n",
      "Epoch 9/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0184\n",
      "Epoch 10/50\n",
      "26/26 [==============================] - 0s 828us/step - loss: 0.0247\n",
      "Epoch 11/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0142\n",
      "Epoch 12/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0290\n",
      "Epoch 13/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0124\n",
      "Epoch 14/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0172\n",
      "Epoch 15/50\n",
      "26/26 [==============================] - 0s 818us/step - loss: 0.0110\n",
      "Epoch 16/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0069\n",
      "Epoch 17/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0088\n",
      "Epoch 18/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0204\n",
      "Epoch 19/50\n",
      "26/26 [==============================] - 0s 818us/step - loss: 0.0088\n",
      "Epoch 20/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0141\n",
      "Epoch 21/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0128\n",
      "Epoch 22/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0120\n",
      "Epoch 23/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0119\n",
      "Epoch 24/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0113\n",
      "Epoch 25/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0067\n",
      "Epoch 26/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0105\n",
      "Epoch 27/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0132\n",
      "Epoch 28/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0128\n",
      "Epoch 29/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0121\n",
      "Epoch 30/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0087\n",
      "Epoch 31/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0109\n",
      "Epoch 32/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0111\n",
      "Epoch 33/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0060\n",
      "Epoch 34/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0054\n",
      "Epoch 35/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0103\n",
      "Epoch 36/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0133\n",
      "Epoch 37/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0079\n",
      "Epoch 38/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0082\n",
      "Epoch 39/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0061\n",
      "Epoch 40/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0092\n",
      "Epoch 41/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0131\n",
      "Epoch 42/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0065\n",
      "Epoch 43/50\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0098\n",
      "Epoch 44/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0089\n",
      "Epoch 45/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0076\n",
      "Epoch 46/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0075\n",
      "Epoch 47/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0081\n",
      "Epoch 48/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0075\n",
      "Epoch 49/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0101\n",
      "Epoch 50/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0145\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B2DC3E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "26/26 [==============================] - 1s 918us/step - loss: 0.2098\n",
      "Epoch 2/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.1093\n",
      "Epoch 3/50\n",
      "26/26 [==============================] - 0s 820us/step - loss: 0.0803\n",
      "Epoch 4/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0533\n",
      "Epoch 5/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0497\n",
      "Epoch 6/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0314\n",
      "Epoch 7/50\n",
      "26/26 [==============================] - 0s 821us/step - loss: 0.0411\n",
      "Epoch 8/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0295\n",
      "Epoch 9/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0305\n",
      "Epoch 10/50\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0260\n",
      "Epoch 11/50\n",
      "26/26 [==============================] - 0s 829us/step - loss: 0.0151\n",
      "Epoch 12/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0197\n",
      "Epoch 13/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0162\n",
      "Epoch 14/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0136\n",
      "Epoch 15/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0204\n",
      "Epoch 16/50\n",
      "26/26 [==============================] - 0s 844us/step - loss: 0.0116\n",
      "Epoch 17/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0103\n",
      "Epoch 18/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0100\n",
      "Epoch 19/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0086\n",
      "Epoch 20/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0154\n",
      "Epoch 21/50\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0123\n",
      "Epoch 22/50\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0081\n",
      "Epoch 23/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0131\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 957us/step - loss: 0.0111\n",
      "Epoch 25/50\n",
      "26/26 [==============================] - 0s 941us/step - loss: 0.0155\n",
      "Epoch 26/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0089\n",
      "Epoch 27/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0086\n",
      "Epoch 28/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0105\n",
      "Epoch 29/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0088\n",
      "Epoch 30/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0096\n",
      "Epoch 31/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0102\n",
      "Epoch 32/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0121\n",
      "Epoch 33/50\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0083\n",
      "Epoch 34/50\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0083\n",
      "Epoch 35/50\n",
      "26/26 [==============================] - 0s 997us/step - loss: 0.0065\n",
      "Epoch 36/50\n",
      "26/26 [==============================] - 0s 941us/step - loss: 0.0063\n",
      "Epoch 37/50\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0119\n",
      "Epoch 38/50\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0196\n",
      "Epoch 39/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0067\n",
      "Epoch 40/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0111\n",
      "Epoch 41/50\n",
      "26/26 [==============================] - 0s 798us/step - loss: 0.0078\n",
      "Epoch 42/50\n",
      "26/26 [==============================] - 0s 838us/step - loss: 0.0108\n",
      "Epoch 43/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0071\n",
      "Epoch 44/50\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0131\n",
      "Epoch 45/50\n",
      "26/26 [==============================] - 0s 951us/step - loss: 0.0086\n",
      "Epoch 46/50\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0132\n",
      "Epoch 47/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0077\n",
      "Epoch 48/50\n",
      "26/26 [==============================] - 0s 957us/step - loss: 0.0083\n",
      "Epoch 49/50\n",
      "26/26 [==============================] - 0s 878us/step - loss: 0.0132\n",
      "Epoch 50/50\n",
      "26/26 [==============================] - 0s 918us/step - loss: 0.0064\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE8F9820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "26/26 [==============================] - 1s 718us/step - loss: 0.1192\n",
      "Epoch 2/50\n",
      "26/26 [==============================] - 0s 669us/step - loss: 0.0755\n",
      "Epoch 3/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0547\n",
      "Epoch 4/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0483\n",
      "Epoch 5/50\n",
      "26/26 [==============================] - 0s 718us/step - loss: 0.0304\n",
      "Epoch 6/50\n",
      "26/26 [==============================] - 0s 718us/step - loss: 0.0434\n",
      "Epoch 7/50\n",
      "26/26 [==============================] - 0s 739us/step - loss: 0.0295\n",
      "Epoch 8/50\n",
      "26/26 [==============================] - 0s 718us/step - loss: 0.0290\n",
      "Epoch 9/50\n",
      "26/26 [==============================] - 0s 718us/step - loss: 0.0183\n",
      "Epoch 10/50\n",
      "26/26 [==============================] - 0s 718us/step - loss: 0.0188\n",
      "Epoch 11/50\n",
      "26/26 [==============================] - 0s 718us/step - loss: 0.0238\n",
      "Epoch 12/50\n",
      "26/26 [==============================] - 0s 718us/step - loss: 0.0281\n",
      "Epoch 13/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0184\n",
      "Epoch 14/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0120\n",
      "Epoch 15/50\n",
      "26/26 [==============================] - 0s 638us/step - loss: 0.0181\n",
      "Epoch 16/50\n",
      "26/26 [==============================] - 0s 718us/step - loss: 0.0198\n",
      "Epoch 17/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0111\n",
      "Epoch 18/50\n",
      "26/26 [==============================] - 0s 638us/step - loss: 0.0162\n",
      "Epoch 19/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0095\n",
      "Epoch 20/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0149\n",
      "Epoch 21/50\n",
      "26/26 [==============================] - 0s 638us/step - loss: 0.0177\n",
      "Epoch 22/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0086\n",
      "Epoch 23/50\n",
      "26/26 [==============================] - 0s 701us/step - loss: 0.0173\n",
      "Epoch 24/50\n",
      "26/26 [==============================] - 0s 638us/step - loss: 0.0146\n",
      "Epoch 25/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0102\n",
      "Epoch 26/50\n",
      "26/26 [==============================] - 0s 638us/step - loss: 0.0092\n",
      "Epoch 27/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0074\n",
      "Epoch 28/50\n",
      "26/26 [==============================] - 0s 671us/step - loss: 0.0073\n",
      "Epoch 29/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0094\n",
      "Epoch 30/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0088\n",
      "Epoch 31/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0081\n",
      "Epoch 32/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0071\n",
      "Epoch 33/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0087\n",
      "Epoch 34/50\n",
      "26/26 [==============================] - 0s 698us/step - loss: 0.0105\n",
      "Epoch 35/50\n",
      "26/26 [==============================] - 0s 638us/step - loss: 0.0101\n",
      "Epoch 36/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0157\n",
      "Epoch 37/50\n",
      "26/26 [==============================] - 0s 638us/step - loss: 0.0084\n",
      "Epoch 38/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0084\n",
      "Epoch 39/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0081\n",
      "Epoch 40/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0115\n",
      "Epoch 41/50\n",
      "26/26 [==============================] - 0s 638us/step - loss: 0.0077\n",
      "Epoch 42/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0088\n",
      "Epoch 43/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0125\n",
      "Epoch 44/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0103\n",
      "Epoch 45/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0090\n",
      "Epoch 46/50\n",
      "26/26 [==============================] - 0s 678us/step - loss: 0.0066\n",
      "Epoch 47/50\n",
      "26/26 [==============================] - 0s 638us/step - loss: 0.0082\n",
      "Epoch 48/50\n",
      "26/26 [==============================] - 0s 718us/step - loss: 0.0081\n",
      "Epoch 49/50\n",
      "26/26 [==============================] - 0s 718us/step - loss: 0.0115\n",
      "Epoch 50/50\n",
      "26/26 [==============================] - 0s 718us/step - loss: 0.0101\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B60F8550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "27/27 [==============================] - 2s 844us/step - loss: 0.1624\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 842us/step - loss: 0.0780\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0441\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0594\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0347\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 826us/step - loss: 0.0381\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0349\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0279\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0272\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0301\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0255\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0263\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0146\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0123\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0096\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0279\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0155\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0111\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0225\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0103\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0156\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0091\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0083\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0094\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0112\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0193\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0180\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0098\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0155\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0144\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0112\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0103\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0075\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0150\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0087\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0077\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0075\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0072\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0122\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 0s 959us/step - loss: 0.0089\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0084\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0119\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 0s 959us/step - loss: 0.0083\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0077\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0094\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0118\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0082\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0087\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0058\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0076\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AEB0E310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "27/27 [==============================] - 2s 921us/step - loss: 0.1532\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 1ms/step - loss: 0.0721\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0356\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0364\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0267\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0332\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0285\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0157\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - ETA: 0s - loss: 0.032 - 0s 921us/step - loss: 0.0225\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 0s 881us/step - loss: 0.0200\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 0s 959us/step - loss: 0.0238\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0112\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0152\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0087\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0129\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0115\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0131\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0095\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 0s 959us/step - loss: 0.0088\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0119\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0159\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0073\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0115\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0062\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0089\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0149\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0085\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0088\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0107\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0187\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0060\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0100\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0112\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0133\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0116\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0138\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0110\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0094\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0104\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0119\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0098\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0093\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0103\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0082\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0067\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "27/27 [==============================] - 0s 882us/step - loss: 0.0050\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 0s 921us/step - loss: 0.0090\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 0s 844us/step - loss: 0.0080\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 0s 997us/step - loss: 0.0065\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B1D494C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "27/27 [==============================] - 1s 690us/step - loss: 0.1131\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0664\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 0s 690us/step - loss: 0.0480\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0484\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0547\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0367\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0417\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 0s 614us/step - loss: 0.0243\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0280\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 0s 690us/step - loss: 0.0245\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0189\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0212\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0196\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0274\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0082\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 0s 690us/step - loss: 0.0097\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0187\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0217\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0147\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 0s 690us/step - loss: 0.0181\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0139\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0114\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 0s 614us/step - loss: 0.0074\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0084\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0102\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 0s 690us/step - loss: 0.0076\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0111\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0071\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0103\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0120\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 0s 690us/step - loss: 0.0072\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0148\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0105\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 0s 690us/step - loss: 0.0119\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0090\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 0s 690us/step - loss: 0.0104\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0093\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0187\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0104\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0116\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0131\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0102\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 0s 690us/step - loss: 0.0076\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0076\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 0s 690us/step - loss: 0.0127\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 0s 806us/step - loss: 0.0054\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 0s 690us/step - loss: 0.0085\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 0s 690us/step - loss: 0.0086\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0045\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 0s 652us/step - loss: 0.0063\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE461AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 2s 1ms/step - loss: 0.1118\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0746\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 997us/step - loss: 0.0405\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0333\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 997us/step - loss: 0.0348\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 997us/step - loss: 0.0280\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 997us/step - loss: 0.0293\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0209\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0158\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 0s 886us/step - loss: 0.0118\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 0s 886us/step - loss: 0.0153\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0094\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0146\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 0s 886us/step - loss: 0.0137\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0114\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 0s 923us/step - loss: 0.0128\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0132\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0142\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0073\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0112\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0092\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0119\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 0s 849us/step - loss: 0.0121\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0069\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0075\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 0s 923us/step - loss: 0.0147\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0123\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0100\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0097\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0132\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 0s 829us/step - loss: 0.0138\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0108\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0079\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 0s 819us/step - loss: 0.0091\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0157\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 0s 997us/step - loss: 0.0051\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0105\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0064\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0072\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0079\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0112\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0053\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 0s 898us/step - loss: 0.0080\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0088\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0116\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0065\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0073\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0094\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0075\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B8A4DE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 2s 883us/step - loss: 0.1066\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0729\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0483\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0423\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 960us/step - loss: 0.0301\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0358\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0275\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0292\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0197\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0241\n",
      "Epoch 11/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0135\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 0s 886us/step - loss: 0.0155\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0143\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 0s 960us/step - loss: 0.0121\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0180\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0121\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0075\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0176\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0160\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0109\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0107\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0209\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 0s 997us/step - loss: 0.0156\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0089\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0062\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 0s 923us/step - loss: 0.0135\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0101\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0072\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0077\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0125\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0051\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0108\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 0s 997us/step - loss: 0.0087\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 0s 960us/step - loss: 0.0081\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0126\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0068\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0102\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0184\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0065\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0093\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0061\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 0s 924us/step - loss: 0.0091\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0049\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0158\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0128\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0078\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 0s 923us/step - loss: 0.0184\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0119\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0056\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 0s 846us/step - loss: 0.0123\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B321D670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "28/28 [==============================] - 2s 850us/step - loss: 0.1478\n",
      "Epoch 2/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0651\n",
      "Epoch 3/50\n",
      "28/28 [==============================] - 0s 702us/step - loss: 0.0417\n",
      "Epoch 4/50\n",
      "28/28 [==============================] - 0s 739us/step - loss: 0.0414\n",
      "Epoch 5/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0318\n",
      "Epoch 6/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0271\n",
      "Epoch 7/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0198\n",
      "Epoch 8/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0141\n",
      "Epoch 9/50\n",
      "28/28 [==============================] - 0s 739us/step - loss: 0.0119\n",
      "Epoch 10/50\n",
      "28/28 [==============================] - 0s 735us/step - loss: 0.0154\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 776us/step - loss: 0.0156\n",
      "Epoch 12/50\n",
      "28/28 [==============================] - 0s 739us/step - loss: 0.0133\n",
      "Epoch 13/50\n",
      "28/28 [==============================] - 0s 923us/step - loss: 0.0162\n",
      "Epoch 14/50\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.0183\n",
      "Epoch 15/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0172\n",
      "Epoch 16/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0103\n",
      "Epoch 17/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0157\n",
      "Epoch 18/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0214\n",
      "Epoch 19/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0176\n",
      "Epoch 20/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0089\n",
      "Epoch 21/50\n",
      "28/28 [==============================] - 0s 923us/step - loss: 0.0074\n",
      "Epoch 22/50\n",
      "28/28 [==============================] - 0s 942us/step - loss: 0.0087\n",
      "Epoch 23/50\n",
      "28/28 [==============================] - 0s 960us/step - loss: 0.0120\n",
      "Epoch 24/50\n",
      "28/28 [==============================] - 0s 923us/step - loss: 0.0162\n",
      "Epoch 25/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0132\n",
      "Epoch 26/50\n",
      "28/28 [==============================] - 0s 702us/step - loss: 0.0069\n",
      "Epoch 27/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0087\n",
      "Epoch 28/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0177\n",
      "Epoch 29/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0096\n",
      "Epoch 30/50\n",
      "28/28 [==============================] - 0s 739us/step - loss: 0.0075\n",
      "Epoch 31/50\n",
      "28/28 [==============================] - 0s 739us/step - loss: 0.0084\n",
      "Epoch 32/50\n",
      "28/28 [==============================] - 0s 739us/step - loss: 0.0049\n",
      "Epoch 33/50\n",
      "28/28 [==============================] - 0s 739us/step - loss: 0.0075\n",
      "Epoch 34/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0086\n",
      "Epoch 35/50\n",
      "28/28 [==============================] - 0s 739us/step - loss: 0.0071\n",
      "Epoch 36/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0062\n",
      "Epoch 37/50\n",
      "28/28 [==============================] - 0s 739us/step - loss: 0.0099\n",
      "Epoch 38/50\n",
      "28/28 [==============================] - 0s 739us/step - loss: 0.0067\n",
      "Epoch 39/50\n",
      "28/28 [==============================] - 0s 850us/step - loss: 0.0089\n",
      "Epoch 40/50\n",
      "28/28 [==============================] - 0s 702us/step - loss: 0.0085\n",
      "Epoch 41/50\n",
      "28/28 [==============================] - 0s 739us/step - loss: 0.0134\n",
      "Epoch 42/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0077\n",
      "Epoch 43/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0149\n",
      "Epoch 44/50\n",
      "28/28 [==============================] - 0s 739us/step - loss: 0.0064\n",
      "Epoch 45/50\n",
      "28/28 [==============================] - 0s 739us/step - loss: 0.0068\n",
      "Epoch 46/50\n",
      "28/28 [==============================] - 0s 776us/step - loss: 0.0116\n",
      "Epoch 47/50\n",
      "28/28 [==============================] - 0s 813us/step - loss: 0.0063\n",
      "Epoch 48/50\n",
      "28/28 [==============================] - 0s 887us/step - loss: 0.0064\n",
      "Epoch 49/50\n",
      "28/28 [==============================] - 0s 960us/step - loss: 0.0170\n",
      "Epoch 50/50\n",
      "28/28 [==============================] - 0s 886us/step - loss: 0.0057\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B456A790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 2s 916us/step - loss: 0.1104\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 962us/step - loss: 0.0784\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 0s 997us/step - loss: 0.0464\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 871us/step - loss: 0.0277\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 890us/step - loss: 0.0315\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0226\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0208\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 784us/step - loss: 0.0287\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 784us/step - loss: 0.0126\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 784us/step - loss: 0.0168\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 784us/step - loss: 0.0184\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 825us/step - loss: 0.0100\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0134\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0118\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0109\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 890us/step - loss: 0.0197\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0193\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0112\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0131\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0103\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 0s 784us/step - loss: 0.0092\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0081\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 890us/step - loss: 0.0118\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0067\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0104\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 784us/step - loss: 0.0079\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 851us/step - loss: 0.0090\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0101\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0146\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 891us/step - loss: 0.0146\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 890us/step - loss: 0.0059\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 890us/step - loss: 0.0109\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0070\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0152\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0116\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0220\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0113\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0189\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0099\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0125\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 997us/step - loss: 0.0093\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 962us/step - loss: 0.0062\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 997us/step - loss: 0.0093\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 962us/step - loss: 0.0158\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0085\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 997us/step - loss: 0.0101\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0074\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0063\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0065\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0055\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B60F8F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "29/29 [==============================] - 2s 890us/step - loss: 0.1408\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 837us/step - loss: 0.0987\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 0s 962us/step - loss: 0.0401\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 962us/step - loss: 0.0239\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0283\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0257\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0209\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0296\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 962us/step - loss: 0.0173\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 997us/step - loss: 0.0262\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 997us/step - loss: 0.0155\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0186\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0098\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 890us/step - loss: 0.0121\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0117\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 890us/step - loss: 0.0126\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0201\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0125\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0165\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 890us/step - loss: 0.0104\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0097\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 890us/step - loss: 0.0070\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0091\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 784us/step - loss: 0.0074\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0096\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0072\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0075\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0099\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 1ms/step - loss: 0.0171\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0092\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 962us/step - loss: 0.0161\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0144\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 873us/step - loss: 0.0058\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0090\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0106\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0068\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 837us/step - loss: 0.0062\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0082\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0055\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0175\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0062\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0065\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 784us/step - loss: 0.0160\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0077\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 784us/step - loss: 0.0100\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0065\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 784us/step - loss: 0.0066\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 855us/step - loss: 0.0104\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 891us/step - loss: 0.0046\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 926us/step - loss: 0.0048\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE9CD9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "29/29 [==============================] - 1s 754us/step - loss: 0.1609\n",
      "Epoch 2/50\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0683\n",
      "Epoch 3/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0546\n",
      "Epoch 4/50\n",
      "29/29 [==============================] - 0s 784us/step - loss: 0.0465\n",
      "Epoch 5/50\n",
      "29/29 [==============================] - 0s 819us/step - loss: 0.0317\n",
      "Epoch 6/50\n",
      "29/29 [==============================] - 0s 731us/step - loss: 0.0338\n",
      "Epoch 7/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0283\n",
      "Epoch 8/50\n",
      "29/29 [==============================] - 0s 748us/step - loss: 0.0292\n",
      "Epoch 9/50\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0208\n",
      "Epoch 10/50\n",
      "29/29 [==============================] - 0s 784us/step - loss: 0.0149\n",
      "Epoch 11/50\n",
      "29/29 [==============================] - 0s 784us/step - loss: 0.0273\n",
      "Epoch 12/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0262\n",
      "Epoch 13/50\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0181\n",
      "Epoch 14/50\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0112\n",
      "Epoch 15/50\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0097\n",
      "Epoch 16/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0156\n",
      "Epoch 17/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0141\n",
      "Epoch 18/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0113\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0112\n",
      "Epoch 20/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0105\n",
      "Epoch 21/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0106\n",
      "Epoch 22/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0124\n",
      "Epoch 23/50\n",
      "29/29 [==============================] - 0s 663us/step - loss: 0.0072\n",
      "Epoch 24/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0076\n",
      "Epoch 25/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0095\n",
      "Epoch 26/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0067\n",
      "Epoch 27/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0121\n",
      "Epoch 28/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0073\n",
      "Epoch 29/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0115\n",
      "Epoch 30/50\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0071\n",
      "Epoch 31/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0127\n",
      "Epoch 32/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0102\n",
      "Epoch 33/50\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0080\n",
      "Epoch 34/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0090\n",
      "Epoch 35/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0103\n",
      "Epoch 36/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0072\n",
      "Epoch 37/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0062\n",
      "Epoch 38/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0113\n",
      "Epoch 39/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0063\n",
      "Epoch 40/50\n",
      "29/29 [==============================] - 0s 712us/step - loss: 0.0072\n",
      "Epoch 41/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0091\n",
      "Epoch 42/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0095\n",
      "Epoch 43/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0073\n",
      "Epoch 44/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0061\n",
      "Epoch 45/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0085\n",
      "Epoch 46/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0140\n",
      "Epoch 47/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0077\n",
      "Epoch 48/50\n",
      "29/29 [==============================] - 0s 677us/step - loss: 0.0052\n",
      "Epoch 49/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0101\n",
      "Epoch 50/50\n",
      "29/29 [==============================] - 0s 641us/step - loss: 0.0088\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B632DF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 2s 860us/step - loss: 0.1493\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0907\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0492\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0418\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0455\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0263\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0261\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0189\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0222\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 0s 963us/step - loss: 0.0159\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0173\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 0s 963us/step - loss: 0.0152\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0169\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0138\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 0s 963us/step - loss: 0.0140\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0093\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 0s 997us/step - loss: 0.0170\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 0s 997us/step - loss: 0.0117\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 0s 997us/step - loss: 0.0089\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0122\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0064\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0112\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0177\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0131\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0133\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0179\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 0s 929us/step - loss: 0.0065\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0198\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 0s 963us/step - loss: 0.0067\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0096\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0208\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0126\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0097\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0120\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0071\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0100\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0089\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0080\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0054\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0117\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0064\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0100\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0151\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 0s 886us/step - loss: 0.0056\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 0s 882us/step - loss: 0.0082\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0067\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 0s 963us/step - loss: 0.0065\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 0s 929us/step - loss: 0.0082\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 0s 997us/step - loss: 0.0134\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0063\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B5FCBAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 2s 997us/step - loss: 0.1323\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 929us/step - loss: 0.0594\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 929us/step - loss: 0.0645\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 929us/step - loss: 0.0489\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0385\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0293\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 929us/step - loss: 0.0308\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 0s 861us/step - loss: 0.0258\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0270\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0298\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0140\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0153\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0219\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 0s 929us/step - loss: 0.0127\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0193\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0110\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0124\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0080\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0092\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0086\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0169\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0115\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0084\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 825us/step - loss: 0.0123\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0093\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0055\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0066\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0057\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0072\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0084\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0159\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0097\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0070\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0065\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0196\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0099\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0061\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0081\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0089\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0081\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0077\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0059\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0086\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0103\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0069\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0081\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0077\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0147\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 0s 845us/step - loss: 0.0052\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0055\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE8F94C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 2s 688us/step - loss: 0.1420\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 722us/step - loss: 0.0614\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 722us/step - loss: 0.0230\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 757us/step - loss: 0.0195\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 757us/step - loss: 0.0266\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 757us/step - loss: 0.0279\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0356\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 0s 722us/step - loss: 0.0287\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 0s 757us/step - loss: 0.0205\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0239\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0123\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 0s 757us/step - loss: 0.0196\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 0s 757us/step - loss: 0.0138\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 0s 722us/step - loss: 0.0103\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0189\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 0s 757us/step - loss: 0.0186\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 0s 653us/step - loss: 0.0116\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 0s 653us/step - loss: 0.0083\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 0s 688us/step - loss: 0.0136\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 0s 653us/step - loss: 0.0095\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 0s 757us/step - loss: 0.0086\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 0s 757us/step - loss: 0.0126\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0085\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 0s 928us/step - loss: 0.0059\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0074\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0085\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 0s 722us/step - loss: 0.0096\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 0s 894us/step - loss: 0.0066\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0110\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0075\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0121\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 0s 929us/step - loss: 0.0063\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0070\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0090\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 0s 730us/step - loss: 0.0057\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 0s 722us/step - loss: 0.0080\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 0s 688us/step - loss: 0.0119\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 0s 757us/step - loss: 0.0163\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 0s 825us/step - loss: 0.0171\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 0s 757us/step - loss: 0.0085\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 0.0102\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 0s 997us/step - loss: 0.0133\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 0s 860us/step - loss: 0.0077\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0160\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0042\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0062\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0065\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0065\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 0s 791us/step - loss: 0.0087\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 0s 929us/step - loss: 0.0076\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B4665CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 2s 895us/step - loss: 0.1323\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0694\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0477\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0269\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 0s 873us/step - loss: 0.0246\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.0349\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0260\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.0321\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0201\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 0s 798us/step - loss: 0.0173\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0178\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0156\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 0s 964us/step - loss: 0.0111\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.0236\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.0137\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.0129\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.0081\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.0107\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 0s 888us/step - loss: 0.0109\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0088\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0071\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 0s 964us/step - loss: 0.0150\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 0s 997us/step - loss: 0.0071\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0136\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0100\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0062\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0114\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0188\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0114\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 0s 931us/step - loss: 0.0142\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0055\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0091\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0066\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0074\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0101\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.0062\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 0s 798us/step - loss: 0.0090\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0087\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 0s 916us/step - loss: 0.0062\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0070\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0063\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0078\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0111\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0118\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0063\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.0094\n",
      "Epoch 47/50\n",
      "31/31 [==============================] - 0s 953us/step - loss: 0.0087\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.0065\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.0074\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0073\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B4B05C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 1s 903us/step - loss: 0.1054\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0353\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0266\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0229\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 0s 885us/step - loss: 0.0260\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0257\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0147\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0137\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0146\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0148\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0126\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0171\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 0s 931us/step - loss: 0.0123\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0092\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 0s 891us/step - loss: 0.0075\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 0s 882us/step - loss: 0.0093\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0112\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0095\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0133\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0160\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0144\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 0s 1ms/step - loss: 0.0070\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0123\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.0068\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 0s 881us/step - loss: 0.0112\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0095\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 0s 931us/step - loss: 0.0088\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0075\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 0s 881us/step - loss: 0.0065\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 0s 964us/step - loss: 0.0135\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 0s 931us/step - loss: 0.0075\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0054\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0101\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 0s 997us/step - loss: 0.0081\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 0s 931us/step - loss: 0.0186\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0056\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 0s 831us/step - loss: 0.0192\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0091\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 0s 931us/step - loss: 0.0081\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 0s 964us/step - loss: 0.0058\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0074\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0130\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0051\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 0s 931us/step - loss: 0.0056\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 0s 931us/step - loss: 0.0098\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 0s 931us/step - loss: 0.0060\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 997us/step - loss: 0.0070\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0065\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0112\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0077\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B47C7F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 1s 731us/step - loss: 0.1405\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0614\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 0s 798us/step - loss: 0.0527\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 0s 864us/step - loss: 0.0343\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 0s 731us/step - loss: 0.0452\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 0s 665us/step - loss: 0.0332\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0272\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0215\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 0s 716us/step - loss: 0.0205\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0209\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0244\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 0s 665us/step - loss: 0.0248\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 0s 731us/step - loss: 0.0187\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 0s 689us/step - loss: 0.0113\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0110\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 0s 765us/step - loss: 0.0094\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 0s 731us/step - loss: 0.0120\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 0s 665us/step - loss: 0.0085\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0206\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 0s 731us/step - loss: 0.0090\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0091\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0112\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 0s 765us/step - loss: 0.0131\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0087\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0054\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 0s 765us/step - loss: 0.0061\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 0s 731us/step - loss: 0.0076\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0077\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0119\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0100\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 0s 798us/step - loss: 0.0097\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0092\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 0s 731us/step - loss: 0.0063\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0156\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0069\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0064\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0121\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0115\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 0s 665us/step - loss: 0.0095\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 0s 731us/step - loss: 0.0137\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 0s 731us/step - loss: 0.0088\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 0s 665us/step - loss: 0.0102\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 0s 731us/step - loss: 0.0097\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0072\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 0s 665us/step - loss: 0.0068\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 0s 731us/step - loss: 0.0042\n",
      "Epoch 47/50\n",
      "31/31 [==============================] - 0s 898us/step - loss: 0.0065\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 0s 731us/step - loss: 0.0088\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 0s 665us/step - loss: 0.0082\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 0s 698us/step - loss: 0.0100\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B46650D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 2s 901us/step - loss: 0.1644\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0919\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0378\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.0380\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0403\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0389\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0226\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.0152\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0138\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0154\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0145\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.0121\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0121\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0091\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0145\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0103\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0170\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0125\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0065\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.0097\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 965us/step - loss: 0.0087\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 965us/step - loss: 0.0112\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0116\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0201\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0059\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0099\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0177\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0095\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 965us/step - loss: 0.0088\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0098\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0130\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 997us/step - loss: 0.0065\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 997us/step - loss: 0.0055\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0124\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 949us/step - loss: 0.0072\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 965us/step - loss: 0.0071\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 965us/step - loss: 0.0079\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 997us/step - loss: 0.0080\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0085\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0060\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0057\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 997us/step - loss: 0.0047\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 997us/step - loss: 0.0097\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0106\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0044\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0053\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 997us/step - loss: 0.0062\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 965us/step - loss: 0.0088\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0050\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 886us/step - loss: 0.0069\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE5AA9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 2s 933us/step - loss: 0.1728\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0846\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 997us/step - loss: 0.0576\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0323\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0335\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0276\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0170\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0333\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0147\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0174\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0134\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0108\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.0205\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0100\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0094\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0239\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0138\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0175\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0094\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0071\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.0072\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0093\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0089\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0088\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0068\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0126\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0079\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0070\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 997us/step - loss: 0.0063\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.0065\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0090\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0062\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0074\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 965us/step - loss: 0.0051\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0049\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0141\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.0063\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.0129\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.0184\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0076\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.0179\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0105\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0061\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.0056\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0102\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0119\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.0056\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.0067\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.0108\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.0063\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE6589D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 1s 771us/step - loss: 0.1424\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0578\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.0394\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0315\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 676us/step - loss: 0.0285\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 676us/step - loss: 0.0196\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0188\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.0242\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.0164\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.0216\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 772us/step - loss: 0.0163\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0147\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.0127\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0110\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0139\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.0100\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.0084\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.0067\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.0077\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0123\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0060\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.0082\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.0117\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.0102\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0170\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0135\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.0097\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.0196\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.0077\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 676us/step - loss: 0.0071\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.0076\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0065\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0109\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0150\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.0067\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0086\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0060\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0143\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 676us/step - loss: 0.0048\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0078\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 676us/step - loss: 0.0065\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 676us/step - loss: 0.0077\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 676us/step - loss: 0.0174\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0077\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0067\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.0111\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 690us/step - loss: 0.0078\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 676us/step - loss: 0.0050\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.0105\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.0053\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B1BE9EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 2s 900us/step - loss: 0.0786\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0272\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0318\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 891us/step - loss: 0.0223\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0183\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0262\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 920us/step - loss: 0.0205\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0134\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0155\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0153\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 920us/step - loss: 0.0112\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0138\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0150\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0098\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.0112\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 935us/step - loss: 0.0178\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 966us/step - loss: 0.0077\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0103\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0129\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 935us/step - loss: 0.0066\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.0065\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0120\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.0078\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0089\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0110\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0127\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0082\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.0068\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 871us/step - loss: 0.0070\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0077\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 997us/step - loss: 0.0078\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0112\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0096\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0052\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0068\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 935us/step - loss: 0.0076\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0074\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0060\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 997us/step - loss: 0.0112\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0058\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0076\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0086\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0064\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0055\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 997us/step - loss: 0.0084\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 966us/step - loss: 0.0082\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 966us/step - loss: 0.0119\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 842us/step - loss: 0.0061\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0085\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 842us/step - loss: 0.0088\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B74ACE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "33/33 [==============================] - 2s 873us/step - loss: 0.0563\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0347\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0310\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.0324\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0167\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0163\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0173\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 832us/step - loss: 0.0168\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.0134\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.0149\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.0112\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0184\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0084\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.0120\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0076\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.0126\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 842us/step - loss: 0.0079\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0093\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 842us/step - loss: 0.0061\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.0079\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0104\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0094\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 842us/step - loss: 0.0096\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 842us/step - loss: 0.0135\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 842us/step - loss: 0.0074\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 842us/step - loss: 0.0082\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0087\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0052\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0141\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0102\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0068\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 935us/step - loss: 0.0056\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0064\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0094\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0161\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0067\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0052\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0068\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 935us/step - loss: 0.0062\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 953us/step - loss: 0.0057\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0072\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0043\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0078\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0060\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0056\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0067\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 859us/step - loss: 0.0051\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.0038\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.0091\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 892us/step - loss: 0.0040\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B4507430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "33/33 [==============================] - 2s 749us/step - loss: 0.1432\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0558\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0282\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.0277\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0386\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0262\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0197\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0222\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0132\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.0118\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0160\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0108\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0113\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0086\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0104\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0114\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0115\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.0074\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 686us/step - loss: 0.0101\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0111\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0208\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0068\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 686us/step - loss: 0.0074\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0128\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.0132\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0077\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0134\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.0077\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.0054\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0138\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0067\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0086\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.0081\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0061\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0058\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 810us/step - loss: 0.0088\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0063\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.0104\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0062\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 810us/step - loss: 0.0158\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0107\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 717us/step - loss: 0.0066\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0079\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 721us/step - loss: 0.0059\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0071\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0085\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.0047\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.0069\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 810us/step - loss: 0.0049\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.0050\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B4507DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "34/34 [==============================] - 2s 880us/step - loss: 0.1413\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0850\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0405\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0365\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0370\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0298\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0263\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0256\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0212\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0223\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0154\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0246\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0260\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0151\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0133\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0153\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0272\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0099\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0128\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0081\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0142\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0114\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0072\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0056\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0156\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0100\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0114\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0080\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0062\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0095\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0073\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0067\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0113\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0058\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0086\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0150\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0073\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0098\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0042\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0081\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0107\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0064\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0123\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0057\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.0056\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0077\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0068\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0081\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0050\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0108\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B321D940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "34/34 [==============================] - 2s 846us/step - loss: 0.1118\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0464\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0419\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0322\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0230\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0264\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0229\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0167\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0161\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0149\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0146\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0131\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0122\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0157\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0145\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0074\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0104\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0088\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0177\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0091\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0095\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.0163\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0137\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0157\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0115\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0088\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0121\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0069\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.0115\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0077\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0110\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0097\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0077\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0058\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0162\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0091\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0079\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0080\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0057\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0136\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0071\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0070\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0097\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.0137\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0065\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.0109\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.0160\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.0073\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0072\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0084\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE435CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "34/34 [==============================] - 1s 788us/step - loss: 0.1336\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.0575\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.0399\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.0338\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.0307\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0356\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.0262\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 0s 710us/step - loss: 0.0271\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.0221\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0145\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0174\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0165\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.0193\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.0115\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.0190\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0147\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.0109\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0118\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.0232\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0137\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.0143\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0076\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0107\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 0s 665us/step - loss: 0.0094\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0113\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0097\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0132\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 0s 665us/step - loss: 0.0107\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.0104\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 0s 665us/step - loss: 0.0205\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0083\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 0s 665us/step - loss: 0.0081\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0072\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0081\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.0081\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0108\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0082\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.0081\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0086\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.0184\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.0092\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0093\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0080\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.0105\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.0094\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0068\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.0095\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0091\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 0s 695us/step - loss: 0.0066\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 0s 660us/step - loss: 0.0077\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE461C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "35/35 [==============================] - 1s 854us/step - loss: 0.1375\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0642\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0375\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 0s 831us/step - loss: 0.0417\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0348\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 821us/step - loss: 0.0262\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0228\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0262\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0218\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.0163\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 0s 868us/step - loss: 0.0153\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0156\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0190\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.0194\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0193\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0233\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.0099\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0216\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 0s 939us/step - loss: 0.0160\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0123\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.0164\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0179\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0177\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.0144\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0126\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0137\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.0110\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.0117\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 0s 939us/step - loss: 0.0222\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 0s 892us/step - loss: 0.0215\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.0152\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0149\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.0119\n",
      "Epoch 34/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0128\n",
      "Epoch 35/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0077\n",
      "Epoch 36/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0135\n",
      "Epoch 37/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0180\n",
      "Epoch 38/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0133\n",
      "Epoch 39/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0148\n",
      "Epoch 40/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0104\n",
      "Epoch 41/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0101\n",
      "Epoch 42/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0113\n",
      "Epoch 43/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0103\n",
      "Epoch 44/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0168\n",
      "Epoch 45/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0194\n",
      "Epoch 46/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0157\n",
      "Epoch 47/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0145\n",
      "Epoch 48/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0119\n",
      "Epoch 49/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0123\n",
      "Epoch 50/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0091\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B5E574C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "35/35 [==============================] - 2s 880us/step - loss: 0.0783\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 0s 909us/step - loss: 0.0343\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0463\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0314\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.0326\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0310\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0251\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0301\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0233\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0177\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0250\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0178\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0202\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0330\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 0s 909us/step - loss: 0.0142\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 0s 997us/step - loss: 0.0164\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 0s 939us/step - loss: 0.0149\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.0125\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0140\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0144\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 0s 909us/step - loss: 0.0147\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 0s 909us/step - loss: 0.0158\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 0s 909us/step - loss: 0.0181\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.0153\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0128\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0136\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.0130\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0128\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.0125\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0123\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0182\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0127\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.0098\n",
      "Epoch 34/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0191\n",
      "Epoch 35/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0114\n",
      "Epoch 36/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0120\n",
      "Epoch 37/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0108\n",
      "Epoch 38/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0153\n",
      "Epoch 39/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0152\n",
      "Epoch 40/50\n",
      "35/35 [==============================] - 0s 959us/step - loss: 0.0127\n",
      "Epoch 41/50\n",
      "35/35 [==============================] - 0s 909us/step - loss: 0.0188\n",
      "Epoch 42/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0197\n",
      "Epoch 43/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0115\n",
      "Epoch 44/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0208\n",
      "Epoch 45/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0101\n",
      "Epoch 46/50\n",
      "35/35 [==============================] - 0s 827us/step - loss: 0.0103\n",
      "Epoch 47/50\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.0120\n",
      "Epoch 48/50\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.0118\n",
      "Epoch 49/50\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.0148\n",
      "Epoch 50/50\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.0135\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B321D8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "35/35 [==============================] - 1s 733us/step - loss: 0.1097\n",
      "Epoch 2/50\n",
      "35/35 [==============================] - 0s 675us/step - loss: 0.0681\n",
      "Epoch 3/50\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.0497\n",
      "Epoch 4/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0506\n",
      "Epoch 5/50\n",
      "35/35 [==============================] - 0s 690us/step - loss: 0.0490\n",
      "Epoch 6/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0391\n",
      "Epoch 7/50\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.0282\n",
      "Epoch 8/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0290\n",
      "Epoch 9/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0269\n",
      "Epoch 10/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0214\n",
      "Epoch 11/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0312\n",
      "Epoch 12/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0219\n",
      "Epoch 13/50\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.0229\n",
      "Epoch 14/50\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.0164\n",
      "Epoch 15/50\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.0194\n",
      "Epoch 16/50\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.0197\n",
      "Epoch 17/50\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.0194\n",
      "Epoch 18/50\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.0167\n",
      "Epoch 19/50\n",
      "35/35 [==============================] - 0s 645us/step - loss: 0.0195\n",
      "Epoch 20/50\n",
      "35/35 [==============================] - 0s 675us/step - loss: 0.0199\n",
      "Epoch 21/50\n",
      "35/35 [==============================] - 0s 675us/step - loss: 0.0239\n",
      "Epoch 22/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0211\n",
      "Epoch 23/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0181\n",
      "Epoch 24/50\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.0176\n",
      "Epoch 25/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0181\n",
      "Epoch 26/50\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.0204\n",
      "Epoch 27/50\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.0133\n",
      "Epoch 28/50\n",
      "35/35 [==============================] - 0s 675us/step - loss: 0.0128\n",
      "Epoch 29/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0197\n",
      "Epoch 30/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0196\n",
      "Epoch 31/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0178\n",
      "Epoch 32/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0132\n",
      "Epoch 33/50\n",
      "35/35 [==============================] - 0s 675us/step - loss: 0.0178\n",
      "Epoch 34/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0124\n",
      "Epoch 35/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0186\n",
      "Epoch 36/50\n",
      "35/35 [==============================] - 0s 675us/step - loss: 0.0178\n",
      "Epoch 37/50\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.0143\n",
      "Epoch 38/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0173\n",
      "Epoch 39/50\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.0134\n",
      "Epoch 40/50\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.0122\n",
      "Epoch 41/50\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.0218\n",
      "Epoch 42/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0104\n",
      "Epoch 43/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0133\n",
      "Epoch 44/50\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.0237\n",
      "Epoch 45/50\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.0164\n",
      "Epoch 46/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0111\n",
      "Epoch 47/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0197\n",
      "Epoch 48/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0142\n",
      "Epoch 49/50\n",
      "35/35 [==============================] - 0s 704us/step - loss: 0.0164\n",
      "Epoch 50/50\n",
      "35/35 [==============================] - 0s 675us/step - loss: 0.0178\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B4415E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 2s 1ms/step - loss: 0.1490\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0733\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0430\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.0466\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0520\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0360\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0315\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0298\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0207\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0376\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0330\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0291\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0246\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.0201\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0181\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0211\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.0219\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0191\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0189\n",
      "Epoch 20/50\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.0180\n",
      "Epoch 21/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0130\n",
      "Epoch 22/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0174\n",
      "Epoch 23/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0215\n",
      "Epoch 24/50\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.0168\n",
      "Epoch 25/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0188\n",
      "Epoch 26/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0156\n",
      "Epoch 27/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0249\n",
      "Epoch 28/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0174\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 855us/step - loss: 0.0129\n",
      "Epoch 30/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0177\n",
      "Epoch 31/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0194\n",
      "Epoch 32/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0266\n",
      "Epoch 33/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0133\n",
      "Epoch 34/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0235\n",
      "Epoch 35/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0163\n",
      "Epoch 36/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0146\n",
      "Epoch 37/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0168\n",
      "Epoch 38/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0176\n",
      "Epoch 39/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0173\n",
      "Epoch 40/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0167\n",
      "Epoch 41/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0199\n",
      "Epoch 42/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0168\n",
      "Epoch 43/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0179\n",
      "Epoch 44/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0141\n",
      "Epoch 45/50\n",
      "36/36 [==============================] - 0s 828us/step - loss: 0.0186\n",
      "Epoch 46/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0134\n",
      "Epoch 47/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0150\n",
      "Epoch 48/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0176\n",
      "Epoch 49/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0207\n",
      "Epoch 50/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0130\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B1D49160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 1s 855us/step - loss: 0.1347\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0652\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0517\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0428\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0357\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0398\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 883us/step - loss: 0.0370\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0288\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0334\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0223\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0256\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0268\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0273\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0223\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0258\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0188\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0277\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0219\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0174\n",
      "Epoch 20/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0175\n",
      "Epoch 21/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0161\n",
      "Epoch 22/50\n",
      "36/36 [==============================] - 0s 883us/step - loss: 0.0207\n",
      "Epoch 23/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0216\n",
      "Epoch 24/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0192\n",
      "Epoch 25/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0200\n",
      "Epoch 26/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0182\n",
      "Epoch 27/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0209\n",
      "Epoch 28/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0187\n",
      "Epoch 29/50\n",
      "36/36 [==============================] - 0s 869us/step - loss: 0.0167\n",
      "Epoch 30/50\n",
      "36/36 [==============================] - 0s 912us/step - loss: 0.0156\n",
      "Epoch 31/50\n",
      "36/36 [==============================] - 0s 883us/step - loss: 0.0138\n",
      "Epoch 32/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0163\n",
      "Epoch 33/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0224\n",
      "Epoch 34/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0182\n",
      "Epoch 35/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0194\n",
      "Epoch 36/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0187\n",
      "Epoch 37/50\n",
      "36/36 [==============================] - 0s 883us/step - loss: 0.0153\n",
      "Epoch 38/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0170\n",
      "Epoch 39/50\n",
      "36/36 [==============================] - 0s 912us/step - loss: 0.0142\n",
      "Epoch 40/50\n",
      "36/36 [==============================] - 0s 940us/step - loss: 0.0189\n",
      "Epoch 41/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0209\n",
      "Epoch 42/50\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.0173\n",
      "Epoch 43/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0169\n",
      "Epoch 44/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0180\n",
      "Epoch 45/50\n",
      "36/36 [==============================] - 0s 830us/step - loss: 0.0132\n",
      "Epoch 46/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0122\n",
      "Epoch 47/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0184\n",
      "Epoch 48/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0135\n",
      "Epoch 49/50\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.0147\n",
      "Epoch 50/50\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.0140\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE8D0AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 1s 684us/step - loss: 0.1127\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0358\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0338\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0299\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0262\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0204\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0261\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0262\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0260\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0274\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0235\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0308\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0284\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 741us/step - loss: 0.0205\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 763us/step - loss: 0.0205\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0216\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0239\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0169\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0228\n",
      "Epoch 20/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0202\n",
      "Epoch 21/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0154\n",
      "Epoch 22/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0163\n",
      "Epoch 23/50\n",
      "36/36 [==============================] - 0s 769us/step - loss: 0.0227\n",
      "Epoch 24/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0178\n",
      "Epoch 25/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0197\n",
      "Epoch 26/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0215\n",
      "Epoch 27/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0159\n",
      "Epoch 28/50\n",
      "36/36 [==============================] - 0s 741us/step - loss: 0.0154\n",
      "Epoch 29/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0173\n",
      "Epoch 30/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0183\n",
      "Epoch 31/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0135\n",
      "Epoch 32/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0153\n",
      "Epoch 33/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0212\n",
      "Epoch 34/50\n",
      "36/36 [==============================] - 0s 741us/step - loss: 0.0229\n",
      "Epoch 35/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0167\n",
      "Epoch 36/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0129\n",
      "Epoch 37/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0156\n",
      "Epoch 38/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0165\n",
      "Epoch 39/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0188\n",
      "Epoch 40/50\n",
      "36/36 [==============================] - 0s 717us/step - loss: 0.0113\n",
      "Epoch 41/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0160\n",
      "Epoch 42/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0148\n",
      "Epoch 43/50\n",
      "36/36 [==============================] - 0s 704us/step - loss: 0.0139\n",
      "Epoch 44/50\n",
      "36/36 [==============================] - 0s 684us/step - loss: 0.0139\n",
      "Epoch 45/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0180\n",
      "Epoch 46/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0163\n",
      "Epoch 47/50\n",
      "36/36 [==============================] - 0s 712us/step - loss: 0.0139\n",
      "Epoch 48/50\n",
      "36/36 [==============================] - 0s 769us/step - loss: 0.0136\n",
      "Epoch 49/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0150\n",
      "Epoch 50/50\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.0141\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B8CD99D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "37/37 [==============================] - 2s 831us/step - loss: 0.1101\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0475\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0516\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0429\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.0365\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0333\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0297\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0304\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0289\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0296\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0257\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0211\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0275\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0231\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0264\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0200\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0198\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0219\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0144\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0197\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0245\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 887us/step - loss: 0.0179\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0185\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0253\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0189\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0163\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 887us/step - loss: 0.0203\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0192\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0239\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0179\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0179\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0211\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0200\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0146\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0161\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0156\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0186\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0185\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.0154\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0147\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0133\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0159\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0140\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0130\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0125\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0183\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0201\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0112\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0110\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0115\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE1C6C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "37/37 [==============================] - 2s 859us/step - loss: 0.1037\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0648\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0444\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0430\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0350\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0259\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.0398\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0353\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0222\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0246\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0301\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0217\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0265\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0222\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0297\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0222\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0244\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0303\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0259\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0235\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 970us/step - loss: 0.0206\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0200\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0181\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0225\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0196\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0233\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0203\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 914us/step - loss: 0.0256\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 914us/step - loss: 0.0216\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0233\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0237\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0163\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0189\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0192\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0185\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0204\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0158\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 824us/step - loss: 0.0192\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0224\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0216\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.0148\n",
      "Epoch 42/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0186\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0151\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0192\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.0182\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0180\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0184\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.0165\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0129\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0146\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B321DEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "37/37 [==============================] - 1s 720us/step - loss: 0.1563\n",
      "Epoch 2/50\n",
      "37/37 [==============================] - 0s 693us/step - loss: 0.0798\n",
      "Epoch 3/50\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.0351\n",
      "Epoch 4/50\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.0683\n",
      "Epoch 5/50\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.0465\n",
      "Epoch 6/50\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.0317\n",
      "Epoch 7/50\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.0571\n",
      "Epoch 8/50\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.0396\n",
      "Epoch 9/50\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.0344\n",
      "Epoch 10/50\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.0286\n",
      "Epoch 11/50\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.0254\n",
      "Epoch 12/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0272\n",
      "Epoch 13/50\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.0257\n",
      "Epoch 14/50\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.0238\n",
      "Epoch 15/50\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.0252\n",
      "Epoch 16/50\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.0209\n",
      "Epoch 17/50\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.0271\n",
      "Epoch 18/50\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.0283\n",
      "Epoch 19/50\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.0208\n",
      "Epoch 20/50\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.0192\n",
      "Epoch 21/50\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.0174\n",
      "Epoch 22/50\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.0236\n",
      "Epoch 23/50\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.0186\n",
      "Epoch 24/50\n",
      "37/37 [==============================] - 0s 693us/step - loss: 0.0261\n",
      "Epoch 25/50\n",
      "37/37 [==============================] - 0s 693us/step - loss: 0.0196\n",
      "Epoch 26/50\n",
      "37/37 [==============================] - 0s 693us/step - loss: 0.0312\n",
      "Epoch 27/50\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.0222\n",
      "Epoch 28/50\n",
      "37/37 [==============================] - 0s 665us/step - loss: 0.0199\n",
      "Epoch 29/50\n",
      "37/37 [==============================] - 0s 693us/step - loss: 0.0231\n",
      "Epoch 30/50\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.0212\n",
      "Epoch 31/50\n",
      "37/37 [==============================] - 0s 693us/step - loss: 0.0199\n",
      "Epoch 32/50\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.0216\n",
      "Epoch 33/50\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.0276\n",
      "Epoch 34/50\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.0186\n",
      "Epoch 35/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0183\n",
      "Epoch 36/50\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.0197\n",
      "Epoch 37/50\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.0236\n",
      "Epoch 38/50\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.0217\n",
      "Epoch 39/50\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.0134\n",
      "Epoch 40/50\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.0264\n",
      "Epoch 41/50\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.0160\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 748us/step - loss: 0.0173\n",
      "Epoch 43/50\n",
      "37/37 [==============================] - 0s 692us/step - loss: 0.0215\n",
      "Epoch 44/50\n",
      "37/37 [==============================] - 0s 693us/step - loss: 0.0177\n",
      "Epoch 45/50\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.0203\n",
      "Epoch 46/50\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.0171\n",
      "Epoch 47/50\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.0178\n",
      "Epoch 48/50\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.0171\n",
      "Epoch 49/50\n",
      "37/37 [==============================] - 0s 720us/step - loss: 0.0168\n",
      "Epoch 50/50\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.0235\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B2E703A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "38/38 [==============================] - 2s 866us/step - loss: 0.0936\n",
      "Epoch 2/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0469\n",
      "Epoch 3/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0296\n",
      "Epoch 4/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0457\n",
      "Epoch 5/50\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0333\n",
      "Epoch 6/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0377\n",
      "Epoch 7/50\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0274\n",
      "Epoch 8/50\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0356\n",
      "Epoch 9/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0204\n",
      "Epoch 10/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0282\n",
      "Epoch 11/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0299\n",
      "Epoch 12/50\n",
      "38/38 [==============================] - 0s 970us/step - loss: 0.0373\n",
      "Epoch 13/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0226\n",
      "Epoch 14/50\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.0246\n",
      "Epoch 15/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0212\n",
      "Epoch 16/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0180\n",
      "Epoch 17/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0229\n",
      "Epoch 18/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0241\n",
      "Epoch 19/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0273\n",
      "Epoch 20/50\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.0226\n",
      "Epoch 21/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0312\n",
      "Epoch 22/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0258\n",
      "Epoch 23/50\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0191\n",
      "Epoch 24/50\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0220\n",
      "Epoch 25/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0222\n",
      "Epoch 26/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0176\n",
      "Epoch 27/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0239\n",
      "Epoch 28/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0149\n",
      "Epoch 29/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0182\n",
      "Epoch 30/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0218\n",
      "Epoch 31/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0201\n",
      "Epoch 32/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0206\n",
      "Epoch 33/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0206\n",
      "Epoch 34/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0143\n",
      "Epoch 35/50\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.0188\n",
      "Epoch 36/50\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0222\n",
      "Epoch 37/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0196\n",
      "Epoch 38/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0200\n",
      "Epoch 39/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0175\n",
      "Epoch 40/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0172\n",
      "Epoch 41/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0151\n",
      "Epoch 42/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0146\n",
      "Epoch 43/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0146\n",
      "Epoch 44/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0133\n",
      "Epoch 45/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0130\n",
      "Epoch 46/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0117\n",
      "Epoch 47/50\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.0169\n",
      "Epoch 48/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0139\n",
      "Epoch 49/50\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.0125\n",
      "Epoch 50/50\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.0122\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B3381DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "38/38 [==============================] - 1s 890us/step - loss: 0.1070\n",
      "Epoch 2/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0687\n",
      "Epoch 3/50\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.0507\n",
      "Epoch 4/50\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.0389\n",
      "Epoch 5/50\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.0349\n",
      "Epoch 6/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0501\n",
      "Epoch 7/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0363\n",
      "Epoch 8/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0333\n",
      "Epoch 9/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0235\n",
      "Epoch 10/50\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.0305\n",
      "Epoch 11/50\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0260\n",
      "Epoch 12/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0258\n",
      "Epoch 13/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0308\n",
      "Epoch 14/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0359\n",
      "Epoch 15/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0242\n",
      "Epoch 16/50\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.0322\n",
      "Epoch 17/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0228\n",
      "Epoch 18/50\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.0216\n",
      "Epoch 19/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0195\n",
      "Epoch 20/50\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.0217\n",
      "Epoch 21/50\n",
      "38/38 [==============================] - 0s 970us/step - loss: 0.0249\n",
      "Epoch 22/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0157\n",
      "Epoch 23/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0196\n",
      "Epoch 24/50\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.0195\n",
      "Epoch 25/50\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.0262\n",
      "Epoch 26/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0248\n",
      "Epoch 27/50\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.0214\n",
      "Epoch 28/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0222\n",
      "Epoch 29/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0248\n",
      "Epoch 30/50\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.0191\n",
      "Epoch 31/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0202\n",
      "Epoch 32/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0181\n",
      "Epoch 33/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0193\n",
      "Epoch 34/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0155\n",
      "Epoch 35/50\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.0220\n",
      "Epoch 36/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0149\n",
      "Epoch 37/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0161\n",
      "Epoch 38/50\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0150\n",
      "Epoch 39/50\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.0134\n",
      "Epoch 40/50\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0149\n",
      "Epoch 41/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0154\n",
      "Epoch 42/50\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0137\n",
      "Epoch 43/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0176\n",
      "Epoch 44/50\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0131\n",
      "Epoch 45/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0150\n",
      "Epoch 46/50\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0145\n",
      "Epoch 47/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0159\n",
      "Epoch 48/50\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0122\n",
      "Epoch 49/50\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0120\n",
      "Epoch 50/50\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.0140\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B74EDF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "38/38 [==============================] - 2s 728us/step - loss: 0.1033\n",
      "Epoch 2/50\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.0508\n",
      "Epoch 3/50\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.0413\n",
      "Epoch 4/50\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0368\n",
      "Epoch 5/50\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.0292\n",
      "Epoch 6/50\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.0329\n",
      "Epoch 7/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0379\n",
      "Epoch 8/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0370\n",
      "Epoch 9/50\n",
      "38/38 [==============================] - 0s 647us/step - loss: 0.0237\n",
      "Epoch 10/50\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.0229\n",
      "Epoch 11/50\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.0306\n",
      "Epoch 12/50\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.0264\n",
      "Epoch 13/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0211\n",
      "Epoch 14/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0225\n",
      "Epoch 15/50\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.0228\n",
      "Epoch 16/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0230\n",
      "Epoch 17/50\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.0216\n",
      "Epoch 18/50\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.0207\n",
      "Epoch 19/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0221\n",
      "Epoch 20/50\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.0224\n",
      "Epoch 21/50\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0183\n",
      "Epoch 22/50\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.0242\n",
      "Epoch 23/50\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.0248\n",
      "Epoch 24/50\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.0221\n",
      "Epoch 25/50\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.0230\n",
      "Epoch 26/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0200\n",
      "Epoch 27/50\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.0235\n",
      "Epoch 28/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0199\n",
      "Epoch 29/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0210\n",
      "Epoch 30/50\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.0217\n",
      "Epoch 31/50\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.0222\n",
      "Epoch 32/50\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.0207\n",
      "Epoch 33/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0152\n",
      "Epoch 34/50\n",
      "38/38 [==============================] - 0s 647us/step - loss: 0.0150\n",
      "Epoch 35/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0190\n",
      "Epoch 36/50\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.0158\n",
      "Epoch 37/50\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.0169\n",
      "Epoch 38/50\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.0150\n",
      "Epoch 39/50\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.0137\n",
      "Epoch 40/50\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.0148\n",
      "Epoch 41/50\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.0129\n",
      "Epoch 42/50\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.0171\n",
      "Epoch 43/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0117\n",
      "Epoch 44/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0161\n",
      "Epoch 45/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0171\n",
      "Epoch 46/50\n",
      "38/38 [==============================] - 0s 674us/step - loss: 0.0110\n",
      "Epoch 47/50\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.0165\n",
      "Epoch 48/50\n",
      "38/38 [==============================] - 0s 688us/step - loss: 0.0119\n",
      "Epoch 49/50\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.0153\n",
      "Epoch 50/50\n",
      "38/38 [==============================] - 0s 701us/step - loss: 0.0107\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B4AE35E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "39/39 [==============================] - 2s 919us/step - loss: 0.1428\n",
      "Epoch 2/50\n",
      "39/39 [==============================] - 0s 919us/step - loss: 0.0517\n",
      "Epoch 3/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0529\n",
      "Epoch 4/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0320\n",
      "Epoch 5/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0311\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 892us/step - loss: 0.0330\n",
      "Epoch 7/50\n",
      "39/39 [==============================] - 0s 919us/step - loss: 0.0388\n",
      "Epoch 8/50\n",
      "39/39 [==============================] - 0s 892us/step - loss: 0.0415\n",
      "Epoch 9/50\n",
      "39/39 [==============================] - 0s 945us/step - loss: 0.0365\n",
      "Epoch 10/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0332\n",
      "Epoch 11/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0262\n",
      "Epoch 12/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0311\n",
      "Epoch 13/50\n",
      "39/39 [==============================] - 0s 862us/step - loss: 0.0356\n",
      "Epoch 14/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0348\n",
      "Epoch 15/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0237\n",
      "Epoch 16/50\n",
      "39/39 [==============================] - 0s 892us/step - loss: 0.0251\n",
      "Epoch 17/50\n",
      "39/39 [==============================] - 0s 892us/step - loss: 0.0217\n",
      "Epoch 18/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0285\n",
      "Epoch 19/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0249\n",
      "Epoch 20/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0289\n",
      "Epoch 21/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0218\n",
      "Epoch 22/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0226\n",
      "Epoch 23/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0259\n",
      "Epoch 24/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0228\n",
      "Epoch 25/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0156\n",
      "Epoch 26/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0210\n",
      "Epoch 27/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0200\n",
      "Epoch 28/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0215\n",
      "Epoch 29/50\n",
      "39/39 [==============================] - 0s 919us/step - loss: 0.0250\n",
      "Epoch 30/50\n",
      "39/39 [==============================] - 0s 892us/step - loss: 0.0186\n",
      "Epoch 31/50\n",
      "39/39 [==============================] - 0s 997us/step - loss: 0.0189\n",
      "Epoch 32/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0158\n",
      "Epoch 33/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0202\n",
      "Epoch 34/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0219\n",
      "Epoch 35/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0177\n",
      "Epoch 36/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0163\n",
      "Epoch 37/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0137\n",
      "Epoch 38/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0137\n",
      "Epoch 39/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0131\n",
      "Epoch 40/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0144\n",
      "Epoch 41/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0176\n",
      "Epoch 42/50\n",
      "39/39 [==============================] - 0s 787us/step - loss: 0.0132\n",
      "Epoch 43/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0150\n",
      "Epoch 44/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0144\n",
      "Epoch 45/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0145\n",
      "Epoch 46/50\n",
      "39/39 [==============================] - 0s 844us/step - loss: 0.0170\n",
      "Epoch 47/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0200\n",
      "Epoch 48/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0093\n",
      "Epoch 49/50\n",
      "39/39 [==============================] - 0s 812us/step - loss: 0.0109\n",
      "Epoch 50/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0111\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE1C6790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "39/39 [==============================] - 2s 883us/step - loss: 0.1694\n",
      "Epoch 2/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0500\n",
      "Epoch 3/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0429\n",
      "Epoch 4/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0418\n",
      "Epoch 5/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0379\n",
      "Epoch 6/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0367\n",
      "Epoch 7/50\n",
      "39/39 [==============================] - 0s 787us/step - loss: 0.0350\n",
      "Epoch 8/50\n",
      "39/39 [==============================] - 0s 854us/step - loss: 0.0303\n",
      "Epoch 9/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0294\n",
      "Epoch 10/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0344\n",
      "Epoch 11/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0229\n",
      "Epoch 12/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0233\n",
      "Epoch 13/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0220\n",
      "Epoch 14/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0297\n",
      "Epoch 15/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0251\n",
      "Epoch 16/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0203\n",
      "Epoch 17/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0263\n",
      "Epoch 18/50\n",
      "39/39 [==============================] - 0s 892us/step - loss: 0.0232\n",
      "Epoch 19/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0242\n",
      "Epoch 20/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0199\n",
      "Epoch 21/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0263\n",
      "Epoch 22/50\n",
      "39/39 [==============================] - 0s 997us/step - loss: 0.0191\n",
      "Epoch 23/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0269\n",
      "Epoch 24/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0316\n",
      "Epoch 25/50\n",
      "39/39 [==============================] - 0s 997us/step - loss: 0.0203\n",
      "Epoch 26/50\n",
      "39/39 [==============================] - 0s 945us/step - loss: 0.0239\n",
      "Epoch 27/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0193\n",
      "Epoch 28/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0216\n",
      "Epoch 29/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0175\n",
      "Epoch 30/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0184\n",
      "Epoch 31/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0216\n",
      "Epoch 32/50\n",
      "39/39 [==============================] - 0s 814us/step - loss: 0.0231\n",
      "Epoch 33/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0199\n",
      "Epoch 34/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0170\n",
      "Epoch 35/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0199\n",
      "Epoch 36/50\n",
      "39/39 [==============================] - 0s 840us/step - loss: 0.0202\n",
      "Epoch 37/50\n",
      "39/39 [==============================] - 0s 866us/step - loss: 0.0205\n",
      "Epoch 38/50\n",
      "39/39 [==============================] - 0s 892us/step - loss: 0.0183\n",
      "Epoch 39/50\n",
      "39/39 [==============================] - 0s 945us/step - loss: 0.0177\n",
      "Epoch 40/50\n",
      "39/39 [==============================] - 0s 902us/step - loss: 0.0164\n",
      "Epoch 41/50\n",
      "39/39 [==============================] - 0s 892us/step - loss: 0.0153\n",
      "Epoch 42/50\n",
      "39/39 [==============================] - 0s 892us/step - loss: 0.0127\n",
      "Epoch 43/50\n",
      "39/39 [==============================] - 0s 892us/step - loss: 0.0183\n",
      "Epoch 44/50\n",
      "39/39 [==============================] - 0s 919us/step - loss: 0.0162\n",
      "Epoch 45/50\n",
      "39/39 [==============================] - 0s 945us/step - loss: 0.0179\n",
      "Epoch 46/50\n",
      "39/39 [==============================] - 0s 971us/step - loss: 0.0163\n",
      "Epoch 47/50\n",
      "39/39 [==============================] - 0s 892us/step - loss: 0.0180\n",
      "Epoch 48/50\n",
      "39/39 [==============================] - 0s 945us/step - loss: 0.0153\n",
      "Epoch 49/50\n",
      "39/39 [==============================] - 0s 945us/step - loss: 0.0126\n",
      "Epoch 50/50\n",
      "39/39 [==============================] - 0s 892us/step - loss: 0.0128\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B5E58B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "39/39 [==============================] - 1s 739us/step - loss: 0.0943\n",
      "Epoch 2/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0507\n",
      "Epoch 3/50\n",
      "39/39 [==============================] - 0s 709us/step - loss: 0.0399\n",
      "Epoch 4/50\n",
      "39/39 [==============================] - 0s 735us/step - loss: 0.0381\n",
      "Epoch 5/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0253\n",
      "Epoch 6/50\n",
      "39/39 [==============================] - 0s 735us/step - loss: 0.0362\n",
      "Epoch 7/50\n",
      "39/39 [==============================] - 0s 709us/step - loss: 0.0332\n",
      "Epoch 8/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0330\n",
      "Epoch 9/50\n",
      "39/39 [==============================] - 0s 709us/step - loss: 0.0389\n",
      "Epoch 10/50\n",
      "39/39 [==============================] - 0s 709us/step - loss: 0.0347\n",
      "Epoch 11/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0284\n",
      "Epoch 12/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0263\n",
      "Epoch 13/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0256\n",
      "Epoch 14/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0304\n",
      "Epoch 15/50\n",
      "39/39 [==============================] - 0s 892us/step - loss: 0.0237\n",
      "Epoch 16/50\n",
      "39/39 [==============================] - 0s 709us/step - loss: 0.0207\n",
      "Epoch 17/50\n",
      "39/39 [==============================] - 0s 787us/step - loss: 0.0251\n",
      "Epoch 18/50\n",
      "39/39 [==============================] - 0s 709us/step - loss: 0.0241\n",
      "Epoch 19/50\n",
      "39/39 [==============================] - 0s 709us/step - loss: 0.0248\n",
      "Epoch 20/50\n",
      "39/39 [==============================] - 0s 709us/step - loss: 0.0244\n",
      "Epoch 21/50\n",
      "39/39 [==============================] - 0s 656us/step - loss: 0.0202\n",
      "Epoch 22/50\n",
      "39/39 [==============================] - 0s 656us/step - loss: 0.0277\n",
      "Epoch 23/50\n",
      "39/39 [==============================] - 0s 656us/step - loss: 0.0217\n",
      "Epoch 24/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0286\n",
      "Epoch 25/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0232\n",
      "Epoch 26/50\n",
      "39/39 [==============================] - 0s 656us/step - loss: 0.0201\n",
      "Epoch 27/50\n",
      "39/39 [==============================] - 0s 656us/step - loss: 0.0265\n",
      "Epoch 28/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0165\n",
      "Epoch 29/50\n",
      "39/39 [==============================] - 0s 656us/step - loss: 0.0188\n",
      "Epoch 30/50\n",
      "39/39 [==============================] - 0s 656us/step - loss: 0.0215\n",
      "Epoch 31/50\n",
      "39/39 [==============================] - 0s 656us/step - loss: 0.0224\n",
      "Epoch 32/50\n",
      "39/39 [==============================] - 0s 709us/step - loss: 0.0233\n",
      "Epoch 33/50\n",
      "39/39 [==============================] - 0s 735us/step - loss: 0.0192\n",
      "Epoch 34/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0219\n",
      "Epoch 35/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0228\n",
      "Epoch 36/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0217\n",
      "Epoch 37/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0239\n",
      "Epoch 38/50\n",
      "39/39 [==============================] - 0s 709us/step - loss: 0.0150\n",
      "Epoch 39/50\n",
      "39/39 [==============================] - 0s 735us/step - loss: 0.0145\n",
      "Epoch 40/50\n",
      "39/39 [==============================] - 0s 735us/step - loss: 0.0179\n",
      "Epoch 41/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0191\n",
      "Epoch 42/50\n",
      "39/39 [==============================] - 0s 761us/step - loss: 0.0213\n",
      "Epoch 43/50\n",
      "39/39 [==============================] - 0s 735us/step - loss: 0.0124\n",
      "Epoch 44/50\n",
      "39/39 [==============================] - 0s 709us/step - loss: 0.0212\n",
      "Epoch 45/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0120\n",
      "Epoch 46/50\n",
      "39/39 [==============================] - 0s 709us/step - loss: 0.0153\n",
      "Epoch 47/50\n",
      "39/39 [==============================] - 0s 656us/step - loss: 0.0138\n",
      "Epoch 48/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0158\n",
      "Epoch 49/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0106\n",
      "Epoch 50/50\n",
      "39/39 [==============================] - 0s 682us/step - loss: 0.0114\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B61F8E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "40/40 [==============================] - 2s 960us/step - loss: 0.0953\n",
      "Epoch 2/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0415\n",
      "Epoch 3/50\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.0330\n",
      "Epoch 4/50\n",
      "40/40 [==============================] - 0s 921us/step - loss: 0.0410\n",
      "Epoch 5/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0432\n",
      "Epoch 6/50\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.0386\n",
      "Epoch 7/50\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.0251\n",
      "Epoch 8/50\n",
      "40/40 [==============================] - 0s 946us/step - loss: 0.0338\n",
      "Epoch 9/50\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.0310\n",
      "Epoch 10/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0205\n",
      "Epoch 11/50\n",
      "40/40 [==============================] - 0s 972us/step - loss: 0.0282\n",
      "Epoch 12/50\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0256\n",
      "Epoch 13/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0238\n",
      "Epoch 14/50\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.0219\n",
      "Epoch 15/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0243\n",
      "Epoch 16/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0235\n",
      "Epoch 17/50\n",
      "40/40 [==============================] - 0s 864us/step - loss: 0.0256\n",
      "Epoch 18/50\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.0205\n",
      "Epoch 19/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0222\n",
      "Epoch 20/50\n",
      "40/40 [==============================] - 0s 972us/step - loss: 0.0236\n",
      "Epoch 21/50\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.0274\n",
      "Epoch 22/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0242\n",
      "Epoch 23/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0168\n",
      "Epoch 24/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0225\n",
      "Epoch 25/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0227\n",
      "Epoch 26/50\n",
      "40/40 [==============================] - 0s 882us/step - loss: 0.0229\n",
      "Epoch 27/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0173\n",
      "Epoch 28/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0209\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 878us/step - loss: 0.0210\n",
      "Epoch 30/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0211\n",
      "Epoch 31/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0244\n",
      "Epoch 32/50\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.0221\n",
      "Epoch 33/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0163\n",
      "Epoch 34/50\n",
      "40/40 [==============================] - 0s 908us/step - loss: 0.0214\n",
      "Epoch 35/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0160\n",
      "Epoch 36/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0164\n",
      "Epoch 37/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0182\n",
      "Epoch 38/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0157\n",
      "Epoch 39/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0155\n",
      "Epoch 40/50\n",
      "40/40 [==============================] - 0s 921us/step - loss: 0.0211\n",
      "Epoch 41/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0165\n",
      "Epoch 42/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0200\n",
      "Epoch 43/50\n",
      "40/40 [==============================] - 0s 861us/step - loss: 0.0135\n",
      "Epoch 44/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0180\n",
      "Epoch 45/50\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.0113\n",
      "Epoch 46/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0129\n",
      "Epoch 47/50\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.0115\n",
      "Epoch 48/50\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.0123\n",
      "Epoch 49/50\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.0113\n",
      "Epoch 50/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0117\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B63BBEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "40/40 [==============================] - 2s 832us/step - loss: 0.1518\n",
      "Epoch 2/50\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.1003\n",
      "Epoch 3/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0590\n",
      "Epoch 4/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0394\n",
      "Epoch 5/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0388\n",
      "Epoch 6/50\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.0480\n",
      "Epoch 7/50\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.0287\n",
      "Epoch 8/50\n",
      "40/40 [==============================] - 0s 921us/step - loss: 0.0371\n",
      "Epoch 9/50\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0313\n",
      "Epoch 10/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0262\n",
      "Epoch 11/50\n",
      "40/40 [==============================] - 0s 946us/step - loss: 0.0278\n",
      "Epoch 12/50\n",
      "40/40 [==============================] - 0s 946us/step - loss: 0.0260\n",
      "Epoch 13/50\n",
      "40/40 [==============================] - 0s 921us/step - loss: 0.0273\n",
      "Epoch 14/50\n",
      "40/40 [==============================] - 0s 921us/step - loss: 0.0315\n",
      "Epoch 15/50\n",
      "40/40 [==============================] - 0s 896us/step - loss: 0.0257\n",
      "Epoch 16/50\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0218\n",
      "Epoch 17/50\n",
      "40/40 [==============================] - 0s 997us/step - loss: 0.0226\n",
      "Epoch 18/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0204\n",
      "Epoch 19/50\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.0227\n",
      "Epoch 20/50\n",
      "40/40 [==============================] - 0s 972us/step - loss: 0.0205\n",
      "Epoch 21/50\n",
      "40/40 [==============================] - 0s 946us/step - loss: 0.0284\n",
      "Epoch 22/50\n",
      "40/40 [==============================] - 0s 997us/step - loss: 0.0249\n",
      "Epoch 23/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0206\n",
      "Epoch 24/50\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.0189\n",
      "Epoch 25/50\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.0234\n",
      "Epoch 26/50\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.0194\n",
      "Epoch 27/50\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.0239\n",
      "Epoch 28/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0197\n",
      "Epoch 29/50\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.0168\n",
      "Epoch 30/50\n",
      "40/40 [==============================] - 0s 883us/step - loss: 0.0221\n",
      "Epoch 31/50\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0217\n",
      "Epoch 32/50\n",
      "40/40 [==============================] - 0s 997us/step - loss: 0.0225\n",
      "Epoch 33/50\n",
      "40/40 [==============================] - 0s 921us/step - loss: 0.0208\n",
      "Epoch 34/50\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "Epoch 35/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0189\n",
      "Epoch 36/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0257\n",
      "Epoch 37/50\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.0186\n",
      "Epoch 38/50\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.0183\n",
      "Epoch 39/50\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.0188\n",
      "Epoch 40/50\n",
      "40/40 [==============================] - 0s 921us/step - loss: 0.0186\n",
      "Epoch 41/50\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.0181\n",
      "Epoch 42/50\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0159\n",
      "Epoch 43/50\n",
      "40/40 [==============================] - 0s 948us/step - loss: 0.0141\n",
      "Epoch 44/50\n",
      "40/40 [==============================] - 0s 857us/step - loss: 0.0120\n",
      "Epoch 45/50\n",
      "40/40 [==============================] - 0s 972us/step - loss: 0.0142\n",
      "Epoch 46/50\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0122\n",
      "Epoch 47/50\n",
      "40/40 [==============================] - 0s 972us/step - loss: 0.0132\n",
      "Epoch 48/50\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 49/50\n",
      "40/40 [==============================] - 0s 914us/step - loss: 0.0087\n",
      "Epoch 50/50\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.0147\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230AE5AA700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "40/40 [==============================] - 2s 716us/step - loss: 0.1112\n",
      "Epoch 2/50\n",
      "40/40 [==============================] - 0s 694us/step - loss: 0.0595\n",
      "Epoch 3/50\n",
      "40/40 [==============================] - 0s 665us/step - loss: 0.0575\n",
      "Epoch 4/50\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.0382\n",
      "Epoch 5/50\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.0396\n",
      "Epoch 6/50\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.0300\n",
      "Epoch 7/50\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.0318\n",
      "Epoch 8/50\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.0323\n",
      "Epoch 9/50\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.0338\n",
      "Epoch 10/50\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.0293\n",
      "Epoch 11/50\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.0343\n",
      "Epoch 12/50\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.0250\n",
      "Epoch 13/50\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.0269\n",
      "Epoch 14/50\n",
      "40/40 [==============================] - 0s 665us/step - loss: 0.0264\n",
      "Epoch 15/50\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.0250\n",
      "Epoch 16/50\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.0279\n",
      "Epoch 17/50\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.0225\n",
      "Epoch 18/50\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.0257\n",
      "Epoch 19/50\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.0221\n",
      "Epoch 20/50\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.0236\n",
      "Epoch 21/50\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.0227\n",
      "Epoch 22/50\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.0217\n",
      "Epoch 23/50\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.0254\n",
      "Epoch 24/50\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.0206\n",
      "Epoch 25/50\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.0225\n",
      "Epoch 26/50\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.0206\n",
      "Epoch 27/50\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.0276\n",
      "Epoch 28/50\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.0232\n",
      "Epoch 29/50\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.0202\n",
      "Epoch 30/50\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.0205\n",
      "Epoch 31/50\n",
      "40/40 [==============================] - 0s 665us/step - loss: 0.0227\n",
      "Epoch 32/50\n",
      "40/40 [==============================] - 0s 665us/step - loss: 0.0236\n",
      "Epoch 33/50\n",
      "40/40 [==============================] - 0s 665us/step - loss: 0.0185\n",
      "Epoch 34/50\n",
      "40/40 [==============================] - 0s 665us/step - loss: 0.0221\n",
      "Epoch 35/50\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.0197\n",
      "Epoch 36/50\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.0215\n",
      "Epoch 37/50\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.0194\n",
      "Epoch 38/50\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.0168\n",
      "Epoch 39/50\n",
      "40/40 [==============================] - 0s 682us/step - loss: 0.0165\n",
      "Epoch 40/50\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.0165\n",
      "Epoch 41/50\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.0192\n",
      "Epoch 42/50\n",
      "40/40 [==============================] - 0s 639us/step - loss: 0.0173\n",
      "Epoch 43/50\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.0145\n",
      "Epoch 44/50\n",
      "40/40 [==============================] - 0s 639us/step - loss: 0.0182\n",
      "Epoch 45/50\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.0178\n",
      "Epoch 46/50\n",
      "40/40 [==============================] - 0s 716us/step - loss: 0.0123\n",
      "Epoch 47/50\n",
      "40/40 [==============================] - 0s 690us/step - loss: 0.0161\n",
      "Epoch 48/50\n",
      "40/40 [==============================] - 0s 665us/step - loss: 0.0159\n",
      "Epoch 49/50\n",
      "40/40 [==============================] - 0s 689us/step - loss: 0.0126\n",
      "Epoch 50/50\n",
      "40/40 [==============================] - 0s 665us/step - loss: 0.0115\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B4665940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "41/41 [==============================] - 2s 848us/step - loss: 0.0921\n",
      "Epoch 2/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0493\n",
      "Epoch 3/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0365\n",
      "Epoch 4/50\n",
      "41/41 [==============================] - 0s 873us/step - loss: 0.0407\n",
      "Epoch 5/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0378\n",
      "Epoch 6/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0397\n",
      "Epoch 7/50\n",
      "41/41 [==============================] - 0s 947us/step - loss: 0.0323\n",
      "Epoch 8/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0306\n",
      "Epoch 9/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0258\n",
      "Epoch 10/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0292\n",
      "Epoch 11/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0279\n",
      "Epoch 12/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0300\n",
      "Epoch 13/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0268\n",
      "Epoch 14/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0223\n",
      "Epoch 15/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0260\n",
      "Epoch 16/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0225\n",
      "Epoch 17/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0248\n",
      "Epoch 18/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0190\n",
      "Epoch 19/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0205\n",
      "Epoch 20/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0279\n",
      "Epoch 21/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0211\n",
      "Epoch 22/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0245\n",
      "Epoch 23/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0205\n",
      "Epoch 24/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0221\n",
      "Epoch 25/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0256\n",
      "Epoch 26/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0174\n",
      "Epoch 27/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0197\n",
      "Epoch 28/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0171\n",
      "Epoch 29/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0232\n",
      "Epoch 30/50\n",
      "41/41 [==============================] - 0s 898us/step - loss: 0.0182\n",
      "Epoch 31/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0240\n",
      "Epoch 32/50\n",
      "41/41 [==============================] - 0s 838us/step - loss: 0.0177\n",
      "Epoch 33/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0189\n",
      "Epoch 34/50\n",
      "41/41 [==============================] - 0s 873us/step - loss: 0.0160\n",
      "Epoch 35/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0175\n",
      "Epoch 36/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0158\n",
      "Epoch 37/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0143\n",
      "Epoch 38/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0153\n",
      "Epoch 39/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0166\n",
      "Epoch 40/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0145\n",
      "Epoch 41/50\n",
      "41/41 [==============================] - 0s 801us/step - loss: 0.0098\n",
      "Epoch 42/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0141\n",
      "Epoch 43/50\n",
      "41/41 [==============================] - 0s 898us/step - loss: 0.0119\n",
      "Epoch 44/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0119\n",
      "Epoch 45/50\n",
      "41/41 [==============================] - 0s 873us/step - loss: 0.0131\n",
      "Epoch 46/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0117\n",
      "Epoch 47/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0122\n",
      "Epoch 48/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0126\n",
      "Epoch 49/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0095\n",
      "Epoch 50/50\n",
      "41/41 [==============================] - 0s 873us/step - loss: 0.0108\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B1BF3430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "41/41 [==============================] - 2s 1ms/step - loss: 0.1777\n",
      "Epoch 2/50\n",
      "41/41 [==============================] - 0s 873us/step - loss: 0.0609\n",
      "Epoch 3/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0579\n",
      "Epoch 4/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0358\n",
      "Epoch 5/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0355\n",
      "Epoch 6/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0404\n",
      "Epoch 7/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0335\n",
      "Epoch 8/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0397\n",
      "Epoch 9/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0301\n",
      "Epoch 10/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0403\n",
      "Epoch 11/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0298\n",
      "Epoch 12/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0346\n",
      "Epoch 13/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0271\n",
      "Epoch 14/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0242\n",
      "Epoch 15/50\n",
      "41/41 [==============================] - 0s 873us/step - loss: 0.0265\n",
      "Epoch 16/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0354\n",
      "Epoch 17/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0224\n",
      "Epoch 18/50\n",
      "41/41 [==============================] - 0s 923us/step - loss: 0.0300\n",
      "Epoch 19/50\n",
      "41/41 [==============================] - 0s 923us/step - loss: 0.0260\n",
      "Epoch 20/50\n",
      "41/41 [==============================] - 0s 947us/step - loss: 0.0306\n",
      "Epoch 21/50\n",
      "41/41 [==============================] - 0s 898us/step - loss: 0.0200\n",
      "Epoch 22/50\n",
      "41/41 [==============================] - 0s 873us/step - loss: 0.0281\n",
      "Epoch 23/50\n",
      "41/41 [==============================] - 0s 873us/step - loss: 0.0201\n",
      "Epoch 24/50\n",
      "41/41 [==============================] - 0s 873us/step - loss: 0.0226\n",
      "Epoch 25/50\n",
      "41/41 [==============================] - 0s 873us/step - loss: 0.0222\n",
      "Epoch 26/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0208\n",
      "Epoch 27/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0191\n",
      "Epoch 28/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0262\n",
      "Epoch 29/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0269\n",
      "Epoch 30/50\n",
      "41/41 [==============================] - 0s 792us/step - loss: 0.0220\n",
      "Epoch 31/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0224\n",
      "Epoch 32/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0220\n",
      "Epoch 33/50\n",
      "41/41 [==============================] - 0s 923us/step - loss: 0.0252\n",
      "Epoch 34/50\n",
      "41/41 [==============================] - 0s 898us/step - loss: 0.0184\n",
      "Epoch 35/50\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.016 - 0s 879us/step - loss: 0.0160\n",
      "Epoch 36/50\n",
      "41/41 [==============================] - 0s 873us/step - loss: 0.0176\n",
      "Epoch 37/50\n",
      "41/41 [==============================] - 0s 1ms/step - loss: 0.0178\n",
      "Epoch 38/50\n",
      "41/41 [==============================] - 0s 869us/step - loss: 0.0169\n",
      "Epoch 39/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0170\n",
      "Epoch 40/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0205\n",
      "Epoch 41/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0147\n",
      "Epoch 42/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0162\n",
      "Epoch 43/50\n",
      "41/41 [==============================] - 0s 836us/step - loss: 0.0163\n",
      "Epoch 44/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0131\n",
      "Epoch 45/50\n",
      "41/41 [==============================] - 0s 923us/step - loss: 0.0162\n",
      "Epoch 46/50\n",
      "41/41 [==============================] - 0s 811us/step - loss: 0.0152\n",
      "Epoch 47/50\n",
      "41/41 [==============================] - 0s 823us/step - loss: 0.0150\n",
      "Epoch 48/50\n",
      "41/41 [==============================] - 0s 848us/step - loss: 0.0149\n",
      "Epoch 49/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0116\n",
      "Epoch 50/50\n",
      "41/41 [==============================] - 0s 798us/step - loss: 0.0110\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B1BF3700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "41/41 [==============================] - 1s 697us/step - loss: 0.1384\n",
      "Epoch 2/50\n",
      "41/41 [==============================] - 0s 723us/step - loss: 0.0566\n",
      "Epoch 3/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0434\n",
      "Epoch 4/50\n",
      "41/41 [==============================] - 0s 773us/step - loss: 0.0387\n",
      "Epoch 5/50\n",
      "41/41 [==============================] - 0s 736us/step - loss: 0.0311\n",
      "Epoch 6/50\n",
      "41/41 [==============================] - 0s 723us/step - loss: 0.0294\n",
      "Epoch 7/50\n",
      "41/41 [==============================] - 0s 723us/step - loss: 0.0395\n",
      "Epoch 8/50\n",
      "41/41 [==============================] - 0s 723us/step - loss: 0.0274\n",
      "Epoch 9/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0246\n",
      "Epoch 10/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0503\n",
      "Epoch 11/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0304\n",
      "Epoch 12/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0285\n",
      "Epoch 13/50\n",
      "41/41 [==============================] - 0s 648us/step - loss: 0.0302\n",
      "Epoch 14/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0289\n",
      "Epoch 15/50\n",
      "41/41 [==============================] - 0s 723us/step - loss: 0.0267\n",
      "Epoch 16/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0219\n",
      "Epoch 17/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0208\n",
      "Epoch 18/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0212\n",
      "Epoch 19/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0192\n",
      "Epoch 20/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0195\n",
      "Epoch 21/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0267\n",
      "Epoch 22/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0192\n",
      "Epoch 23/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0290\n",
      "Epoch 24/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0213\n",
      "Epoch 25/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0183\n",
      "Epoch 26/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0194\n",
      "Epoch 27/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0267\n",
      "Epoch 28/50\n",
      "41/41 [==============================] - 0s 648us/step - loss: 0.0181\n",
      "Epoch 29/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0276\n",
      "Epoch 30/50\n",
      "41/41 [==============================] - 0s 667us/step - loss: 0.0229\n",
      "Epoch 31/50\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.009 - 0s 698us/step - loss: 0.0197\n",
      "Epoch 32/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0179\n",
      "Epoch 33/50\n",
      "41/41 [==============================] - 0s 748us/step - loss: 0.0170\n",
      "Epoch 34/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0166\n",
      "Epoch 35/50\n",
      "41/41 [==============================] - 0s 773us/step - loss: 0.0179\n",
      "Epoch 36/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0175\n",
      "Epoch 37/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0166\n",
      "Epoch 38/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0187\n",
      "Epoch 39/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0145\n",
      "Epoch 40/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0158\n",
      "Epoch 41/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0115\n",
      "Epoch 42/50\n",
      "41/41 [==============================] - 0s 677us/step - loss: 0.0153\n",
      "Epoch 43/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0218\n",
      "Epoch 44/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0153\n",
      "Epoch 45/50\n",
      "41/41 [==============================] - 0s 654us/step - loss: 0.0123\n",
      "Epoch 46/50\n",
      "41/41 [==============================] - 0s 648us/step - loss: 0.0170\n",
      "Epoch 47/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0147\n",
      "Epoch 48/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0125\n",
      "Epoch 49/50\n",
      "41/41 [==============================] - 0s 673us/step - loss: 0.0114\n",
      "Epoch 50/50\n",
      "41/41 [==============================] - 0s 698us/step - loss: 0.0114\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B5C7CCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/50\n",
      "42/42 [==============================] - 1s 851us/step - loss: 0.0752\n",
      "Epoch 2/50\n",
      "42/42 [==============================] - 0s 851us/step - loss: 0.0317\n",
      "Epoch 3/50\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.0418\n",
      "Epoch 4/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0478\n",
      "Epoch 5/50\n",
      "42/42 [==============================] - 0s 851us/step - loss: 0.0269\n",
      "Epoch 6/50\n",
      "42/42 [==============================] - 0s 900us/step - loss: 0.0367\n",
      "Epoch 7/50\n",
      "42/42 [==============================] - 0s 895us/step - loss: 0.0301\n",
      "Epoch 8/50\n",
      "42/42 [==============================] - 0s 778us/step - loss: 0.0287\n",
      "Epoch 9/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0342\n",
      "Epoch 10/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0352\n",
      "Epoch 11/50\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.0291\n",
      "Epoch 12/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0286\n",
      "Epoch 13/50\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.0342\n",
      "Epoch 14/50\n",
      "42/42 [==============================] - 0s 851us/step - loss: 0.0261\n",
      "Epoch 15/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0304\n",
      "Epoch 16/50\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.0297\n",
      "Epoch 17/50\n",
      "42/42 [==============================] - 0s 851us/step - loss: 0.0204\n",
      "Epoch 18/50\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.0270\n",
      "Epoch 19/50\n",
      "42/42 [==============================] - 0s 851us/step - loss: 0.0212\n",
      "Epoch 20/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0246\n",
      "Epoch 21/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0300\n",
      "Epoch 22/50\n",
      "42/42 [==============================] - 0s 851us/step - loss: 0.0226\n",
      "Epoch 23/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0246\n",
      "Epoch 24/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0185\n",
      "Epoch 25/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0185\n",
      "Epoch 26/50\n",
      "42/42 [==============================] - 0s 876us/step - loss: 0.0218\n",
      "Epoch 27/50\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.0194\n",
      "Epoch 28/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0208\n",
      "Epoch 29/50\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.0194\n",
      "Epoch 30/50\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.0171\n",
      "Epoch 31/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0171\n",
      "Epoch 32/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0183\n",
      "Epoch 33/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0164\n",
      "Epoch 34/50\n",
      "42/42 [==============================] - 0s 900us/step - loss: 0.0163\n",
      "Epoch 35/50\n",
      "42/42 [==============================] - 0s 900us/step - loss: 0.0158\n",
      "Epoch 36/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0192\n",
      "Epoch 37/50\n",
      "42/42 [==============================] - 0s 876us/step - loss: 0.0139\n",
      "Epoch 38/50\n",
      "42/42 [==============================] - 0s 891us/step - loss: 0.0178\n",
      "Epoch 39/50\n",
      "42/42 [==============================] - 0s 900us/step - loss: 0.0133\n",
      "Epoch 40/50\n",
      "42/42 [==============================] - 0s 870us/step - loss: 0.0154\n",
      "Epoch 41/50\n",
      "42/42 [==============================] - 0s 924us/step - loss: 0.0164\n",
      "Epoch 42/50\n",
      "42/42 [==============================] - 0s 924us/step - loss: 0.0133\n",
      "Epoch 43/50\n",
      "42/42 [==============================] - 0s 900us/step - loss: 0.0126\n",
      "Epoch 44/50\n",
      "42/42 [==============================] - 0s 852us/step - loss: 0.0135\n",
      "Epoch 45/50\n",
      "42/42 [==============================] - 0s 827us/step - loss: 0.0153\n",
      "Epoch 46/50\n",
      "42/42 [==============================] - 0s 851us/step - loss: 0.0157\n",
      "Epoch 47/50\n",
      "42/42 [==============================] - 0s 851us/step - loss: 0.0121\n",
      "Epoch 48/50\n",
      "42/42 [==============================] - 0s 868us/step - loss: 0.0150\n",
      "Epoch 49/50\n",
      "42/42 [==============================] - 0s 851us/step - loss: 0.0103\n",
      "Epoch 50/50\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.0100\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000230B3259EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1 Error of the model batch_size 3 epochs 50 units 30 4383.808448176191\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. MinMaxScaler expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-a77dd19532c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# stops after the first iteration ValueError: Found array with dim 3. MinMaxScaler expected <= 2.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcv_sc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_x_train_list\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcv_x_test_list\u001b[0m  \u001b[1;33m,\u001b[0m\u001b[0mcv_y_train_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_y_test_list\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-b0bc6609b088>\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(cv_x_train_list, cv_x_test_list, cv_y_train, cv_y_test, metric)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mhyper_parameter_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"batch_size \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" epochs \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" units \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munit_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m                 \u001b[0mcv_score_of_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_cv_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_x_test_list_temp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv_y_test_temp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv_x_train_list_temp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv_y_train_temp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0munit_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m                 \u001b[0mi_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi_num\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" Error of the model \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mhyper_parameter_string\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mcv_score_of_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-b0bc6609b088>\u001b[0m in \u001b[0;36mcalculate_cv_scores\u001b[1;34m(cv_x_test_list_param, cv_y_test_param, metric, cv_x_train_list_param, cv_y_train_param, unit_count, batch_count, epoch_count)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mcv_x_train_list_param\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfold_min_max_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_x_train_list_param\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mcv_y_train_param\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfold_min_max_scaler_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_y_train_param\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'n_samples_seen_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m         X = self._validate_data(X, reset=first_pass,\n\u001b[0m\u001b[0;32m    370\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m                                 force_all_finite=\"allow-nan\")\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 )\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 640\u001b[1;33m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0m\u001b[0;32m    641\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. MinMaxScaler expected <= 2."
     ]
    }
   ],
   "source": [
    "# stops after the first iteration ValueError: Found array with dim 3. MinMaxScaler expected <= 2.\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "cv_sc = cross_val_score(cv_x_train_list , cv_x_test_list  ,cv_y_train_list, cv_y_test_list , mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cross validation scores for each train and validation cases (rolling forecast origin)\n",
    "cv_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameter, score = find_least_error_hyperparameters(cv_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
